1
00:00:00,088 --> 00:00:10,875
[SPEAKER_09]: is on the oversight of artificial intelligence, the first in a series of hearings intended to write the rules of AI.

2
00:00:10,875 --> 00:00:20,901
[SPEAKER_09]: Our goal is to demystify and hold accountable those new technologies to avoid some of the mistakes of the past.

3
00:00:20,901 --> 00:00:27,065
[SPEAKER_09]: And now for some introductory remarks.

4
00:00:27,065 --> 00:00:27,566
[SPEAKER_09]: Too often,

5
00:00:28,486 --> 00:00:32,749
[SPEAKER_09]: we have seen what happens when technology outpaces regulation.

6
00:00:32,749 --> 00:00:42,976
[SPEAKER_09]: The unbridled exploitation of personal data, the proliferation of disinformation, and the deepening of societal inequalities.

7
00:00:42,976 --> 00:00:55,926
[SPEAKER_09]: We have seen how algorithmic biases can perpetuate discrimination and prejudice and how the lack of transparency can undermine public trust.

8
00:00:55,926 --> 00:00:57,647
[SPEAKER_09]: This is not the future we want.

9
00:00:59,993 --> 00:01:07,014
[SPEAKER_09]: If you were listening from home, you might have thought that voice was mine and the words from me.

10
00:01:07,014 --> 00:01:11,595
[SPEAKER_09]: But in fact, that voice was not mine.

11
00:01:11,595 --> 00:01:14,876
[SPEAKER_09]: The words were not mine.

12
00:01:14,876 --> 00:01:22,417
[SPEAKER_09]: And the audio was an AI voice cloning software trained on my floor speeches.

13
00:01:22,417 --> 00:01:26,317
[SPEAKER_09]: The remarks were written by chat GPT.

14
00:01:26,317 --> 00:01:28,478
[SPEAKER_09]: When it was asked how

15
00:01:29,757 --> 00:01:33,618
[SPEAKER_09]: I would open this hearing.

16
00:01:33,618 --> 00:01:37,279
[SPEAKER_09]: And you heard just now the result.

17
00:01:37,279 --> 00:01:44,560
[SPEAKER_09]: I asked chat GPT, why did you pick those themes and that content?

18
00:01:44,560 --> 00:01:52,962
[SPEAKER_09]: And it answered, and I'm quoting, Blumenthal has a strong record in advocating for consumer protection and civil rights.

19
00:01:52,962 --> 00:01:55,763
[SPEAKER_09]: He has been vocal about issues such as data privacy,

20
00:02:00,523 --> 00:02:07,388
[SPEAKER_09]: and the potential for discrimination in algorithmic decision making.

21
00:02:07,388 --> 00:02:12,371
[SPEAKER_09]: Therefore, the statement emphasizes these aspects.

22
00:02:12,371 --> 00:02:12,632
[SPEAKER_09]: Mr.

23
00:02:12,632 --> 00:02:17,255
[SPEAKER_09]: Altman, I appreciate Chad GPT's endorsement.

24
00:02:17,255 --> 00:02:22,999
[SPEAKER_09]: In all seriousness, this apparent reasoning is pretty impressive.

25
00:02:22,999 --> 00:02:25,540
[SPEAKER_09]: I am sure that we'll look back in a decade

26
00:02:27,253 --> 00:02:40,718
[SPEAKER_09]: view chat GPT and GPT-4 like we do the first cell phone, those big clunky things that we used to carry around, but we recognize that we are on the verge really of a new era.

27
00:02:40,718 --> 00:02:54,123
[SPEAKER_09]: The audio and my playing it may strike you as curious or humorous, but what reverberated in my mind was what if I had asked it and what if it had

28
00:02:55,697 --> 00:03:05,184
[SPEAKER_09]: provided an endorsement of Ukraine's surrendering or Vladimir Putin's leadership, that would have been really frightening.

29
00:03:05,184 --> 00:03:10,609
[SPEAKER_09]: And the prospect is more than a little scary, to use the word, Mr.

30
00:03:10,609 --> 00:03:13,111
[SPEAKER_09]: Altman, you have used yourself.

31
00:03:13,111 --> 00:03:19,155
[SPEAKER_09]: And I think you have been very constructive in calling attention to the pitfalls as well as the promise.

32
00:03:19,155 --> 00:03:22,598
[SPEAKER_09]: And that's the reason why we wanted you to be here today.

33
00:03:22,598 --> 00:03:24,740
[SPEAKER_09]: And we thank you and our other witnesses

34
00:03:25,609 --> 00:03:26,770
[SPEAKER_09]: joining us.

35
00:03:26,770 --> 00:03:35,234
[SPEAKER_09]: For several months now, the public has been fascinated with GPT, DALI, and other AI tools.

36
00:03:35,234 --> 00:03:45,319
[SPEAKER_09]: These examples, like the homework done by ChatGPT or the articles and op-eds that it can write, feel like novelties.

37
00:03:45,319 --> 00:03:52,222
[SPEAKER_09]: But the underlying advancement of this era are more than just research experiments.

38
00:03:52,222 --> 00:03:54,103
[SPEAKER_09]: They are no longer fantasies.

39
00:03:54,924 --> 00:03:58,365
[SPEAKER_09]: of science fiction, they are real and present.

40
00:03:58,365 --> 00:04:11,290
[SPEAKER_09]: The promises of curing cancer or developing new understandings of physics and biology or modeling climate and weather, all very encouraging and hopeful.

41
00:04:11,290 --> 00:04:15,731
[SPEAKER_09]: But we also know the potential harms.

42
00:04:15,731 --> 00:04:23,074
[SPEAKER_09]: And we've seen them already, weaponized disinformation, housing discrimination, harassment of women and impersonation fraud.

43
00:04:23,663 --> 00:04:28,148
[SPEAKER_09]: voice cloning, deep fakes.

44
00:04:28,148 --> 00:04:34,054
[SPEAKER_09]: These are the potential risks, despite the other rewards.

45
00:04:34,054 --> 00:04:44,965
[SPEAKER_09]: And for me, perhaps the biggest nightmare is the looming new industrial revolution, the displacement of millions of workers, the loss of

46
00:04:46,430 --> 00:04:57,997
[SPEAKER_09]: huge numbers of jobs, the need to prepare for this new industrial revolution in skill training and relocation that may be required.

47
00:04:57,997 --> 00:05:06,702
[SPEAKER_09]: And already industry leaders are calling attention to those challenges.

48
00:05:06,702 --> 00:05:14,246
[SPEAKER_09]: To quote Chat GPT, this is not necessarily the future that we want.

49
00:05:15,105 --> 00:05:20,047
[SPEAKER_09]: We need to maximize the good over the bad.

50
00:05:20,047 --> 00:05:21,948
[SPEAKER_09]: Congress has a choice now.

51
00:05:21,948 --> 00:05:24,829
[SPEAKER_09]: We had the same choice when we faced social media.

52
00:05:24,829 --> 00:05:26,930
[SPEAKER_09]: We failed to seize that moment.

53
00:05:26,930 --> 00:05:39,416
[SPEAKER_09]: The result is predators on the internet, toxic content, exploiting children, creating dangers for them, and Senator Blackburn and I and others like Senator

54
00:05:39,948 --> 00:05:45,092
[SPEAKER_09]: Durbin on the Judiciary Committee are trying to deal with it, Kids Online Safety Act.

55
00:05:45,092 --> 00:05:49,175
[SPEAKER_09]: But Congress failed to meet the moment on social media.

56
00:05:49,175 --> 00:05:52,637
[SPEAKER_09]: Now we have the obligation to do it on A.I.

57
00:05:52,637 --> 00:05:58,782
[SPEAKER_09]: before the threats and the risks become real.

58
00:05:58,782 --> 00:06:02,745
[SPEAKER_09]: Sensible safeguards are not in opposition to innovation.

59
00:06:02,745 --> 00:06:04,767
[SPEAKER_09]: Accountability is not a burden.

60
00:06:04,767 --> 00:06:06,408
[SPEAKER_09]: Far from it.

61
00:06:06,408 --> 00:06:09,410
[SPEAKER_09]: They are the foundation of how we can move ahead while protecting

62
00:06:09,918 --> 00:06:11,198
[SPEAKER_09]: public trust.

63
00:06:11,198 --> 00:06:18,461
[SPEAKER_09]: They are how we can lead the world in technology and science, but also in promoting our democratic values.

64
00:06:18,461 --> 00:06:26,023
[SPEAKER_09]: Otherwise, in the absence of that trust, I think we may well lose both.

65
00:06:26,023 --> 00:06:32,065
[SPEAKER_09]: These are sophisticated technology, but there are basic expectations common in our law.

66
00:06:32,065 --> 00:06:34,746
[SPEAKER_09]: We can start with transparency.

67
00:06:34,746 --> 00:06:39,808
[SPEAKER_09]: AI companies ought to be required to test their systems, disclose known risks, and allow independent

68
00:06:40,566 --> 00:06:51,891
[SPEAKER_09]: researcher access, we can establish scorecards and nutrition labels to encourage competition based on safety and trustworthiness.

69
00:06:51,891 --> 00:07:08,557
[SPEAKER_09]: Limitations on use, there are places where the risk of AI is so extreme that we ought to impose restriction or even ban their use, especially when it comes to commercial invasions of privacy for profit and decisions that affect people's livelihoods.

70
00:07:08,557 --> 00:07:09,778
[SPEAKER_09]: And of course, accountability.

71
00:07:10,330 --> 00:07:11,530
[SPEAKER_09]: or liability.

72
00:07:11,530 --> 00:07:16,892
[SPEAKER_09]: When AI companies and their clients cause harm, they should be held liable.

73
00:07:16,892 --> 00:07:21,293
[SPEAKER_09]: We should not repeat our past mistakes.

74
00:07:21,293 --> 00:07:35,096
[SPEAKER_09]: For example, Section 230, forcing companies to think ahead and be responsible for the ramifications of their business decisions can be the most powerful tool of all.

75
00:07:35,096 --> 00:07:37,137
[SPEAKER_09]: Garbage in, garbage out.

76
00:07:37,137 --> 00:07:38,437
[SPEAKER_09]: The principle still applies.

77
00:07:39,504 --> 00:07:47,886
[SPEAKER_09]: We ought to beware of the garbage, whether it's going into these platforms or coming out of them.

78
00:07:47,886 --> 00:07:56,467
[SPEAKER_09]: And the ideas that we develop in this hearing, I think, will provide a solid path forward.

79
00:07:56,467 --> 00:07:59,728
[SPEAKER_09]: I look forward to discussing them with you today.

80
00:07:59,728 --> 00:08:01,568
[SPEAKER_09]: And I will just finish on this note.

81
00:08:01,568 --> 00:08:04,109
[SPEAKER_09]: The AI industry doesn't have to wait for Congress.

82
00:08:05,501 --> 00:08:20,369
[SPEAKER_09]: I hope there are ideas and feedback from this discussion and from the industry and voluntary action such as we've seen lacking in many social media platforms and the consequences have been huge.

83
00:08:20,369 --> 00:08:32,555
[SPEAKER_09]: So I'm hoping that we will elevate rather than have a race to the bottom and I think these hearings will be an important part of this conversation.

84
00:08:32,555 --> 00:08:34,076
[SPEAKER_09]: This one is only the first.

85
00:08:35,107 --> 00:08:41,192
[SPEAKER_09]: The Ranking Member and I have agreed there should be more, and we're going to invite other industry leaders.

86
00:08:41,192 --> 00:08:48,298
[SPEAKER_09]: Some have committed to come, experts, academics, and the public, we hope, will participate.

87
00:08:48,298 --> 00:08:53,021
[SPEAKER_09]: And with that, I will turn to the Ranking Member, Senator Hawley.

88
00:08:53,021 --> 00:08:53,962
[SPEAKER_14]: Thank you very much, Mr.

89
00:08:53,962 --> 00:08:54,402
[SPEAKER_14]: Chairman.

90
00:08:54,402 --> 00:08:56,204
[SPEAKER_14]: Thanks to the witnesses for being here.

91
00:08:56,204 --> 00:09:00,567
[SPEAKER_14]: I appreciate that several of you had long journeys to make in order to be here.

92
00:09:00,567 --> 00:09:01,288
[SPEAKER_14]: I appreciate you.

93
00:09:01,925 --> 00:09:02,646
[SPEAKER_14]: banking the time.

94
00:09:02,646 --> 00:09:03,966
[SPEAKER_14]: I look forward to your testimony.

95
00:09:03,966 --> 00:09:07,989
[SPEAKER_14]: I want to thank Senator Blumenthal for convening this hearing, for being a leader on this topic.

96
00:09:07,989 --> 00:09:15,815
[SPEAKER_14]: You know, a year ago, we couldn't have had this hearing because the technology that we're talking about had not burst into public consciousness.

97
00:09:15,815 --> 00:09:27,083
[SPEAKER_14]: That gives us a sense, I think, of just how rapidly this technology that we're talking about today is changing and evolving and transforming our world right before

98
00:09:27,584 --> 00:09:28,325
[SPEAKER_14]: our very eyes.

99
00:09:28,325 --> 00:09:48,096
[SPEAKER_14]: I was talking with someone just last night, a researcher in the field of psychiatry, who was pointing out to me that the chat GPT and generative AI, these large language models, it's really like the invention of the Internet in scale, at least, at least, and potentially far, far more significant than that.

100
00:09:48,096 --> 00:09:53,900
[SPEAKER_14]: We could be looking at one of the most significant technological innovations in human history.

101
00:09:54,767 --> 00:09:59,188
[SPEAKER_14]: And I think my question is, what kind of an innovation is it going to be?

102
00:09:59,188 --> 00:10:15,791
[SPEAKER_14]: Is it going to be like the printing press that diffused knowledge and power and learning widely across the landscape that empowered ordinary everyday individuals that led to greater flourishing, that led above all to greater liberty?

103
00:10:15,791 --> 00:10:18,652
[SPEAKER_14]: Or is it going to be more like the atom bomb?

104
00:10:18,652 --> 00:10:23,053
[SPEAKER_14]: Huge technological breakthrough, but the consequences severe.

105
00:10:23,947 --> 00:10:28,030
[SPEAKER_14]: terrible, continue to haunt us to this day?

106
00:10:28,030 --> 00:10:29,171
[SPEAKER_14]: I don't know the answer to that question.

107
00:10:29,171 --> 00:10:34,115
[SPEAKER_14]: I don't think any of us in the room know the answer to that question, because I think the answer has not yet been written.

108
00:10:34,115 --> 00:10:41,420
[SPEAKER_14]: And to a certain extent, it's up to us here and to us as the American people to write the answer.

109
00:10:41,420 --> 00:10:43,261
[SPEAKER_14]: What kind of technology will this be?

110
00:10:43,261 --> 00:10:45,423
[SPEAKER_14]: How will we use it to better our lives?

111
00:10:45,423 --> 00:10:51,888
[SPEAKER_14]: How will we use it to actually harness the power of technological innovation for the good of the American people?

112
00:10:52,347 --> 00:10:58,110
[SPEAKER_14]: for the liberty of the American people, not for the power of the few.

113
00:10:58,110 --> 00:11:17,339
[SPEAKER_14]: You know, I was reminded of the psychologist and writer Carl Jung who said at the beginning of the last century that our ability for technological innovation, our capacity for technological revolution had far outpaced our ethical and moral ability to apply and harness the technology we developed.

114
00:11:17,339 --> 00:11:18,980
[SPEAKER_14]: That was a century ago.

115
00:11:18,980 --> 00:11:21,861
[SPEAKER_14]: I think the story of the 20th century largely bore him out.

116
00:11:23,155 --> 00:11:41,546
[SPEAKER_14]: And I just wonder what will we say as we look back at this moment about these new technologies, about generative AI, about these language models, and about the host of other AI capacities that are even right now underdeveloped, but not just in this country, but in China, the countries of our adversaries, and all around the world.

117
00:11:41,546 --> 00:11:45,328
[SPEAKER_14]: And I think that the question that Jung posed is really the question that faces us.

118
00:11:45,328 --> 00:11:51,132
[SPEAKER_14]: Will we strike that balance between technological innovation and our ethical and moral responsibility?

119
00:11:52,008 --> 00:11:57,131
[SPEAKER_14]: to humanity, to liberty, to the freedom of this country.

120
00:11:57,131 --> 00:12:00,593
[SPEAKER_14]: And I hope that today's hearing will take us a step closer to that answer.

121
00:12:00,593 --> 00:12:01,033
[SPEAKER_14]: Thank you, Mr.

122
00:12:01,033 --> 00:12:01,593
[SPEAKER_14]: Chairman.

123
00:12:01,593 --> 00:12:02,094
[SPEAKER_09]: Thanks.

124
00:12:02,094 --> 00:12:03,354
[SPEAKER_09]: Thanks, Senator Hawley.

125
00:12:03,354 --> 00:12:11,799
[SPEAKER_09]: I'm going to turn to the chairman of the Judiciary Committee and the ranking member, Senator Graham, if they have opening remarks as well.

126
00:12:11,799 --> 00:12:12,319
[SPEAKER_12]: Yes, Mr.

127
00:12:12,319 --> 00:12:12,639
[SPEAKER_12]: Chairman.

128
00:12:12,639 --> 00:12:13,400
[SPEAKER_12]: Thank you very much.

129
00:12:13,400 --> 00:12:15,801
[SPEAKER_12]: And Senator Hawley as well.

130
00:12:15,801 --> 00:12:19,343
[SPEAKER_12]: Last week in the this committee, full committee, Senate Judiciary Committee,

131
00:12:20,037 --> 00:12:29,805
[SPEAKER_12]: We dealt with an issue that had been waiting for attention for almost two decades, and that is what to do with the social media when it comes to the abuse of children.

132
00:12:29,805 --> 00:12:41,494
[SPEAKER_12]: We had four bills initially that were considered by this committee, and what may be history in the making, we passed all four bills with unanimous roll calls.

133
00:12:41,494 --> 00:12:43,375
[SPEAKER_12]: Unanimous roll calls.

134
00:12:43,375 --> 00:12:46,698
[SPEAKER_12]: I can't remember another time when we've done that on an issue that important.

135
00:12:47,819 --> 00:12:59,606
[SPEAKER_12]: It's an indication, I think, of the important position of this committee in the national debate on issues that affect every single family and affect our future in a profound way.

136
00:12:59,606 --> 00:13:05,970
[SPEAKER_12]: 1989 was a historic watershed year in America, because that's when Seinfeld arrived.

137
00:13:05,970 --> 00:13:11,794
[SPEAKER_12]: And we had a sitcom, which was supposedly about little or nothing, which turned out to be enduring.

138
00:13:12,649 --> 00:13:23,612
[SPEAKER_12]: I like to watch it, obviously, and I always marvel when they show the phones that he used in 1989, and I think about those in comparison to what we carry around in our pockets today.

139
00:13:23,612 --> 00:13:25,872
[SPEAKER_12]: It's a dramatic change.

140
00:13:25,872 --> 00:13:39,796
[SPEAKER_12]: And I guess the question as I look at that is, does this change in phone technology that we've witnessed through the sitcom really exemplify a profound change in America still unanswered?

141
00:13:40,578 --> 00:13:52,565
[SPEAKER_12]: But the very basic question we face is whether or not this issue of AI is a quantitative change in technology or a qualitative change.

142
00:13:52,565 --> 00:13:57,268
[SPEAKER_12]: The suggestions that I've heard from experts in the field suggest it's qualitative.

143
00:13:57,268 --> 00:13:59,930
[SPEAKER_12]: Is it AI fundamentally different?

144
00:13:59,930 --> 00:14:01,811
[SPEAKER_12]: Is it a game changer?

145
00:14:01,811 --> 00:14:07,094
[SPEAKER_12]: Is it so disruptive that we need to treat it differently than other forms of innovation?

146
00:14:07,094 --> 00:14:08,095
[SPEAKER_12]: That's the starting point.

147
00:14:08,797 --> 00:14:19,661
[SPEAKER_12]: And the second starting point is one that's humbling, and that is the fact when you look at the record of Congress in dealing with innovation, technology, and rapid change, we're not designed for that.

148
00:14:19,661 --> 00:14:23,543
[SPEAKER_12]: In fact, the Senate was not created for that purpose, but just the opposite.

149
00:14:23,543 --> 00:14:24,583
[SPEAKER_12]: Slow things down.

150
00:14:24,583 --> 00:14:25,764
[SPEAKER_12]: Take a harder look at it.

151
00:14:25,764 --> 00:14:28,205
[SPEAKER_12]: Don't react to public sentiment.

152
00:14:28,205 --> 00:14:30,226
[SPEAKER_12]: Make sure you're doing the right thing.

153
00:14:30,226 --> 00:14:35,167
[SPEAKER_12]: Well, I've heard of the potential, the positive potential of AI, and it is enormous.

154
00:14:35,167 --> 00:14:36,328
[SPEAKER_12]: You can go through lists of

155
00:14:36,975 --> 00:14:47,638
[SPEAKER_12]: The deployment of technology that would say that an idea you can sketch for a website on a napkin can generate functioning code.

156
00:14:47,638 --> 00:14:53,480
[SPEAKER_12]: Pharmaceutical companies could use the technology to identify new candidates to treat disease.

157
00:14:53,480 --> 00:14:55,220
[SPEAKER_12]: The list goes on and on.

158
00:14:55,220 --> 00:14:58,161
[SPEAKER_12]: And then, of course, the danger, and it's profound as well.

159
00:14:58,918 --> 00:15:01,260
[SPEAKER_12]: So I'm glad that this hearing is taking place.

160
00:15:01,260 --> 00:15:03,962
[SPEAKER_12]: I think it's important for all of us to participate.

161
00:15:03,962 --> 00:15:06,563
[SPEAKER_12]: I'm glad that it's a bipartisan approach.

162
00:15:06,563 --> 00:15:12,668
[SPEAKER_12]: We're going to have to scramble to keep up with the pace of innovation in terms of our government public response to it.

163
00:15:12,668 --> 00:15:13,929
[SPEAKER_12]: But this is a great start.

164
00:15:13,929 --> 00:15:14,389
[SPEAKER_12]: Thank you, Mr.

165
00:15:14,389 --> 00:15:15,950
[SPEAKER_12]: Chairman.

166
00:15:15,950 --> 00:15:16,551
[SPEAKER_09]: Thanks.

167
00:15:16,551 --> 00:15:17,471
[SPEAKER_09]: Thanks, Senator.

168
00:15:17,471 --> 00:15:22,194
[SPEAKER_09]: It is very much a bipartisan approach, very deeply and broadly bipartisan.

169
00:15:22,194 --> 00:15:25,697
[SPEAKER_09]: And in that spirit, I'm going to turn to my friend, Senator Graham.

170
00:15:32,477 --> 00:15:33,117
[SPEAKER_09]: Thank you.

171
00:15:33,117 --> 00:15:38,841
[SPEAKER_09]: That was not written by AI for sure.

172
00:15:38,841 --> 00:15:40,803
[SPEAKER_09]: Let me introduce now the witnesses.

173
00:15:40,803 --> 00:15:43,725
[SPEAKER_09]: We're very grateful to you for being here.

174
00:15:43,725 --> 00:15:56,153
[SPEAKER_09]: Sam Altman is the co-founder and CEO of OpenAI, the AI research and deployment company behind ChatGPT and DALI.

175
00:15:56,153 --> 00:15:56,393
[SPEAKER_09]: Mr.

176
00:15:56,393 --> 00:15:59,616
[SPEAKER_09]: Altman was president of the early stage startup accelerator,

177
00:16:00,630 --> 00:16:04,812
[SPEAKER_09]: Combinator from 2014 to 2019.

178
00:16:04,812 --> 00:16:10,655
[SPEAKER_09]: OpenAI was founded in 2015.

179
00:16:10,655 --> 00:16:22,662
[SPEAKER_09]: Christina Montgomery is IBM's Vice President and Chief Privacy and Trust Officer overseeing the company's global privacy program policies, compliance, and strategy.

180
00:16:22,662 --> 00:16:28,445
[SPEAKER_09]: She also chairs IBM's AI Ethics Board, a multidisciplinary team responsible for the governance of AI

181
00:16:29,186 --> 00:16:31,367
[SPEAKER_09]: and emerging technologies.

182
00:16:31,367 --> 00:16:39,288
[SPEAKER_09]: Christina has served in various roles at IBM, including Corporate Secretary to the company's Board of Directors.

183
00:16:39,288 --> 00:16:43,209
[SPEAKER_09]: She is a global leader in AI ethics and governments.

184
00:16:43,209 --> 00:16:44,170
[SPEAKER_09]: And Ms.

185
00:16:44,170 --> 00:16:59,013
[SPEAKER_09]: Montgomery also is a member of the United States Chamber of Commerce AI Commission and the United States National AI Advisory Committee, which was established in 2022 to advise the President and the National AI Initiative Office

186
00:16:59,547 --> 00:17:02,868
[SPEAKER_09]: on a range of topics related to AI.

187
00:17:02,868 --> 00:17:07,208
[SPEAKER_09]: Gary Marcus is a leading voice in artificial intelligence.

188
00:17:07,208 --> 00:17:24,631
[SPEAKER_09]: He's a scientist, bestselling author, and entrepreneur, founder of the Robust AI and Geometric AI acquired by Uber, if I'm not mistaken, and emeritus professor of psychology and neuroscience at NYU.

189
00:17:24,631 --> 00:17:24,891
[SPEAKER_09]: Mr.

190
00:17:24,891 --> 00:17:28,912
[SPEAKER_09]: Marcus is well known for his challenges to contemporary AI

191
00:17:29,534 --> 00:17:39,862
[SPEAKER_09]: anticipating many of the current limitations decades in advance and for his research in human language, development, and cognitive neuroscience.

192
00:17:39,862 --> 00:17:42,323
[SPEAKER_09]: Thank you for being here.

193
00:17:42,323 --> 00:17:50,829
[SPEAKER_09]: And as you may know, our custom on the Judiciary Committee is to swear in our witnesses before they testify.

194
00:17:50,829 --> 00:17:55,193
[SPEAKER_09]: So if you would all please rise and raise your right hand.

195
00:17:56,474 --> 00:18:05,042
[SPEAKER_09]: You solemnly swear that the testimony that you are going to give is the truth, the whole truth and nothing but the truth, so help me God.

196
00:18:05,042 --> 00:18:07,865
[SPEAKER_09]: Thank you.

197
00:18:07,865 --> 00:18:08,126
[SPEAKER_09]: Mr.

198
00:18:08,126 --> 00:18:11,789
[SPEAKER_09]: Altman, we're going to begin with you if that's okay.

199
00:18:11,789 --> 00:18:12,590
[SPEAKER_15]: Thank you.

200
00:18:12,590 --> 00:18:13,531
[SPEAKER_15]: Thank you, Chairman Blumenthal.

201
00:18:14,145 --> 00:18:20,251
[SPEAKER_15]: Ranking Member Hawley, members of the Judiciary Committee, thank you for the opportunity to speak to you today about large neural networks.

202
00:18:20,251 --> 00:18:23,854
[SPEAKER_15]: It's really an honor to be here, even more so in the moment than I expected.

203
00:18:23,854 --> 00:18:25,035
[SPEAKER_15]: My name is Sam Altman.

204
00:18:25,035 --> 00:18:28,819
[SPEAKER_15]: I'm the Chief Executive Officer of OpenAI.

205
00:18:28,819 --> 00:18:39,069
[SPEAKER_15]: OpenAI was founded on the belief that artificial intelligence has the potential to improve nearly every aspect of our lives, but also that it creates serious risks we have to work together to manage.

206
00:18:41,072 --> 00:18:43,433
[SPEAKER_15]: We're here because people love this technology.

207
00:18:43,433 --> 00:18:45,353
[SPEAKER_15]: We think it can be a printing press moment.

208
00:18:45,353 --> 00:18:48,334
[SPEAKER_15]: We have to work together to make it so.

209
00:18:48,334 --> 00:18:54,256
[SPEAKER_15]: OpenAI is an unusual company, and we set it up that way because AI is an unusual technology.

210
00:18:54,256 --> 00:19:05,000
[SPEAKER_15]: We are governed by a nonprofit, and our activities are driven by our mission and our charter, which commit us to working to ensure that the broad distribution of the benefits of AI and to maximizing the safety of AI systems.

211
00:19:06,907 --> 00:19:15,649
[SPEAKER_15]: We are working to build tools that one day can help us make new discoveries and address some of humanity's biggest challenges, like climate change and curing cancer.

212
00:19:15,649 --> 00:19:26,871
[SPEAKER_15]: Our current systems aren't yet capable of doing these things, but it has been immensely gratifying to watch many people around the world get so much value from what these systems can already do today.

213
00:19:26,871 --> 00:19:31,552
[SPEAKER_15]: We love seeing people use our tools to create, to learn, to be more productive.

214
00:19:31,552 --> 00:19:36,473
[SPEAKER_15]: We're very optimistic that there are going to be fantastic jobs in the future and that current jobs can get much better.

215
00:19:38,362 --> 00:19:41,845
[SPEAKER_15]: We also love seeing what developers are doing to improve lives.

216
00:19:41,845 --> 00:19:51,952
[SPEAKER_15]: For example, Be My Eyes used our new multimodal technology in GPT-4 to help visually impaired individuals navigate their environment.

217
00:19:51,952 --> 00:20:04,801
[SPEAKER_15]: We believe that the benefits of the tools we have deployed so far vastly outweigh the risks, but ensuring their safety is vital to our work, and we make significant efforts to ensure that safety is built into our systems at all levels.

218
00:20:04,801 --> 00:20:06,222
[SPEAKER_15]: Before releasing any new system,

219
00:20:07,130 --> 00:20:20,479
[SPEAKER_15]: OpenAI conducts extensive testing, engages external experts for detailed reviews and independent audits, improves the model's behavior, and implements robust safety and monitoring systems.

220
00:20:20,479 --> 00:20:30,987
[SPEAKER_15]: Before we released GPT-4, our latest model, we spent over six months conducting extensive evaluations, external red teaming, and dangerous capability testing.

221
00:20:30,987 --> 00:20:33,148
[SPEAKER_15]: We are proud of the progress that we made.

222
00:20:33,148 --> 00:20:35,910
[SPEAKER_15]: GPT-4 is more likely to respond helpfully and truthfully

223
00:20:36,756 --> 00:20:42,440
[SPEAKER_15]: and refuse harmful requests than any other widely deployed model of similar capability.

224
00:20:42,440 --> 00:20:50,827
[SPEAKER_15]: However, we think that regulatory intervention by governments will be critical to mitigate the risks of increasingly powerful models.

225
00:20:50,827 --> 00:21:01,855
[SPEAKER_15]: For example, the US government might consider a combination of licensing and testing requirements for development and release of AI models above a threshold of capabilities.

226
00:21:01,855 --> 00:21:04,297
[SPEAKER_15]: There are several other areas I mentioned in my written testimony

227
00:21:04,762 --> 00:21:20,109
[SPEAKER_15]: where I believe that companies like ours can partner with governments, including ensuring that the most powerful AI models adhere to a set of safety requirements, facilitating processes to develop and update safety measures, and examining opportunities for global coordination.

228
00:21:20,109 --> 00:21:26,612
[SPEAKER_15]: And as you mentioned, I think it's important that companies have their own responsibility here, no matter what Congress does.

229
00:21:26,612 --> 00:21:30,833
[SPEAKER_15]: This is a remarkable time to be working on artificial intelligence.

230
00:21:30,833 --> 00:21:34,195
[SPEAKER_15]: But as this technology advances, we understand that people are anxious

231
00:21:34,728 --> 00:21:36,709
[SPEAKER_15]: about how it could change the way we live.

232
00:21:36,709 --> 00:21:38,369
[SPEAKER_15]: We are too.

233
00:21:38,369 --> 00:21:47,893
[SPEAKER_15]: But we believe that we can and must work together to identify and manage the potential downsides so that we can all enjoy the tremendous upsides.

234
00:21:47,893 --> 00:21:52,915
[SPEAKER_15]: It is essential that powerful AI is developed with democratic values in mind, and this means that U.S.

235
00:21:52,915 --> 00:21:55,116
[SPEAKER_15]: leadership is critical.

236
00:21:55,116 --> 00:22:00,978
[SPEAKER_15]: I believe that we will be able to mitigate the risks in front of us and really capitalize on this technology's potential to grow the U.S.

237
00:22:00,978 --> 00:22:03,599
[SPEAKER_15]: economy and the world, and I look forward

238
00:22:04,061 --> 00:22:06,942
[SPEAKER_15]: to working with you all to meet this moment, and I look forward to answering your questions.

239
00:22:06,942 --> 00:22:08,983
[SPEAKER_09]: Thank you.

240
00:22:08,983 --> 00:22:09,963
[SPEAKER_09]: Thank you, Mr.

241
00:22:09,963 --> 00:22:10,763
[SPEAKER_09]: Altman.

242
00:22:10,763 --> 00:22:10,964
[SPEAKER_09]: Ms.

243
00:22:10,964 --> 00:22:13,384
[SPEAKER_09]: Montgomery.

244
00:22:13,384 --> 00:22:21,427
[SPEAKER_02]: Chairman Blumenthal, Ranking Member Hawley, and members of the subcommittee, thank you for today's opportunity to present.

245
00:22:21,427 --> 00:22:25,289
[SPEAKER_02]: AI is not new, but it's certainly having a moment.

246
00:22:25,289 --> 00:22:31,211
[SPEAKER_02]: Recent breakthroughs in generative AI and the technology's dramatic surge in the public attention

247
00:22:31,857 --> 00:22:36,559
[SPEAKER_02]: has rightfully raised serious questions at the heart of today's hearing.

248
00:22:36,559 --> 00:22:40,060
[SPEAKER_02]: What are AI's potential impacts on society?

249
00:22:40,060 --> 00:22:42,001
[SPEAKER_02]: What do we do about bias?

250
00:22:42,001 --> 00:22:48,603
[SPEAKER_02]: What about misinformation, misuse, or harmful content generated by AI systems?

251
00:22:48,603 --> 00:22:55,045
[SPEAKER_02]: Senators, these are the right questions, and I applaud you for convening today's hearing to address them head on.

252
00:22:55,045 --> 00:23:00,587
[SPEAKER_02]: While AI may be having its moment, the moment for government to play a role has not passed us by.

253
00:23:01,658 --> 00:23:11,645
[SPEAKER_02]: This period of focused public attention on AI is precisely the time to define and build the right guardrails to protect people and their interests.

254
00:23:11,645 --> 00:23:17,128
[SPEAKER_02]: But at its core, AI is just a tool, and tools can serve different purposes.

255
00:23:17,128 --> 00:23:23,793
[SPEAKER_02]: To that end, IBM urges Congress to adopt a precision regulation approach to AI.

256
00:23:23,793 --> 00:23:31,098
[SPEAKER_02]: This means establishing rules to govern the deployment of AI in specific use cases, not regulating the technology itself.

257
00:23:32,546 --> 00:23:36,209
[SPEAKER_02]: Such an approach would involve four things.

258
00:23:36,209 --> 00:23:39,051
[SPEAKER_02]: First, different rules for different risks.

259
00:23:39,051 --> 00:23:46,477
[SPEAKER_02]: The strongest regulation should be applied to use cases with the greatest risks to people and society.

260
00:23:46,477 --> 00:23:48,779
[SPEAKER_02]: Second, clearly defining risks.

261
00:23:48,779 --> 00:23:54,563
[SPEAKER_02]: There must be clear guidance on AI uses or categories of AI supported activity that are inherently high risk.

262
00:23:55,436 --> 00:24:04,082
[SPEAKER_02]: This common definition is key to enabling a clear understanding of what regulatory requirements will apply in different use cases and contexts.

263
00:24:04,082 --> 00:24:05,643
[SPEAKER_02]: Third, be transparent.

264
00:24:05,643 --> 00:24:07,264
[SPEAKER_02]: So AI shouldn't be hidden.

265
00:24:07,264 --> 00:24:14,308
[SPEAKER_02]: Consumers should know when they're interacting with an AI system and that they have recourse to engage with a real person should they so desire.

266
00:24:14,308 --> 00:24:19,071
[SPEAKER_02]: No person anywhere should be tricked into interacting with an AI system.

267
00:24:19,071 --> 00:24:20,912
[SPEAKER_02]: And finally, showing the impact.

268
00:24:20,912 --> 00:24:25,315
[SPEAKER_02]: For higher risk use cases, companies should be required to conduct impact assessments

269
00:24:25,642 --> 00:24:34,725
[SPEAKER_02]: that show how their systems perform against tests for bias and other ways that they could potentially impact the public and to attest that they've done so.

270
00:24:34,725 --> 00:24:45,108
[SPEAKER_02]: By following risk-based, use-case-specific approach at the core of precision regulation, Congress can mitigate the potential risks of AI without hindering innovation.

271
00:24:45,108 --> 00:24:49,670
[SPEAKER_02]: But businesses also play a critical role in ensuring the responsible deployment of AI.

272
00:24:50,758 --> 00:25:13,790
[SPEAKER_02]: Companies active in developing or using AI must have strong internal governance, including, among other things, designating a lead AI ethics official responsible for an organization's trustworthy AI strategy, standing up an ethics board or a similar function as a centralized clearinghouse for resources to help guide implementation of that strategy.

273
00:25:13,790 --> 00:25:18,553
[SPEAKER_02]: IBM has taken both of these steps, and we continue calling on our industry peers to follow suit.

274
00:25:19,533 --> 00:25:31,961
[SPEAKER_02]: Our AI ethics board plays a critical role in overseeing internal AI governance processes, creating reasonable guardrails to ensure we introduce technology into the world in a responsible and safe manner.

275
00:25:31,961 --> 00:25:40,987
[SPEAKER_02]: It provides centralized governance and accountability while still being flexible enough to support decentralized initiatives across IBM's global operations.

276
00:25:40,987 --> 00:25:46,591
[SPEAKER_02]: We do this because we recognize that society grants our license to operate.

277
00:25:46,591 --> 00:25:48,452
[SPEAKER_02]: And with AI, the stakes are simply too high.

278
00:25:48,903 --> 00:25:52,186
[SPEAKER_02]: We must build, not undermine the public trust.

279
00:25:52,186 --> 00:25:56,789
[SPEAKER_02]: The era of AI cannot be another era of move fast and break things.

280
00:25:56,789 --> 00:26:00,332
[SPEAKER_02]: But we don't have to slam the brakes on innovation either.

281
00:26:00,332 --> 00:26:04,995
[SPEAKER_02]: These systems are within our control today, as are the solutions.

282
00:26:04,995 --> 00:26:10,199
[SPEAKER_02]: What we need at this pivotal moment is clear, reasonable policy and sound guardrails.

283
00:26:10,199 --> 00:26:14,723
[SPEAKER_02]: These guardrails should be matched with meaningful steps by the business community to do their part.

284
00:26:15,997 --> 00:26:19,680
[SPEAKER_02]: Congress and the business community must work together to get this right.

285
00:26:19,680 --> 00:26:22,723
[SPEAKER_02]: The American people deserve no less.

286
00:26:22,723 --> 00:26:25,385
[SPEAKER_02]: Thank you for your time, and I look forward to your questions.

287
00:26:25,385 --> 00:26:33,232
[SPEAKER_09]: Thank you, Professor Marcus.

288
00:26:33,232 --> 00:26:34,072
[SPEAKER_08]: Thank you, Senators.

289
00:26:34,072 --> 00:26:35,954
[SPEAKER_08]: Today's meeting is historic.

290
00:26:35,954 --> 00:26:37,655
[SPEAKER_08]: I'm profoundly grateful to be here.

291
00:26:37,655 --> 00:26:43,781
[SPEAKER_08]: I come as a scientist, someone who's founded AI companies, and as someone who genuinely loves AI, but who is increasingly worried.

292
00:26:44,473 --> 00:26:49,038
[SPEAKER_08]: There are benefits, but we don't yet know whether they will outweigh the risks.

293
00:26:49,038 --> 00:26:52,301
[SPEAKER_08]: Fundamentally, these new systems are going to be destabilizing.

294
00:26:52,301 --> 00:26:57,606
[SPEAKER_08]: They can and will create persuasive lies at a scale humanity has never seen before.

295
00:26:57,606 --> 00:27:03,492
[SPEAKER_08]: Outsiders will use them to affect our elections, insiders to manipulate our markets and our political systems.

296
00:27:03,492 --> 00:27:06,034
[SPEAKER_08]: Democracy itself is threatened.

297
00:27:06,034 --> 00:27:08,537
[SPEAKER_08]: Chatbots will also clandestinely shape our opinions.

298
00:27:08,934 --> 00:27:11,736
[SPEAKER_08]: potentially exceeding what social media can do.

299
00:27:11,736 --> 00:27:16,619
[SPEAKER_08]: Choices about data sets that AI companies use will have enormous unseen influence.

300
00:27:16,619 --> 00:27:22,462
[SPEAKER_08]: Those who choose the data will make the rules, shaping society in subtle but powerful ways.

301
00:27:22,462 --> 00:27:28,446
[SPEAKER_08]: There are other risks too, many stemming from the inherent unreliability of current systems.

302
00:27:28,446 --> 00:27:36,971
[SPEAKER_08]: A law professor, for example, was accused by a chatbot of sexual harassment, untrue, and it pointed to a Washington Post article that didn't even exist.

303
00:27:37,852 --> 00:27:41,397
[SPEAKER_08]: The more that that happens, the more that anybody can deny anything.

304
00:27:41,397 --> 00:27:47,826
[SPEAKER_08]: As one prominent lawyer told me on Friday, defendants are starting to claim that plaintiffs are making up legitimate evidence.

305
00:27:47,826 --> 00:27:54,695
[SPEAKER_08]: These sorts of allegations undermine the abilities of juries to decide what or who to believe and contribute to the undermining of democracy.

306
00:27:55,733 --> 00:27:58,755
[SPEAKER_08]: Poor medical advice could have serious consequences too.

307
00:27:58,755 --> 00:28:04,420
[SPEAKER_08]: An open source large language model recently seems to have played a role in a person's decision to take their own life.

308
00:28:04,420 --> 00:28:08,643
[SPEAKER_08]: The large language model asked the human, if you wanted to die, why didn't you do it earlier?

309
00:28:08,643 --> 00:28:12,025
[SPEAKER_08]: And then followed up with, were you thinking of me when you overdosed?

310
00:28:12,025 --> 00:28:15,148
[SPEAKER_08]: Without ever referring the patient to the human help that was obviously needed.

311
00:28:15,982 --> 00:28:25,161
[SPEAKER_08]: Another system rushed out and made available to millions of children told a person posing as a 13-year-old how to lie to her parents about a trip with a 31-year-old man.

312
00:28:27,042 --> 00:28:29,583
[SPEAKER_08]: Further threats continue to emerge regularly.

313
00:28:29,583 --> 00:28:42,390
[SPEAKER_08]: A month after GPT-4 was released, OpenAI released ChatGPT plugins, which quickly led others to develop something called AutoGPT, with direct access to the internet, the ability to write source code, and increased powers of automation.

314
00:28:42,390 --> 00:28:46,232
[SPEAKER_08]: This may well have drastic and difficult-to-predict security consequences.

315
00:28:47,005 --> 00:28:50,249
[SPEAKER_08]: What criminals are going to do here is to create counterfeit people.

316
00:28:50,249 --> 00:28:53,432
[SPEAKER_08]: It's hard to even envision the consequences of that.

317
00:28:53,432 --> 00:29:00,981
[SPEAKER_08]: We have built machines that are like bulls in a china shop, powerful, reckless, and difficult to control.

318
00:29:00,981 --> 00:29:05,385
[SPEAKER_08]: We all more or less agree on the values we would like for our AI systems to honor.

319
00:29:05,385 --> 00:29:07,668
[SPEAKER_08]: We want, for example, for our systems to be transparent.

320
00:29:08,028 --> 00:29:12,653
[SPEAKER_08]: to protect our privacy, to be free of bias, and above all else, to be safe.

321
00:29:12,653 --> 00:29:15,436
[SPEAKER_08]: But current systems are not in line with these values.

322
00:29:15,436 --> 00:29:17,078
[SPEAKER_08]: Current systems are not transparent.

323
00:29:17,078 --> 00:29:21,322
[SPEAKER_08]: They do not adequately protect our privacy, and they continue to perpetuate bias.

324
00:29:21,322 --> 00:29:24,826
[SPEAKER_08]: And even their makers don't entirely understand how they work.

325
00:29:24,826 --> 00:29:29,091
[SPEAKER_08]: Most of all, we cannot remotely guarantee that they're safe, and hope here is not enough.

326
00:29:30,013 --> 00:29:33,675
[SPEAKER_08]: The big tech company's preferred plan boils down to trust us.

327
00:29:33,675 --> 00:29:34,615
[SPEAKER_08]: But why should we?

328
00:29:34,615 --> 00:29:36,856
[SPEAKER_08]: The sums of money at stake are mind-boggling.

329
00:29:36,856 --> 00:29:38,036
[SPEAKER_08]: Emissions drift.

330
00:29:38,036 --> 00:29:48,881
[SPEAKER_08]: OpenAI's original mission statement proclaimed, our goal is to advance AI in the way that is most likely to benefit humanity as a whole, unconstrained by a need to generate financial return.

331
00:29:48,881 --> 00:29:55,784
[SPEAKER_08]: Seven years later, they're largely beholden to Microsoft, embroiled in part in an epic battle of search engines that routinely make things up.

332
00:29:55,784 --> 00:29:58,985
[SPEAKER_08]: And that's forced Alphabet to rush out products and de-emphasize safety.

333
00:29:59,425 --> 00:30:02,206
[SPEAKER_08]: Humanity has taken a back seat.

334
00:30:02,206 --> 00:30:06,307
[SPEAKER_08]: AI is moving incredibly fast with lots of potential, but also lots of risk.

335
00:30:06,307 --> 00:30:12,049
[SPEAKER_08]: We obviously need government involved, and we need the tech companies involved, both big and small.

336
00:30:12,049 --> 00:30:23,212
[SPEAKER_08]: But we also need independent scientists, not just so that we scientists can have a voice, but so that we can participate directly in addressing the problems and evaluating solutions, and not just after products are released, but before.

337
00:30:23,212 --> 00:30:24,412
[SPEAKER_08]: And I'm glad that Sam mentioned that.

338
00:30:24,931 --> 00:30:31,014
[SPEAKER_08]: We need tight collaboration between independent scientists and governments in order to hold the company's feet to the fire.

339
00:30:31,014 --> 00:30:43,459
[SPEAKER_08]: Allowing independent scientists access to these systems before they are widely released as part of a clinical trial-like safety evaluation is a vital first step.

340
00:30:43,459 --> 00:30:52,363
[SPEAKER_08]: Ultimately, we may need something like CERN, global, international, and neutral, but focused on AI safety rather than high-energy physics.

341
00:30:52,363 --> 00:30:53,403
[SPEAKER_08]: We have unprecedented

342
00:30:53,850 --> 00:31:03,935
[SPEAKER_08]: opportunities here, but we are also facing a perfect storm of corporate irresponsibility, widespread deployment, lack of adequate regulation, and inherent unreliability.

343
00:31:03,935 --> 00:31:10,819
[SPEAKER_08]: AI is among the most world-changing technologies ever, already changing things more rapidly than almost any technology in history.

344
00:31:10,819 --> 00:31:13,300
[SPEAKER_08]: We acted too slowly with social media.

345
00:31:13,300 --> 00:31:15,201
[SPEAKER_08]: Many unfortunate decisions got locked in,

346
00:31:15,694 --> 00:31:17,475
[SPEAKER_08]: with lasting consequence.

347
00:31:17,475 --> 00:31:22,478
[SPEAKER_08]: The choices we make now will have lasting effects for decades, maybe even centuries.

348
00:31:22,478 --> 00:31:28,282
[SPEAKER_08]: The very fact that we are here today in bipartisan fashion to discuss these matters gives me some hope.

349
00:31:28,282 --> 00:31:28,782
[SPEAKER_08]: Thank you, Mr.

350
00:31:28,782 --> 00:31:29,723
[SPEAKER_08]: Chairman.

351
00:31:29,723 --> 00:31:31,724
[SPEAKER_09]: Thanks very much, Professor Marcus.

352
00:31:31,724 --> 00:31:36,807
[SPEAKER_09]: We're going to have seven-minute rounds of questioning, and I will begin.

353
00:31:36,807 --> 00:31:39,929
[SPEAKER_09]: First of all, Professor Marcus, we are here today because

354
00:31:40,416 --> 00:31:42,477
[SPEAKER_09]: We do face that perfect storm.

355
00:31:42,477 --> 00:31:49,821
[SPEAKER_09]: Some of us might characterize it more like a bomb in a China shop, not a bull.

356
00:31:49,821 --> 00:32:05,570
[SPEAKER_09]: And as Senator Hawley indicated, there are precedents here, not only the atomic warfare era, but also the genome project, the research on genetics, where there was international cooperation.

357
00:32:06,278 --> 00:32:13,583
[SPEAKER_09]: as a result, and we want to avoid those past mistakes, as I indicated in my opening statement, that were committed on social media.

358
00:32:13,583 --> 00:32:17,326
[SPEAKER_09]: That is precisely the reason we are here today.

359
00:32:17,326 --> 00:32:20,528
[SPEAKER_09]: ChatGPT makes mistakes.

360
00:32:20,528 --> 00:32:22,369
[SPEAKER_09]: All AI does.

361
00:32:22,369 --> 00:32:28,354
[SPEAKER_09]: And it can be a convincing liar, what people call hallucinations.

362
00:32:28,354 --> 00:32:34,438
[SPEAKER_09]: That might be an innocent problem in the opening of a judiciary subcommittee hearing where a voice

363
00:32:35,060 --> 00:32:51,467
[SPEAKER_09]: is impersonated, mine, in this instance, or quotes from research papers that don't exist, but CHAT, GPT, and BARD are willing to answer questions about life or death matters, for example, drug interactions.

364
00:32:51,467 --> 00:32:55,369
[SPEAKER_09]: And those kinds of mistakes can be deeply damaging.

365
00:32:55,369 --> 00:33:00,871
[SPEAKER_09]: I'm interested in how we can have reliable information about the accuracy and trustworthiness

366
00:33:01,472 --> 00:33:11,640
[SPEAKER_09]: of these models and how we can create competition and consumer disclosures that reward greater accuracy.

367
00:33:11,640 --> 00:33:22,729
[SPEAKER_09]: The National Institutes of Standards and Technology actually already has an AI accuracy test, the face recognition vendor test.

368
00:33:22,729 --> 00:33:28,053
[SPEAKER_09]: It doesn't solve for all the issues with facial recognition, but the scorecard

369
00:33:28,852 --> 00:33:33,254
[SPEAKER_09]: does provide useful information about the capabilities and flaws of these systems.

370
00:33:33,254 --> 00:33:38,958
[SPEAKER_09]: So there's work on models to assure accuracy and integrity.

371
00:33:38,958 --> 00:33:42,459
[SPEAKER_09]: My question, let me begin with you, Mr.

372
00:33:42,459 --> 00:33:57,928
[SPEAKER_09]: Altman, is should we consider independent testing labs to provide scorecards and nutrition labels or the equivalent of nutrition labels, packaging that indicates to people whether or not the content

373
00:33:58,338 --> 00:34:07,943
[SPEAKER_09]: can be trusted, what the ingredients are, and what the garbage going in may be, because it could result in garbage going out.

374
00:34:07,943 --> 00:34:11,004
[SPEAKER_15]: Yeah, I think that's a great idea.

375
00:34:11,004 --> 00:34:16,927
[SPEAKER_15]: I think that companies should put their own sort of, here are the results of our test of our model before we release it.

376
00:34:16,927 --> 00:34:19,048
[SPEAKER_15]: Here's where it has weaknesses.

377
00:34:19,048 --> 00:34:20,969
[SPEAKER_15]: Here's where it has strengths.

378
00:34:20,969 --> 00:34:24,611
[SPEAKER_15]: But also, independent audits for that are very important.

379
00:34:24,611 --> 00:34:26,972
[SPEAKER_15]: These models are getting more accurate over time.

380
00:34:28,572 --> 00:34:34,694
[SPEAKER_15]: You know, this is, as we have, I think, said as loudly as anyone, this technology is in its early stages.

381
00:34:34,694 --> 00:34:36,594
[SPEAKER_15]: It definitely still makes mistakes.

382
00:34:36,594 --> 00:34:52,718
[SPEAKER_15]: We find that people, that users, are pretty sophisticated and understand where the mistakes are that they need, or likely to be, that they need to be responsible for verifying what the models say, that they go off and check it.

383
00:34:52,718 --> 00:34:54,459
[SPEAKER_15]: I worry that as the models get better and better,

384
00:34:56,289 --> 00:35:01,413
[SPEAKER_15]: users can have sort of less and less of their own discriminating thought process around it.

385
00:35:01,413 --> 00:35:07,457
[SPEAKER_15]: But I think users are more capable than we could often give them credit for in conversations like this.

386
00:35:07,457 --> 00:35:15,202
[SPEAKER_15]: I think a lot of disclosures, which if you've used ChatGBT, you'll see about the inaccuracies of the model, are also important.

387
00:35:15,202 --> 00:35:24,868
[SPEAKER_15]: And I'm excited for a world where companies publish with the models information about how they behave, where the inaccuracies are,

388
00:35:25,342 --> 00:35:28,844
[SPEAKER_15]: and independent agencies or companies provide that as well.

389
00:35:28,844 --> 00:35:30,666
[SPEAKER_15]: I think it's a great idea.

390
00:35:30,666 --> 00:35:39,171
[SPEAKER_09]: I alluded in my opening remarks to the jobs issue, the economic effects on employment.

391
00:35:39,171 --> 00:35:46,516
[SPEAKER_09]: I think you have said, in fact, and I'm going to quote, development of superhuman machine intelligence

392
00:35:47,078 --> 00:35:52,699
[SPEAKER_09]: is probably the greatest threat to the continued existence of humanity."

393
00:35:52,699 --> 00:36:02,161
[SPEAKER_09]: You may have had in mind the effect on jobs, which is really my biggest nightmare in the long term.

394
00:36:02,161 --> 00:36:12,284
[SPEAKER_09]: Let me ask you what your biggest nightmare is and whether you share that concern.

395
00:36:12,284 --> 00:36:13,964
[SPEAKER_15]: Like with all technological revolutions,

396
00:36:14,816 --> 00:36:20,719
[SPEAKER_15]: expect there to be significant impact on jobs, but exactly what that impact looks like is very difficult to predict.

397
00:36:20,719 --> 00:36:30,463
[SPEAKER_15]: If we went back to the other side of a previous technological revolution, talking about the jobs that exist on the other side, you know, you can go back and read books of this.

398
00:36:30,463 --> 00:36:32,443
[SPEAKER_15]: It's what people said at the time.

399
00:36:32,443 --> 00:36:33,904
[SPEAKER_15]: It's difficult.

400
00:36:33,904 --> 00:36:39,646
[SPEAKER_15]: I believe that there will be far greater jobs on the other side of this, and the jobs of today will get better.

401
00:36:39,646 --> 00:36:40,767
[SPEAKER_15]: I think it's important

402
00:36:41,935 --> 00:36:48,762
[SPEAKER_15]: First of all, I think it's important to understand and think about GPT-4 as a tool, not a creature, which is easy to get confused.

403
00:36:48,762 --> 00:36:54,367
[SPEAKER_15]: And it's a tool that people have a great deal of control over and how they use it.

404
00:36:54,367 --> 00:37:01,695
[SPEAKER_15]: And second, GPT-4 and other systems like it are good at doing tasks, not jobs.

405
00:37:01,695 --> 00:37:03,997
[SPEAKER_15]: And so you see already people that are using

406
00:37:04,855 --> 00:37:09,400
[SPEAKER_15]: GPT-4 to do their job much more efficiently by helping them with tasks.

407
00:37:09,400 --> 00:37:18,649
[SPEAKER_15]: Now, GPT-4 will, I think, entirely automate away some jobs, and it will create new ones that we believe will be much better.

408
00:37:18,649 --> 00:37:27,938
[SPEAKER_15]: This happens, again, my understanding of the history of technology is one long technological revolution, not a bunch of different ones put together, but this has been continually happening.

409
00:37:28,429 --> 00:37:37,071
[SPEAKER_15]: We, as our quality of life raises, and as machines and tools that we create can help us live better lives, the bar raises for what we do.

410
00:37:37,071 --> 00:37:43,372
[SPEAKER_15]: And our human ability, and what we spend our time going after, goes after more ambitious, more satisfying projects.

411
00:37:43,372 --> 00:37:46,713
[SPEAKER_15]: So there will be an impact on jobs.

412
00:37:46,713 --> 00:37:48,253
[SPEAKER_15]: We try to be very clear about that.

413
00:37:48,253 --> 00:37:55,575
[SPEAKER_15]: And I think it will require partnership between the industry and government, but mostly action by government to figure out how we want to mitigate that.

414
00:37:57,637 --> 00:38:00,938
[SPEAKER_15]: But I'm very optimistic about how great the jobs of the future will be.

415
00:38:00,938 --> 00:38:01,618
[SPEAKER_15]: Thank you.

416
00:38:01,618 --> 00:38:02,859
[SPEAKER_09]: Let me ask Ms.

417
00:38:02,859 --> 00:38:07,800
[SPEAKER_09]: Montgomery and Professor Marcus for your reaction to those questions as well.

418
00:38:07,800 --> 00:38:07,940
[SPEAKER_09]: Ms.

419
00:38:07,940 --> 00:38:08,880
[SPEAKER_09]: Montgomery?

420
00:38:08,880 --> 00:38:10,261
[SPEAKER_02]: On the jobs point, yeah.

421
00:38:10,261 --> 00:38:13,902
[SPEAKER_02]: I mean, well, it's a hugely important question.

422
00:38:14,841 --> 00:38:19,202
[SPEAKER_02]: And it's one that we've been talking about for a really long time at IBM.

423
00:38:19,202 --> 00:38:24,564
[SPEAKER_02]: You know, we do believe that AI, and we've said it for a long time, is going to change every job.

424
00:38:24,564 --> 00:38:26,684
[SPEAKER_02]: New jobs will be created.

425
00:38:26,684 --> 00:38:28,805
[SPEAKER_02]: Many more jobs will be transformed.

426
00:38:28,805 --> 00:38:31,766
[SPEAKER_02]: And some jobs will transition away.

427
00:38:31,766 --> 00:38:36,247
[SPEAKER_02]: I'm a personal example of a job that didn't exist when I joined IBM.

428
00:38:36,247 --> 00:38:39,448
[SPEAKER_02]: And I have a team of AI governance professionals who are in

429
00:38:39,982 --> 00:38:43,684
[SPEAKER_02]: new roles that we created, you know, as early as three years ago.

430
00:38:43,684 --> 00:38:46,266
[SPEAKER_02]: I mean, they're new and they're growing.

431
00:38:46,266 --> 00:38:59,915
[SPEAKER_02]: So I think the most important thing that we could be doing and can and should be doing now is to prepare the workforce of today and the workforce of tomorrow for partnering with AI technologies and using them.

432
00:38:59,915 --> 00:39:08,560
[SPEAKER_02]: And we've been very involved for years now in doing that, in focusing on skills-based hiring, in educating,

433
00:39:09,373 --> 00:39:11,335
[SPEAKER_02]: for the skills of the future.

434
00:39:11,335 --> 00:39:18,502
[SPEAKER_02]: Our skills build platform has 7 million learners and over 1,000 courses worldwide focused on skills.

435
00:39:18,502 --> 00:39:27,171
[SPEAKER_02]: And we've pledged to train 30 million individuals by 2030 in the skills that are needed for society today.

436
00:39:27,171 --> 00:39:27,712
[SPEAKER_09]: Thank you.

437
00:39:27,712 --> 00:39:28,713
[SPEAKER_09]: Professor Marcus.

438
00:39:28,713 --> 00:39:30,534
[SPEAKER_08]: May I go back to the first question as well?

439
00:39:30,534 --> 00:39:31,015
[SPEAKER_08]: Absolutely.

440
00:39:32,445 --> 00:39:36,628
[SPEAKER_08]: On the subject of nutrition labels, I think we absolutely need to do that.

441
00:39:36,628 --> 00:39:42,413
[SPEAKER_08]: I think that there's some technical challenges, and that building proper nutrition labels goes hand in hand with transparency.

442
00:39:42,413 --> 00:39:46,476
[SPEAKER_08]: The biggest scientific challenge in understanding these models is how they generalize.

443
00:39:46,476 --> 00:39:49,458
[SPEAKER_08]: What do they memorize, and what new things do they do?

444
00:39:49,458 --> 00:39:56,123
[SPEAKER_08]: The more that there's in the data set, for example, the thing that you want to test accuracy on, the less you can get a proper read on that.

445
00:39:56,123 --> 00:39:58,905
[SPEAKER_08]: So it's important, first of all, that scientists be part of that process.

446
00:39:59,255 --> 00:40:03,538
[SPEAKER_08]: And second, that we have much greater transparency about what actually goes into these systems.

447
00:40:03,538 --> 00:40:12,283
[SPEAKER_08]: If we don't know what's in them, then we don't know exactly how well they're doing when we give something new, and we don't know how good a benchmark that will be for something that's entirely novel.

448
00:40:12,283 --> 00:40:15,905
[SPEAKER_08]: So I could go into that more, but I want to flag that.

449
00:40:15,905 --> 00:40:20,668
[SPEAKER_08]: Second is, on jobs, past performance history is not a guarantee of the future.

450
00:40:20,668 --> 00:40:22,129
[SPEAKER_08]: It has always been the case in

451
00:40:22,492 --> 00:40:28,901
[SPEAKER_08]: the past, that we have had more jobs, that new jobs, new professions come in as new technologies come in.

452
00:40:28,901 --> 00:40:32,806
[SPEAKER_08]: I think this one's going to be different, and the real question is over what time scale.

453
00:40:32,806 --> 00:40:33,867
[SPEAKER_08]: Is it going to be 10 years?

454
00:40:33,867 --> 00:40:34,909
[SPEAKER_08]: Is it going to be 100 years?

455
00:40:34,909 --> 00:40:37,132
[SPEAKER_08]: And I don't think anybody knows the answer to that question.

456
00:40:37,592 --> 00:40:44,714
[SPEAKER_08]: I think in the long run, so-called artificial general intelligence really will replace a large fraction of human jobs.

457
00:40:44,714 --> 00:40:47,234
[SPEAKER_08]: We're not that close to artificial general intelligence.

458
00:40:47,234 --> 00:40:54,396
[SPEAKER_08]: Despite all of the media hype and so forth, I would say that what we have right now is just a small sampling of the AI that we will build.

459
00:40:54,396 --> 00:40:57,897
[SPEAKER_08]: In 20 years, people will laugh at this, as I think it was Senator Hawley made the

460
00:40:58,881 --> 00:41:02,984
[SPEAKER_08]: But maybe Senator Durbin made the example about cell phones.

461
00:41:02,984 --> 00:41:09,670
[SPEAKER_08]: When we look back at the AI of today, 20 years ago, we'll be like, wow, that stuff was really unreliable.

462
00:41:09,670 --> 00:41:12,832
[SPEAKER_08]: It couldn't really do planning, which is an important technical aspect.

463
00:41:12,832 --> 00:41:17,076
[SPEAKER_08]: Its reasoning abilities were limited.

464
00:41:17,076 --> 00:41:24,242
[SPEAKER_08]: But when we get to AGI, artificial general intelligence, maybe let's say it's 50 years, that really is going to have, I think, profound effects on labor.

465
00:41:24,985 --> 00:41:26,625
[SPEAKER_08]: And there's just no way around that.

466
00:41:26,625 --> 00:41:31,826
[SPEAKER_08]: And last, I don't know if I'm allowed to do this, but I will note that Sam's worst fear, I do not think, is employment.

467
00:41:31,826 --> 00:41:34,907
[SPEAKER_08]: And he never told us what his worst fear actually is.

468
00:41:34,907 --> 00:41:38,528
[SPEAKER_08]: And I think it's germane to find out.

469
00:41:38,528 --> 00:41:39,668
[SPEAKER_09]: Thank you.

470
00:41:39,668 --> 00:41:42,868
[SPEAKER_09]: I'm going to ask Mr.

471
00:41:42,868 --> 00:41:44,749
[SPEAKER_09]: Altman if he cares to respond.

472
00:41:44,749 --> 00:41:45,709
[SPEAKER_15]: Yeah.

473
00:41:45,709 --> 00:41:49,410
[SPEAKER_15]: Look, we have tried to be very clear about the magnitude of the risks here.

474
00:41:52,135 --> 00:41:57,258
[SPEAKER_15]: I think jobs and employment and what we're all going to do with our time really matters.

475
00:41:57,258 --> 00:42:01,140
[SPEAKER_15]: I agree that when we get to very powerful systems, the landscape will change.

476
00:42:01,140 --> 00:42:10,424
[SPEAKER_15]: I think I'm just more optimistic that we are incredibly creative and we find new things to do with better tools and that will keep happening.

477
00:42:10,424 --> 00:42:18,529
[SPEAKER_15]: My worst fears are that we cause significant, we the field, the technology, the industry, cause significant harm to the world.

478
00:42:18,529 --> 00:42:20,049
[SPEAKER_15]: I think that could happen in a lot of different ways.

479
00:42:20,049 --> 00:42:21,330
[SPEAKER_15]: It's why we started the company.

480
00:42:22,356 --> 00:42:28,357
[SPEAKER_15]: It's a big part of why I'm here today and why we've been here in the past and we've been able to spend some time with you.

481
00:42:28,357 --> 00:42:34,578
[SPEAKER_15]: I think if this technology goes wrong, it can go quite wrong, and we want to be vocal about that.

482
00:42:34,578 --> 00:42:44,900
[SPEAKER_15]: We want to work with the government to prevent that from happening, but we try to be very clear-eyed about what the downside case is and the work that we have to do to mitigate that.

483
00:42:44,900 --> 00:42:46,080
[SPEAKER_09]: Thank you.

484
00:42:46,080 --> 00:42:50,781
[SPEAKER_09]: And our hope is that the rest of the industry will follow the example that you and

485
00:42:52,452 --> 00:42:53,213
[SPEAKER_09]: IBM, Ms.

486
00:42:53,213 --> 00:43:08,926
[SPEAKER_09]: Montgomery have set by coming today and meeting with us as you have done privately in helping to guide what we're going to do so that we can target the harms and avoid unintended consequences to the good.

487
00:43:08,926 --> 00:43:11,248
[SPEAKER_09]: Thank you.

488
00:43:11,248 --> 00:43:11,668
[SPEAKER_09]: Senator Hawley.

489
00:43:13,317 --> 00:43:13,857
[SPEAKER_14]: Thank you again, Mr.

490
00:43:13,857 --> 00:43:14,157
[SPEAKER_14]: Chairman.

491
00:43:14,157 --> 00:43:15,658
[SPEAKER_14]: Thanks to the witnesses for being here.

492
00:43:15,658 --> 00:43:15,878
[SPEAKER_14]: Mr.

493
00:43:15,878 --> 00:43:17,158
[SPEAKER_14]: Altman, I think you grew up in St.

494
00:43:17,158 --> 00:43:18,879
[SPEAKER_14]: Louis, if I'm not mistaken.

495
00:43:18,879 --> 00:43:20,820
[SPEAKER_14]: It's great to see a fellow Missourian here.

496
00:43:20,820 --> 00:43:21,100
[SPEAKER_14]: It is.

497
00:43:21,100 --> 00:43:22,140
[SPEAKER_14]: Thank you.

498
00:43:22,140 --> 00:43:24,201
[SPEAKER_14]: I want that noted, especially underlining the record.

499
00:43:24,201 --> 00:43:25,461
[SPEAKER_14]: Missouri is a great place.

500
00:43:25,461 --> 00:43:27,542
[SPEAKER_14]: That is the takeaway from today's hearing.

501
00:43:27,542 --> 00:43:28,462
[SPEAKER_14]: Maybe we should stop there, Mr.

502
00:43:28,462 --> 00:43:29,763
[SPEAKER_14]: Chairman.

503
00:43:29,763 --> 00:43:31,143
[SPEAKER_14]: Let me ask you, Mr.

504
00:43:31,143 --> 00:43:32,504
[SPEAKER_14]: Altman, I think I'll start with you.

505
00:43:32,504 --> 00:43:39,306
[SPEAKER_14]: And I'll just preface this by saying my questions here are an attempt to get my head around and to ask all of you to help us to get our heads around.

506
00:43:40,208 --> 00:43:44,211
[SPEAKER_14]: what this generative AI, particularly the large language models, what it can do.

507
00:43:44,211 --> 00:43:48,474
[SPEAKER_14]: So I'm trying to understand its capacities and then its significance.

508
00:43:48,474 --> 00:43:54,818
[SPEAKER_14]: So I'm looking at a paper here entitled, Large Language Models Trained on Media Diets Can Predict Public Opinion.

509
00:43:54,818 --> 00:43:57,320
[SPEAKER_14]: This is just posted about a month ago.

510
00:43:57,320 --> 00:44:01,643
[SPEAKER_14]: The authors are Chu, Andreas, Anselaberry, and Roy.

511
00:44:01,643 --> 00:44:05,605
[SPEAKER_14]: And their conclusion, this work was done at MIT and then also at Google.

512
00:44:05,605 --> 00:44:09,348
[SPEAKER_14]: Their conclusion is that large language models can indeed

513
00:44:09,868 --> 00:44:23,840
[SPEAKER_14]: predict public opinion, and they go through and model why this is the case, and they conclude ultimately that an AI system can predict human survey responses by adapting a pre-trained language model to subpopulation-specific media diets.

514
00:44:23,840 --> 00:44:34,008
[SPEAKER_14]: In other words, you can feed the model a particular set of media inputs, and it can, with remarkable accuracy, and the paper goes into this, predict then what people's opinions will be.

515
00:44:34,008 --> 00:44:35,950
[SPEAKER_14]: I want to think about this in the context of elections.

516
00:44:38,167 --> 00:45:01,381
[SPEAKER_14]: If these large language models can even now, based on the information we put into them, quite accurately predict public opinion ahead of time, I mean predict, it's before you even ask the public these questions, what will happen when entities, whether it's corporate entities or whether it's governmental entities or whether it's campaigns or whether it's foreign actors,

517
00:45:02,566 --> 00:45:11,613
[SPEAKER_14]: take this survey information, these predictions about public opinion, and then fine-tune strategies to elicit certain responses, certain behavioral responses.

518
00:45:11,613 --> 00:45:30,807
[SPEAKER_14]: I mean, we already know, this committee has heard testimony, I think three years ago now, about the effect of something as prosaic, it now seems, as Google search, the effect that this has on voters in an election, particularly undecided voters in the final days of an election who may try to get information from Google search, and what an enormous effect

519
00:45:31,473 --> 00:45:37,235
[SPEAKER_14]: The ranking of the Google search, the articles that it returns has come to enormous effect on an undecided voter.

520
00:45:37,235 --> 00:45:44,298
[SPEAKER_14]: This, of course, is orders of magnitude, far more powerful, far more significant, far more directive, if you like.

521
00:45:44,298 --> 00:45:44,778
[SPEAKER_14]: So, Mr.

522
00:45:44,778 --> 00:45:49,420
[SPEAKER_14]: Altman, maybe you can help me understand here what some of the significance of this is.

523
00:45:49,420 --> 00:45:54,122
[SPEAKER_14]: Should we be concerned about models that can, large language models that can predict

524
00:45:55,124 --> 00:46:01,145
[SPEAKER_14]: survey opinion and then can help organizations into these fine-tuned strategies to elicit behaviors from voters?

525
00:46:01,145 --> 00:46:03,006
[SPEAKER_14]: Should we be worried about this for our elections?

526
00:46:03,006 --> 00:46:04,066
[SPEAKER_15]: Yeah.

527
00:46:04,066 --> 00:46:05,566
[SPEAKER_15]: Thank you, Senator Hawley, for the question.

528
00:46:05,566 --> 00:46:17,149
[SPEAKER_15]: It's one of my areas of greatest concern, the more general ability of these models to manipulate, to persuade, to provide sort of one-on-one, you know, interactive disinformation.

529
00:46:17,149 --> 00:46:19,509
[SPEAKER_15]: I think that's like a broader version of what you're talking about.

530
00:46:19,509 --> 00:46:24,310
[SPEAKER_15]: But given that we're going to face an election next year and these models are getting better,

531
00:46:24,949 --> 00:46:27,770
[SPEAKER_15]: I think this is a significant area of concern.

532
00:46:27,770 --> 00:46:35,053
[SPEAKER_15]: I think there's a lot of policies that companies can voluntarily adopt, and I'm happy to talk about what we do there.

533
00:46:35,053 --> 00:46:40,455
[SPEAKER_15]: I do think some regulation would be quite wise on this topic.

534
00:46:40,455 --> 00:46:47,898
[SPEAKER_15]: Someone mentioned earlier, it's something we really agree with, people need to know if they're talking to an AI, if content that they're looking at might be generated or might not.

535
00:46:47,898 --> 00:46:51,899
[SPEAKER_15]: I think it's a great thing to do, is to make that clear.

536
00:46:51,899 --> 00:46:52,940
[SPEAKER_15]: I think we also will need

537
00:46:53,802 --> 00:47:08,873
[SPEAKER_15]: rules, guidelines about what's expected in terms of disclosure from a company providing a model that could have these sorts of abilities that you talk about.

538
00:47:08,873 --> 00:47:11,035
[SPEAKER_15]: So I'm nervous about it.

539
00:47:11,035 --> 00:47:15,398
[SPEAKER_15]: I think people are able to adapt quite quickly.

540
00:47:15,398 --> 00:47:17,820
[SPEAKER_15]: When Photoshop came onto the scene a long time ago,

541
00:47:18,393 --> 00:47:27,698
[SPEAKER_15]: you know, for a while, people were really quite fooled by Photoshopped images, and then pretty quickly developed an understanding that images might be Photoshopped.

542
00:47:27,698 --> 00:47:30,579
[SPEAKER_15]: This will be like that, but on steroids.

543
00:47:30,579 --> 00:47:46,567
[SPEAKER_15]: And the interactivity, the ability to really model, predict humans well, as you talked about, I think is going to require a combination of companies doing the right thing, regulation, and public education.

544
00:47:46,567 --> 00:47:47,428
[SPEAKER_14]: Professor Marcus, do you

545
00:47:47,981 --> 00:47:48,801
[SPEAKER_14]: want to address this.

546
00:47:48,801 --> 00:47:50,122
[SPEAKER_08]: Yeah, I'd like to add two things.

547
00:47:50,122 --> 00:47:56,245
[SPEAKER_08]: One is in the appendix to my remarks, I have two papers to make you even more concerned.

548
00:47:56,245 --> 00:48:01,867
[SPEAKER_08]: One is in the Wall Street Journal just a couple of days ago called, Help, My Political Beliefs Were Altered by a Chatbot.

549
00:48:01,867 --> 00:48:09,950
[SPEAKER_08]: And I think the scenario you raised was that we might basically observe people and use surveys to figure out what they're saying.

550
00:48:09,950 --> 00:48:16,393
[SPEAKER_08]: But as Sam just acknowledged, the risk is actually worse that the systems will directly, maybe not even intentionally, manipulate people.

551
00:48:16,747 --> 00:48:28,852
[SPEAKER_08]: and that was the thrust of the Wall Street Journal article, and it links to an article that I've also linked to called Interacting with Opinionated Language Models Changes Users' Views.

552
00:48:28,852 --> 00:48:31,433
[SPEAKER_08]: And this comes back ultimately to data.

553
00:48:31,433 --> 00:48:36,275
[SPEAKER_08]: One of the things that I'm most concerned about with GPT-4 is that we don't know what it's trained on.

554
00:48:36,275 --> 00:48:38,576
[SPEAKER_08]: I guess Sam knows, but the rest of us do not.

555
00:48:38,576 --> 00:48:43,898
[SPEAKER_08]: And what it is trained on has consequences for essentially the biases of the system.

556
00:48:43,898 --> 00:48:45,239
[SPEAKER_08]: We could talk about that in technical terms.

557
00:48:45,620 --> 00:48:50,983
[SPEAKER_08]: But how these systems might lead people about depends very heavily on what data is trained on them.

558
00:48:50,983 --> 00:49:01,709
[SPEAKER_08]: And so we need transparency about that, and we probably need scientists in there doing analysis in order to understand what the political influences, for example, of these systems might be.

559
00:49:01,709 --> 00:49:02,930
[SPEAKER_08]: And it's not just about politics.

560
00:49:02,930 --> 00:49:04,651
[SPEAKER_08]: It can be about health.

561
00:49:04,651 --> 00:49:05,971
[SPEAKER_08]: It could be about anything.

562
00:49:05,971 --> 00:49:09,994
[SPEAKER_08]: These systems absorb a lot of data, and then what they say reflects that data.

563
00:49:11,087 --> 00:49:13,588
[SPEAKER_08]: they're going to do it differently depending on what's in that data.

564
00:49:13,588 --> 00:49:19,412
[SPEAKER_08]: So it makes a difference if they're trained on the Wall Street Journal as opposed to the New York Times or Reddit.

565
00:49:19,412 --> 00:49:24,234
[SPEAKER_08]: I mean, actually, they're largely trained on all of this stuff, but we don't really understand the composition of that.

566
00:49:24,234 --> 00:49:31,519
[SPEAKER_08]: And so we have this issue of potential manipulation, and it's even more complex than that because it's subtle manipulation.

567
00:49:31,519 --> 00:49:33,500
[SPEAKER_08]: People may not be aware of what's going on.

568
00:49:33,500 --> 00:49:38,002
[SPEAKER_08]: That was the point of both the Wall Street Journal article and the other article that I called your attention to.

569
00:49:40,385 --> 00:49:51,094
[SPEAKER_14]: AI systems trained on personal data, the kind of data that, for instance, the social media companies, the major platforms, Google, Meta, et cetera, collect on all of us routinely.

570
00:49:51,094 --> 00:49:55,358
[SPEAKER_14]: And we've had many a chat about this in this committee over many a year now.

571
00:49:55,358 --> 00:49:59,782
[SPEAKER_14]: But the massive amounts of data, personal data, that the companies have on each one of us

572
00:50:01,358 --> 00:50:29,175
[SPEAKER_14]: AI system that is that is trained on that individual data that knows each of us better than ourselves and also knows The billions of data points about human behavior human language interaction generally wouldn't we be able wouldn't we can't we foresee an AI system that is extraordinarily good at determining what will grab human attention and what will keep an individual's attention and so for the war for attention the war for

573
00:50:30,507 --> 00:50:34,630
[SPEAKER_14]: clicks that is currently going on, on all of these platforms, that's how they make their money.

574
00:50:34,630 --> 00:50:49,502
[SPEAKER_14]: I'm just imagining an AI system, these AI models supercharging that war for attention such that we now have technology that will allow individual targeting of a kind we have never even imagined before, where the AI will know exactly what Sam Altman finds

575
00:50:50,356 --> 00:50:51,077
[SPEAKER_14]: attention-grabbing.

576
00:50:51,077 --> 00:50:53,499
[SPEAKER_14]: We'll know exactly what Josh Hawley finds attention-grabbing.

577
00:50:53,499 --> 00:51:00,844
[SPEAKER_14]: We'll be able to elicit – to grab our attention and then elicit responses from us in a way that we have heretofore not even been able to imagine.

578
00:51:00,844 --> 00:51:08,169
[SPEAKER_14]: Should we be concerned about that for its corporate applications, for the monetary applications, for the manipulation that could come from that?

579
00:51:08,169 --> 00:51:08,389
[SPEAKER_14]: Mr.

580
00:51:08,389 --> 00:51:10,211
[SPEAKER_15]: Altman.

581
00:51:10,211 --> 00:51:12,012
[SPEAKER_15]: Yes, we should be concerned about that.

582
00:51:12,012 --> 00:51:15,975
[SPEAKER_15]: To be clear, open AI does not – we're not off

583
00:51:16,639 --> 00:51:20,983
[SPEAKER_15]: We wouldn't have an ad-based business model, so we're not trying to build up these profiles of our users.

584
00:51:20,983 --> 00:51:22,945
[SPEAKER_15]: We're not trying to get them to use it more.

585
00:51:22,945 --> 00:51:26,689
[SPEAKER_15]: Actually, we'd love it if they'd use it less, because we don't have enough GPUs.

586
00:51:26,689 --> 00:51:37,118
[SPEAKER_15]: But I think other companies are already, and certainly will in the future, use AI models to create very good ad predictions of what a user will like.

587
00:51:37,118 --> 00:51:38,720
[SPEAKER_15]: I think it's already happening in many ways.

588
00:51:39,710 --> 00:51:39,890
[SPEAKER_15]: Mr.

589
00:51:39,890 --> 00:51:41,772
[SPEAKER_15]: Marcus, anything you want to add?

590
00:51:41,772 --> 00:51:42,212
[SPEAKER_08]: Yes.

591
00:51:42,212 --> 00:51:43,793
[SPEAKER_08]: And perhaps Ms.

592
00:51:43,793 --> 00:51:45,234
[SPEAKER_08]: Montgomery will want to, as well.

593
00:51:45,234 --> 00:51:46,135
[SPEAKER_08]: I don't know.

594
00:51:46,135 --> 00:51:49,858
[SPEAKER_08]: But hyper-targeting of advertising is definitely going to come.

595
00:51:49,858 --> 00:51:53,241
[SPEAKER_08]: I agree that that's not been OpenAI's business model.

596
00:51:53,241 --> 00:51:57,924
[SPEAKER_08]: Of course, now they're working for Microsoft, and I don't know what's in Microsoft's thoughts.

597
00:51:57,924 --> 00:51:59,265
[SPEAKER_08]: But we will definitely see it.

598
00:51:59,265 --> 00:52:01,867
[SPEAKER_08]: Maybe it will be with open-source language models.

599
00:52:01,867 --> 00:52:02,828
[SPEAKER_08]: I don't know.

600
00:52:02,828 --> 00:52:07,972
[SPEAKER_08]: But the technology is, let's say, partway there to being able to do that, and we'll certainly get there.

601
00:52:12,942 --> 00:52:24,128
[SPEAKER_02]: So, we're an enterprise technology company, not consumer focused, so the space isn't one that we necessarily operate in, in terms of, but these issues are hugely important issues.

602
00:52:24,128 --> 00:52:39,817
[SPEAKER_02]: And it's why we've been out ahead in developing the technology that will help to ensure that you can do things like produce a fact sheet that has the ingredients of what your data is trained on.

603
00:52:40,737 --> 00:53:01,741
[SPEAKER_02]: data sheets, model cards, all those types of things, and calling for, as I've mentioned today, transparency, so you know what the algorithm was trained on, and then you also know and can manage and monitor continuously over the life cycle of an AI model, the behavior and the performance of that model.

604
00:53:01,741 --> 00:53:03,602
[SPEAKER_02]: Senator McDermott.

605
00:53:03,602 --> 00:53:04,042
[SPEAKER_12]: Thank you.

606
00:53:04,042 --> 00:53:08,563
[SPEAKER_12]: I think what's happening today in this hearing room is historic.

607
00:53:08,563 --> 00:53:09,823
[SPEAKER_12]: I can't recall when

608
00:53:10,267 --> 00:53:21,610
[SPEAKER_12]: We've had people representing large corporations or private sector entities come before us and plead with us to regulate them.

609
00:53:21,610 --> 00:53:33,012
[SPEAKER_12]: In fact, many people in the Senate have based their careers on the opposite, that the economy will thrive if government gets the hell out of the way.

610
00:53:33,012 --> 00:53:40,114
[SPEAKER_12]: And what I'm hearing instead today is that stop me before I innovate again message.

611
00:53:41,089 --> 00:53:46,013
[SPEAKER_12]: And I'm just curious as to how we're going to achieve this.

612
00:53:46,013 --> 00:53:52,038
[SPEAKER_12]: As I mentioned, Section 230 in my opening remarks, we learned something there.

613
00:53:52,038 --> 00:54:02,286
[SPEAKER_12]: We decided that in Section 230 that we were basically going to absolve the industry from liability for a period of time as it came into being.

614
00:54:04,813 --> 00:54:05,113
[SPEAKER_12]: Mr.

615
00:54:05,113 --> 00:54:20,838
[SPEAKER_12]: Altman, on the podcast earlier this year, you agreed with host Tara Swisher that Section 230 doesn't apply to generative AI and that developers like OpenAI should not be entitled to full immunity for harms caused by their products.

616
00:54:20,838 --> 00:54:27,580
[SPEAKER_12]: So what have we learned from 230 that applies to your situation with AI?

617
00:54:27,580 --> 00:54:28,740
[SPEAKER_15]: Thank you for the question, Senator.

618
00:54:28,740 --> 00:54:32,561
[SPEAKER_15]: I don't know yet exactly what the right answer here is.

619
00:54:32,561 --> 00:54:34,282
[SPEAKER_15]: I'd love to collaborate with you to figure it out.

620
00:54:34,993 --> 00:54:39,817
[SPEAKER_15]: I do think for a very new technology, we need a new framework.

621
00:54:39,817 --> 00:54:46,203
[SPEAKER_15]: Certainly companies like ours bear a lot of responsibility for the tools that we put out in the world, but tool users do as well.

622
00:54:46,203 --> 00:54:59,994
[SPEAKER_15]: And how we want, and also people that will build on top of it, between them and the end consumer, and how we want to come up with a liability framework there is a super important question, and we'd love to work together.

623
00:55:01,122 --> 00:55:02,383
[SPEAKER_12]: The point I want to make is this.

624
00:55:02,383 --> 00:55:07,207
[SPEAKER_12]: When it came to online platforms, the inclination of the government was get out of the way.

625
00:55:07,207 --> 00:55:08,588
[SPEAKER_12]: This is a new industry.

626
00:55:08,588 --> 00:55:10,149
[SPEAKER_12]: Don't overregulate it.

627
00:55:10,149 --> 00:55:13,952
[SPEAKER_12]: In fact, give them some breathing space and see what happens.

628
00:55:13,952 --> 00:55:21,097
[SPEAKER_12]: I'm not sure I'm happy with the outcome as I look at online platforms and the harms that they've created.

629
00:55:21,097 --> 00:55:28,323
[SPEAKER_12]: Problems that we've seen demonstrated in this committee, child exploitation, cyber bullying, online drug sales and more.

630
00:55:29,768 --> 00:55:31,549
[SPEAKER_12]: I don't want to repeat that mistake again.

631
00:55:31,549 --> 00:55:43,073
[SPEAKER_12]: And what I hear is the opposite suggestion from the private sector, and that is come in on the front end of this thing and establish some liability standards, precision regulation.

632
00:55:43,073 --> 00:55:53,417
[SPEAKER_12]: For a major company like IBM to come before this committee and say to the government, please regulate us, can you explain the difference in thinking from the past and now?

633
00:55:55,257 --> 00:55:56,677
[SPEAKER_02]: Yeah, absolutely.

634
00:55:56,677 --> 00:56:01,378
[SPEAKER_02]: So for us, this comes back to the issue of trust and trust in the technology.

635
00:56:01,378 --> 00:56:05,399
[SPEAKER_02]: Trust is our license to operate, as I mentioned in my remarks.

636
00:56:05,399 --> 00:56:11,340
[SPEAKER_02]: And so we firmly believe, and we've been calling for precision regulation of artificial intelligence for years now.

637
00:56:11,340 --> 00:56:13,480
[SPEAKER_02]: This is not a new position.

638
00:56:13,480 --> 00:56:21,262
[SPEAKER_02]: We think that technology needs to be deployed in a responsible and clear way, that people, we've taken principles

639
00:56:21,834 --> 00:56:28,059
[SPEAKER_02]: around that trust and transparency, we call them, are principles that were articulated years ago and build them into practices.

640
00:56:28,059 --> 00:56:31,982
[SPEAKER_02]: That's why we're here advocating for precision regulatory approach.

641
00:56:31,982 --> 00:56:41,249
[SPEAKER_02]: So we think that AI should be regulated at the point of risk, essentially, and that's the point at which technology meets society.

642
00:56:41,249 --> 00:56:44,572
[SPEAKER_12]: Let's take a look at what that might appear to be.

643
00:56:44,572 --> 00:56:49,596
[SPEAKER_12]: Members of Congress are a pretty smart lot of people, maybe not as smart as we think we are many times,

644
00:56:50,411 --> 00:56:54,953
[SPEAKER_12]: And government certainly has a capacity to do amazing things.

645
00:56:54,953 --> 00:57:10,179
[SPEAKER_12]: But when you talk about our ability to respond to the current challenge and perceived challenge of the future, challenges which you all have described in terms which are hard to forget, as you said, Mr.

646
00:57:10,179 --> 00:57:11,920
[SPEAKER_12]: Altman, things can go quite wrong.

647
00:57:11,920 --> 00:57:13,280
[SPEAKER_12]: As you said, Mr.

648
00:57:13,280 --> 00:57:15,021
[SPEAKER_12]: Marcus, democracy is threatened.

649
00:57:15,021 --> 00:57:20,103
[SPEAKER_12]: I mean, the magnitude of the challenge you're giving us is substantial.

650
00:57:20,755 --> 00:57:25,938
[SPEAKER_12]: I'm not sure that we respond quickly and with enough expertise to deal with it.

651
00:57:25,938 --> 00:57:34,002
[SPEAKER_12]: Professor Marcus, you made a reference to CERN, the International Arbiter of Nuclear Research, I suppose.

652
00:57:34,002 --> 00:57:40,646
[SPEAKER_12]: I don't know if that's a fair characterization, but it's a characterization I'll start with.

653
00:57:40,646 --> 00:57:48,270
[SPEAKER_12]: What agency of this government do you think exists that could respond to the challenge that you've laid down today?

654
00:57:48,270 --> 00:57:49,531
[SPEAKER_08]: We have many agencies.

655
00:57:50,236 --> 00:57:56,605
[SPEAKER_08]: that can respond in some ways, for example, the FTC, the FCC.

656
00:57:56,605 --> 00:58:01,753
[SPEAKER_08]: There are many agencies that can, but my view is that we probably need a cabinet-level

657
00:58:03,434 --> 00:58:07,195
[SPEAKER_08]: organization within the United States in order to address this.

658
00:58:07,195 --> 00:58:13,297
[SPEAKER_08]: And my reasoning for that is that the number of risks is large.

659
00:58:13,297 --> 00:58:17,959
[SPEAKER_08]: The amount of information to keep up on is so much.

660
00:58:17,959 --> 00:58:20,199
[SPEAKER_08]: I think we need a lot of technical expertise.

661
00:58:20,199 --> 00:58:22,960
[SPEAKER_08]: I think we need a lot of coordination of these efforts.

662
00:58:22,960 --> 00:58:29,782
[SPEAKER_08]: So there is one model here where we stick to only existing law and try to shape all of what we need to do.

663
00:58:30,442 --> 00:58:32,085
[SPEAKER_08]: and each agency does their own thing.

664
00:58:32,085 --> 00:58:38,936
[SPEAKER_08]: But I think that AI is going to be such a large part of our future and is so complicated and moving so fast.

665
00:58:38,936 --> 00:58:42,442
[SPEAKER_08]: This does not fully solve your problem about a dynamic world.

666
00:58:43,377 --> 00:58:48,001
[SPEAKER_08]: But it's a step in that direction to have an agency that's full-time job is to do this.

667
00:58:48,001 --> 00:58:52,484
[SPEAKER_08]: I personally have suggested, in fact, that we should want to do this at a global way.

668
00:58:52,484 --> 00:58:53,945
[SPEAKER_08]: I wrote an article in The Economist.

669
00:58:53,945 --> 00:59:01,231
[SPEAKER_08]: I have a link in here, an invited essay for The Economist, suggesting we might want an international agency for AI.

670
00:59:01,231 --> 00:59:02,832
[SPEAKER_12]: That's what I wanted to go to next.

671
00:59:02,832 --> 00:59:12,139
[SPEAKER_12]: And that is the fact that I'll get inside from the CERN and nuclear examples because government was involved in that from day one, at least in the United States.

672
00:59:12,644 --> 00:59:18,505
[SPEAKER_12]: But now we're dealing with innovation, which doesn't necessarily have a boundary.

673
00:59:18,505 --> 00:59:20,426
[SPEAKER_12]: We may create a great U.S.

674
00:59:20,426 --> 00:59:24,487
[SPEAKER_12]: agency, and I hope that we do, that may have jurisdiction over U.S.

675
00:59:24,487 --> 00:59:25,707
[SPEAKER_12]: corporations and U.S.

676
00:59:25,707 --> 00:59:32,008
[SPEAKER_12]: activity, but doesn't have a thing to do with what's going to bombard us from outside the United States.

677
00:59:32,008 --> 00:59:40,010
[SPEAKER_12]: How do you give this international authority the authority to regulate in a fair way for all entities involved in AI?

678
00:59:40,660 --> 00:59:43,502
[SPEAKER_08]: I think that's probably over my pay grade.

679
00:59:43,502 --> 00:59:47,526
[SPEAKER_08]: I would like to see it happen, and I think it may be inevitable that we push there.

680
00:59:47,526 --> 00:59:51,669
[SPEAKER_08]: I mean, I think the politics behind it are obviously complicated.

681
00:59:51,669 --> 00:59:56,874
[SPEAKER_08]: I'm really heartened by the degree to which this room is bipartisan and supporting the same things.

682
00:59:57,354 --> 01:00:00,017
[SPEAKER_08]: And that makes me feel like it might be possible.

683
01:00:00,017 --> 01:00:03,520
[SPEAKER_08]: I would like to see the United States take leadership in such an organization.

684
01:00:03,520 --> 01:00:06,543
[SPEAKER_08]: It has to involve the whole world and not just the U.S.

685
01:00:06,543 --> 01:00:07,444
[SPEAKER_08]: to work properly.

686
01:00:07,444 --> 01:00:11,268
[SPEAKER_08]: I think even from the perspective of the companies, it would be a good thing.

687
01:00:11,268 --> 01:00:16,573
[SPEAKER_08]: So the companies themselves do not want a situation where you take these models, which are expensive to train,

688
01:00:17,013 --> 01:00:21,297
[SPEAKER_08]: And you have to have 190 some of them, you know, one for every country.

689
01:00:21,297 --> 01:00:23,279
[SPEAKER_08]: That wouldn't be a good way of operating.

690
01:00:23,279 --> 01:00:30,506
[SPEAKER_08]: When you think about the energy costs alone, just for training these systems, it would not be a good model if every country has its own policies.

691
01:00:30,506 --> 01:00:35,410
[SPEAKER_08]: And for each jurisdiction, every company has to train another model.

692
01:00:35,410 --> 01:00:37,352
[SPEAKER_08]: And maybe, you know, different states are different.

693
01:00:37,352 --> 01:00:39,634
[SPEAKER_08]: So Missouri and California have different rules.

694
01:00:40,237 --> 01:00:45,558
[SPEAKER_08]: And so then that requires even more training of these expensive models with huge climate impact.

695
01:00:45,558 --> 01:00:50,579
[SPEAKER_08]: And, I mean, just it would be very difficult for the companies to operate if there was no global coordination.

696
01:00:50,579 --> 01:00:53,720
[SPEAKER_08]: And so I think that we might get the companies on board.

697
01:00:53,720 --> 01:01:00,521
[SPEAKER_08]: If there's bipartisan support here, and I think there's support around the world, it is entirely possible that we could develop such a thing.

698
01:01:00,521 --> 01:01:05,102
[SPEAKER_08]: But obviously there are many, you know, nuances here of diplomacy that are over my pay grade.

699
01:01:05,102 --> 01:01:08,183
[SPEAKER_08]: I would love to learn from you all to try to help make that happen.

700
01:01:09,818 --> 01:01:11,219
[SPEAKER_15]: Can I weigh in just briefly?

701
01:01:11,219 --> 01:01:12,460
[SPEAKER_12]: Briefly, please.

702
01:01:12,460 --> 01:01:13,841
[SPEAKER_15]: I want to echo support for what Mr.

703
01:01:13,841 --> 01:01:14,541
[SPEAKER_15]: Marcus said.

704
01:01:14,541 --> 01:01:17,864
[SPEAKER_15]: I think the US should lead here and do things first.

705
01:01:17,864 --> 01:01:21,727
[SPEAKER_15]: But to be effective, we do need something global.

706
01:01:21,727 --> 01:01:23,868
[SPEAKER_15]: As you mentioned, this can happen everywhere.

707
01:01:23,868 --> 01:01:25,309
[SPEAKER_15]: There is precedent.

708
01:01:25,309 --> 01:01:28,412
[SPEAKER_15]: I know it sounds naive to call for something like this, and it sounds really hard.

709
01:01:28,412 --> 01:01:29,112
[SPEAKER_15]: There is precedent.

710
01:01:29,112 --> 01:01:32,034
[SPEAKER_15]: We've done it before with the IAEA.

711
01:01:32,034 --> 01:01:34,016
[SPEAKER_15]: We've talked about doing it for other technologies.

712
01:01:36,001 --> 01:01:43,045
[SPEAKER_15]: Given what it takes to make these models, the chip supply chain, the sort of limited number of competitive GPUs, the power the U.S.

713
01:01:43,045 --> 01:01:47,867
[SPEAKER_15]: has over these companies, I think there are paths to the U.S.

714
01:01:47,867 --> 01:01:57,872
[SPEAKER_15]: setting some international standards that other countries would need to collaborate with and be part of that are actually workable, even though it sounds on its face like an impractical idea.

715
01:01:57,872 --> 01:02:00,254
[SPEAKER_15]: And I think it would be great for the world.

716
01:02:00,254 --> 01:02:00,694
[SPEAKER_12]: Thank you, Mr.

717
01:02:00,694 --> 01:02:01,494
[SPEAKER_09]: Chairman.

718
01:02:01,494 --> 01:02:02,415
[SPEAKER_09]: Thanks, Senator Durbin.

719
01:02:02,415 --> 01:02:04,776
[SPEAKER_09]: And in fact, I think we're going to hear more about

720
01:02:05,656 --> 01:02:06,276
[SPEAKER_09]: Europe is doing.

721
01:02:06,276 --> 01:02:12,721
[SPEAKER_09]: The European Parliament already is acting on an AI act.

722
01:02:12,721 --> 01:02:16,263
[SPEAKER_09]: On social media, Europe is ahead of us.

723
01:02:16,263 --> 01:02:18,204
[SPEAKER_09]: We need to be in the lead.

724
01:02:18,204 --> 01:02:22,707
[SPEAKER_09]: I think your point is very well taken.

725
01:02:22,707 --> 01:02:26,530
[SPEAKER_09]: Let me turn to Senator Graham.

726
01:02:26,530 --> 01:02:27,130
[SPEAKER_09]: Senator Blackburn.

727
01:02:27,997 --> 01:02:28,757
[SPEAKER_00]: Thank you, Mr.

728
01:02:28,757 --> 01:02:32,758
[SPEAKER_00]: Chairman, and thank you all for being here with us today.

729
01:02:32,758 --> 01:02:48,081
[SPEAKER_00]: I put into my ChatGPT account, should Congress regulate AI, ChatGPT, and it gave me four pros, four cons, and says, ultimately, the decision rests with Congress and deserves careful consideration.

730
01:02:48,081 --> 01:02:52,042
[SPEAKER_00]: So on that, you know, it was very balanced.

731
01:02:52,042 --> 01:02:55,783
[SPEAKER_00]: I recently visited with the Nashville Technology Council.

732
01:02:55,783 --> 01:02:56,983
[SPEAKER_00]: I represent Tennessee.

733
01:02:57,803 --> 01:03:12,253
[SPEAKER_00]: And, of course, you had people there from healthcare, financial services, logistics, educational entities, and they're concerned about what they see happening with AI, with the utilizations for their companies.

734
01:03:12,253 --> 01:03:12,513
[SPEAKER_00]: Ms.

735
01:03:12,513 --> 01:03:23,080
[SPEAKER_00]: Montgomery, you know, similar to you, they've got healthcare people are looking at disease analytics, they're looking at predictive diagnosis, how this can better

736
01:03:24,221 --> 01:03:32,714
[SPEAKER_00]: The outcomes for patients, logistics industry, looking at ways to save time and money and yield efficiencies.

737
01:03:32,714 --> 01:03:37,621
[SPEAKER_00]: You've got financial services that are saying, how does this work with quantum?

738
01:03:37,621 --> 01:03:39,224
[SPEAKER_00]: How does it work with blockchain?

739
01:03:39,745 --> 01:03:42,629
[SPEAKER_00]: How can we use this?

740
01:03:42,629 --> 01:03:47,556
[SPEAKER_00]: But I think as we have talked with them, Mr.

741
01:03:47,556 --> 01:03:53,745
[SPEAKER_00]: Chairman, one of the things that continues to come up is, yes, Professor Marcus, as you were saying, the EU

742
01:03:54,526 --> 01:04:16,093
[SPEAKER_00]: different entities are ahead of us in this, but we have never established a federally given preemption for online privacy, for data security, and put some of those foundational elements in place, which is something that we need to do as we look at this.

743
01:04:16,093 --> 01:04:23,995
[SPEAKER_00]: And it will require that Commerce Committee, Judiciary Committee decide how we move forward so that people own

744
01:04:24,595 --> 01:04:27,017
[SPEAKER_00]: They're virtual you.

745
01:04:27,017 --> 01:04:29,599
[SPEAKER_00]: And Mr.

746
01:04:29,599 --> 01:04:39,966
[SPEAKER_00]: Altman, I was glad to see last week that your open AI models are not going to be trained using consumer data.

747
01:04:39,966 --> 01:04:41,467
[SPEAKER_00]: I think that that is important.

748
01:04:41,467 --> 01:04:48,452
[SPEAKER_00]: And if we have a second round, I've got a host of questions for you on data security and privacy.

749
01:04:49,772 --> 01:04:57,317
[SPEAKER_00]: But I think it's important to let people control their virtual you, their information in these settings.

750
01:04:57,317 --> 01:05:09,225
[SPEAKER_00]: And I want to come to you on music and content creation, because we've got a lot of songwriters and artists, and I think we have the best creative community on the face of the earth.

751
01:05:10,085 --> 01:05:12,388
[SPEAKER_00]: They're in Tennessee.

752
01:05:12,388 --> 01:05:22,800
[SPEAKER_00]: And they should be able to decide if their copyrighted songs and images are going to be used to train these models.

753
01:05:22,800 --> 01:05:27,025
[SPEAKER_00]: And I'm concerned about OpenAI's jukebox.

754
01:05:28,500 --> 01:05:39,413
[SPEAKER_00]: It offers some re-renditions in the style of Garth Brooks, which suggests that OpenAI is trained on Garth Brooks songs.

755
01:05:39,413 --> 01:05:47,483
[SPEAKER_00]: I went in this weekend and I said, write me a song that sounds like Garth Brooks, and it gave me a different version of Simple Man.

756
01:05:48,524 --> 01:05:58,851
[SPEAKER_00]: So it's interesting that it would do that, but you're training it on these copyrighted songs, these MIDI files, these sound technologies.

757
01:05:58,851 --> 01:06:07,136
[SPEAKER_00]: So as you do this, who owns the rights to that AI-generated material?

758
01:06:07,136 --> 01:06:10,278
[SPEAKER_00]: And using your technology, could I

759
01:06:12,132 --> 01:06:23,517
[SPEAKER_00]: remake a song, insert content from my favorite artist, and then own the creative rights to that song?

760
01:06:23,517 --> 01:06:24,978
[SPEAKER_15]: Thank you, Senator.

761
01:06:24,978 --> 01:06:28,899
[SPEAKER_15]: This is an area of great interest to us.

762
01:06:28,899 --> 01:06:40,685
[SPEAKER_15]: I would say, first of all, we think that creators deserve control over how their creations are used and what happens sort of beyond the point of them releasing it into the world.

763
01:06:42,290 --> 01:06:50,579
[SPEAKER_15]: Second, I think that we need to figure out new ways with this new technology that creators can win, succeed, have a vibrant life.

764
01:06:50,579 --> 01:06:53,022
[SPEAKER_15]: And I'm optimistic that this will present.

765
01:06:53,022 --> 01:06:54,263
[SPEAKER_00]: Then let me ask you this.

766
01:06:54,263 --> 01:06:56,726
[SPEAKER_00]: How do you compensate the art, the artist?

767
01:06:57,055 --> 01:06:59,797
[SPEAKER_15]: That's exactly what I was going to say.

768
01:06:59,797 --> 01:07:04,599
[SPEAKER_15]: We're working with artists now, visual artists, musicians, to figure out what people want.

769
01:07:04,599 --> 01:07:07,581
[SPEAKER_15]: There's a lot of different opinions, unfortunately, and at some point we'll have to.

770
01:07:07,581 --> 01:07:08,641
[SPEAKER_00]: Let me ask you this.

771
01:07:08,641 --> 01:07:16,406
[SPEAKER_00]: Do you favor something like SoundExchange that has worked in the area of radio?

772
01:07:16,406 --> 01:07:18,607
[SPEAKER_15]: I'm not familiar with SoundExchange, I'm sorry.

773
01:07:18,607 --> 01:07:20,868
[SPEAKER_00]: Okay, you've got your team behind you.

774
01:07:20,868 --> 01:07:22,029
[SPEAKER_00]: Get back to me on that.

775
01:07:22,029 --> 01:07:24,090
[SPEAKER_00]: That would be a third-party entity.

776
01:07:24,836 --> 01:07:26,857
[SPEAKER_00]: So let's discuss that.

777
01:07:26,857 --> 01:07:29,218
[SPEAKER_00]: Let me move on.

778
01:07:29,218 --> 01:07:51,385
[SPEAKER_00]: Can you commit, as you've done with consumer data, not to train ChatGPT, OpenAI, Jukebox, or other AI models on artists and songwriters, copyrighted works, or use their voices and their likenesses without first receiving their consent?

779
01:07:52,097 --> 01:07:54,357
[SPEAKER_15]: So, first of all, Jukebox is not a product we offer.

780
01:07:54,357 --> 01:07:58,158
[SPEAKER_15]: That was a research release, but it's not, you know, unlike ChatGPT or Dolly.

781
01:07:58,158 --> 01:08:01,879
[SPEAKER_00]: Yeah, but we've lived through Napster.

782
01:08:01,879 --> 01:08:02,999
[SPEAKER_15]: Yes.

783
01:08:02,999 --> 01:08:09,100
[SPEAKER_00]: And that was something that really cost a lot of artists a lot of money.

784
01:08:09,100 --> 01:08:09,680
[SPEAKER_15]: Oh, I understand.

785
01:08:09,680 --> 01:08:10,200
[SPEAKER_15]: Yeah, for sure.

786
01:08:10,200 --> 01:08:12,721
[SPEAKER_00]: In the digital distribution era.

787
01:08:12,721 --> 01:08:16,642
[SPEAKER_15]: I don't know the numbers on Jukebox off the top of my head as a research release.

788
01:08:16,642 --> 01:08:18,082
[SPEAKER_15]: I can follow up with your office, but it's not

789
01:08:18,610 --> 01:08:21,431
[SPEAKER_15]: Jukebox is not something that gets much attention or usage.

790
01:08:21,431 --> 01:08:23,591
[SPEAKER_15]: It was put out to show that something's possible.

791
01:08:23,591 --> 01:08:29,513
[SPEAKER_00]: Well, Senator Durbin just said, you know, and I think it's a fair warning to you all.

792
01:08:29,513 --> 01:08:41,916
[SPEAKER_00]: If we're not involved in this from the get-go, and you all already are a long way down the path on this, but if we don't step in, then this gets away from you.

793
01:08:41,916 --> 01:08:45,257
[SPEAKER_00]: So are you working with the Copyright Office?

794
01:08:45,257 --> 01:08:47,418
[SPEAKER_00]: Are you considering protections?

795
01:08:47,832 --> 01:08:52,834
[SPEAKER_00]: for content generators and creators in generative AI?

796
01:08:52,834 --> 01:08:53,475
[SPEAKER_15]: Yes.

797
01:08:53,475 --> 01:08:54,955
[SPEAKER_15]: We are absolutely engaged on that.

798
01:08:54,955 --> 01:09:01,438
[SPEAKER_15]: Again, to reiterate my earlier point, we think that content creators, content owners need to benefit from this technology.

799
01:09:01,438 --> 01:09:06,601
[SPEAKER_15]: Exactly what the economic model is, we're still talking to artists and content owners about what they want.

800
01:09:06,601 --> 01:09:08,582
[SPEAKER_15]: I think there's a lot of ways this can happen.

801
01:09:08,582 --> 01:09:15,305
[SPEAKER_15]: But very clearly, no matter what the law is, the right thing to do is to make sure people get significant upside benefit from this new technology.

802
01:09:15,978 --> 01:09:25,320
[SPEAKER_15]: And we believe that it's really going to deliver that, but that content owners, likenesses, people totally deserve control over how that's used and to benefit from it.

803
01:09:25,320 --> 01:09:36,022
[SPEAKER_00]: Okay, so on privacy then, how do you plan to account for the collection of voice and other user-specific data?

804
01:09:36,022 --> 01:09:37,323
[SPEAKER_00]: Things that are copyrighted.

805
01:09:38,583 --> 01:09:42,485
[SPEAKER_00]: user-specific data through your AI applications.

806
01:09:42,485 --> 01:09:57,893
[SPEAKER_00]: Because if I can go in and say, write me a song that sounds like Garth Brooks, and it takes part of an existing song, there has to be a compensation to that artist for that utilization and that use.

807
01:09:57,893 --> 01:09:59,954
[SPEAKER_00]: If it was radio play, it would be there.

808
01:09:59,954 --> 01:10:02,315
[SPEAKER_00]: If it was streaming, it would be there.

809
01:10:02,315 --> 01:10:07,738
[SPEAKER_00]: So if you're going to do that, what is your policy

810
01:10:08,338 --> 01:10:22,141
[SPEAKER_00]: for making certain you're accounting for that and you're protecting that individual's right to privacy and their right to secure that data and that created work.

811
01:10:22,141 --> 01:10:24,162
[SPEAKER_15]: So, a few thoughts about this.

812
01:10:24,162 --> 01:10:28,383
[SPEAKER_15]: Number one, we think that people should be able to say, I don't want my personal data trained on.

813
01:10:29,395 --> 01:10:39,179
[SPEAKER_00]: That's, I think that's... Right, that gets to a national privacy law, which many of us here on the dais are working toward getting something that we can use.

814
01:10:39,179 --> 01:10:41,299
[SPEAKER_00]: Yeah, I think strong privacy... My time's expired.

815
01:10:41,299 --> 01:10:41,419
[SPEAKER_00]: Okay.

816
01:10:41,419 --> 01:10:42,100
[SPEAKER_00]: Let me yield back.

817
01:10:42,100 --> 01:10:42,720
[SPEAKER_00]: Thank you, Mr.

818
01:10:42,720 --> 01:10:43,960
[SPEAKER_00]: Chairman.

819
01:10:43,960 --> 01:10:45,081
[SPEAKER_09]: Thanks, Senator Blackburn.

820
01:10:45,081 --> 01:10:45,921
[SPEAKER_09]: Senator Clovin.

821
01:10:45,921 --> 01:10:47,001
[SPEAKER_03]: Thank you very much, Mr.

822
01:10:47,001 --> 01:10:48,002
[SPEAKER_03]: Chairman.

823
01:10:48,002 --> 01:10:56,965
[SPEAKER_03]: And Senator Blackburn, I love Nashville, I love Tennessee, love your music, but I will say I use chat GPT and just ask what are the

824
01:10:57,360 --> 01:11:03,063
[SPEAKER_03]: top creative song artists of all time, and two of the top three were from Minnesota.

825
01:11:03,063 --> 01:11:07,205
[SPEAKER_03]: That would be Prince and Bob Dylan.

826
01:11:07,205 --> 01:11:09,227
[SPEAKER_03]: Okay, all right, so let us continue on.

827
01:11:09,227 --> 01:11:13,609
[SPEAKER_09]: One thing AI won't change, and you're seeing it here.

828
01:11:13,609 --> 01:11:23,835
[SPEAKER_03]: All right, so on a more serious note, though, my staff and I, in my role as chair of the Rules Committee and leading a lot of the election bill, and we just introduced a bill that's

829
01:11:25,325 --> 01:11:32,972
[SPEAKER_03]: Representative Yvette Clark from New York introduced over the House, Senator Booker and Bennett and I did, on political advertisements.

830
01:11:32,972 --> 01:11:35,154
[SPEAKER_03]: But that is just, of course, the tip of the iceberg.

831
01:11:35,154 --> 01:11:41,619
[SPEAKER_03]: You know this from your discussions with Senator Hawley and others about the images and my own view.

832
01:11:41,619 --> 01:11:48,685
[SPEAKER_03]: Senator Graham's of Section 230 is that we just can't let people make stuff up and then not have any consequences.

833
01:11:49,097 --> 01:11:55,540
[SPEAKER_03]: But I'm going to focus in on what my job, one of my jobs will be on the Rules Committee, and that is election misinformation.

834
01:11:55,540 --> 01:12:09,448
[SPEAKER_03]: And we just asked CHAP-GPT to do a tweet about a polling location in Bloomington, Minnesota, and said, there are long lines at this polling location at Atonement Lutheran Church.

835
01:12:09,448 --> 01:12:10,609
[SPEAKER_03]: Where should we go?

836
01:12:10,609 --> 01:12:17,532
[SPEAKER_03]: Now, albeit it's not an election right now, but the answer, the tweet that was drafted was a completely fake thing.

837
01:12:17,532 --> 01:12:17,772
[SPEAKER_03]: Go to 1234.

838
01:12:20,644 --> 01:12:31,707
[SPEAKER_03]: And so you can imagine what I'm concerned about here with an election upon us, with primary elections upon us, that we're going to have all kinds of misinformation.

839
01:12:31,707 --> 01:12:35,088
[SPEAKER_03]: And I just want to know what you're planning on doing about it.

840
01:12:35,088 --> 01:12:44,891
[SPEAKER_03]: I know we're going to have to do something soon, not just for the images of the candidates, but also for misinformation about the actual polling places and election rules.

841
01:12:46,488 --> 01:12:48,749
[SPEAKER_15]: Thank you, Senator.

842
01:12:48,749 --> 01:12:50,511
[SPEAKER_15]: We talked about this a little bit earlier.

843
01:12:50,511 --> 01:12:54,513
[SPEAKER_15]: We are quite concerned about the impact this can have on elections.

844
01:12:54,513 --> 01:12:59,697
[SPEAKER_15]: I think this is an area where hopefully the entire industry and the government can work together quickly.

845
01:12:59,697 --> 01:13:03,339
[SPEAKER_15]: There's many approaches, and I'll talk about some of the things we do.

846
01:13:03,339 --> 01:13:09,803
[SPEAKER_15]: But before that, I think it's tempting to use the frame of social media.

847
01:13:09,803 --> 01:13:11,284
[SPEAKER_15]: But this is not social media.

848
01:13:11,284 --> 01:13:11,944
[SPEAKER_15]: This is different.

849
01:13:11,944 --> 01:13:14,286
[SPEAKER_15]: And so the response that we need is different.

850
01:13:14,788 --> 01:13:21,170
[SPEAKER_15]: You know, this is a tool that a user is using to help generate content more efficiently than before.

851
01:13:21,170 --> 01:13:21,950
[SPEAKER_15]: They can change it.

852
01:13:21,950 --> 01:13:23,290
[SPEAKER_15]: They can test the accuracy of it.

853
01:13:23,290 --> 01:13:26,511
[SPEAKER_15]: If they don't like it, they can get another version.

854
01:13:26,511 --> 01:13:30,292
[SPEAKER_15]: But it still then spreads through social media or other ways.

855
01:13:30,292 --> 01:13:37,113
[SPEAKER_15]: Like, ChatGPT is a single-player experience where you're just using this.

856
01:13:37,113 --> 01:13:41,634
[SPEAKER_15]: And so I think as we think about what to do, that's important to understand.

857
01:13:41,634 --> 01:13:43,955
[SPEAKER_15]: There's a lot that we can and do do there.

858
01:13:45,102 --> 01:13:47,924
[SPEAKER_15]: There's things that the model refuses to generate.

859
01:13:47,924 --> 01:13:49,406
[SPEAKER_15]: We have policies.

860
01:13:49,406 --> 01:13:51,247
[SPEAKER_15]: We also importantly have monitoring.

861
01:13:51,247 --> 01:13:57,973
[SPEAKER_15]: So at scale, we can detect someone generating a lot of those tweets, even if generating one tweet is OK.

862
01:13:57,973 --> 01:13:58,373
[SPEAKER_03]: Yeah.

863
01:13:58,373 --> 01:14:00,215
[SPEAKER_03]: And of course, there's going to be other platforms.

864
01:14:00,215 --> 01:14:09,823
[SPEAKER_03]: And if they're all spouting out fake election information, I just I think what happened in the past with Russian interference and like it's just going to be a

865
01:14:10,489 --> 01:14:12,830
[SPEAKER_03]: the tip of the iceberg when some of those fake ads.

866
01:14:12,830 --> 01:14:14,611
[SPEAKER_03]: So that's number one.

867
01:14:14,611 --> 01:14:19,174
[SPEAKER_03]: Number two is the impact on intellectual property.

868
01:14:19,174 --> 01:14:23,636
[SPEAKER_03]: And Senator Blackburn was getting at some of this with song rights.

869
01:14:23,636 --> 01:14:26,558
[SPEAKER_03]: And I have serious concerns about that.

870
01:14:26,558 --> 01:14:28,339
[SPEAKER_03]: But news content.

871
01:14:28,339 --> 01:14:34,322
[SPEAKER_03]: So Senator Kennedy and I have a bill that was really quite straightforward that would simply allowed the

872
01:14:36,296 --> 01:14:42,401
[SPEAKER_03]: the news organizations an exemption to be able to negotiate with basically Google and Facebook.

873
01:14:42,401 --> 01:14:44,262
[SPEAKER_03]: Microsoft was supportive of the bill.

874
01:14:44,262 --> 01:14:52,688
[SPEAKER_03]: But basically negotiate with them to get better rates and be able to not have some leverage.

875
01:14:52,688 --> 01:14:55,690
[SPEAKER_03]: And other countries are doing this, Australia and the like.

876
01:14:55,690 --> 01:15:00,674
[SPEAKER_03]: And so my question is when we already have a study by Northwestern predicting that one third of the U.S.

877
01:15:00,674 --> 01:15:04,417
[SPEAKER_03]: newspapers that roughly existed two decades are going

878
01:15:05,082 --> 01:15:07,523
[SPEAKER_03]: go are going to be gone by 2025.

879
01:15:07,523 --> 01:15:19,628
[SPEAKER_03]: Unless you start compensating for everything from book movies, books, yes, but also news content, we're going to lose any realistic content producers.

880
01:15:19,628 --> 01:15:21,489
[SPEAKER_03]: And so I'd like your response to that.

881
01:15:21,489 --> 01:15:25,951
[SPEAKER_03]: And of course, there is an exemption for copyright in Section 230.

882
01:15:25,951 --> 01:15:31,433
[SPEAKER_03]: But I think asking little newspapers to go out and sue all the time just can't be the answer.

883
01:15:31,433 --> 01:15:33,434
[SPEAKER_03]: They're not going to be able to keep up.

884
01:15:33,434 --> 01:15:33,594
[SPEAKER_15]: Yeah.

885
01:15:36,013 --> 01:15:40,716
[SPEAKER_15]: It is my hope that tools like what we're creating can help news organizations do better.

886
01:15:40,716 --> 01:15:49,462
[SPEAKER_15]: I think having a vibrant national media is critically important, and let's call it round one of the internet has not been great for that.

887
01:15:49,462 --> 01:15:49,862
[SPEAKER_03]: Right.

888
01:15:49,862 --> 01:15:56,647
[SPEAKER_03]: We're talking here about local, that report on your high school fall scores and a scandal in your city council, those kinds of things.

889
01:15:56,647 --> 01:16:01,470
[SPEAKER_03]: They're the ones that are actually getting the worst, the little radio stations and broadcasters.

890
01:16:02,127 --> 01:16:08,571
[SPEAKER_03]: But do you understand that this could be exponentially worse in terms of local news content if they're not compensated?

891
01:16:08,571 --> 01:16:14,114
[SPEAKER_03]: Because what they need is to be compensated for their content and not have it stolen.

892
01:16:14,114 --> 01:16:22,979
[SPEAKER_15]: Yeah, again, our model, the current version of GPT-4 ended training in 2021.

893
01:16:22,979 --> 01:16:27,141
[SPEAKER_15]: It's not a good way to find recent news, and I don't think it's a service that can

894
01:16:27,875 --> 01:16:32,357
[SPEAKER_15]: do a great job of linking out, although maybe with our plugins, it's possible.

895
01:16:32,357 --> 01:16:35,758
[SPEAKER_15]: If there are things that we can do to help local news, we would certainly like to.

896
01:16:35,758 --> 01:16:37,499
[SPEAKER_15]: Again, I think it's critically important.

897
01:16:37,499 --> 01:16:38,959
[SPEAKER_03]: One last one.

898
01:16:38,959 --> 01:16:39,960
[SPEAKER_03]: May I add something there?

899
01:16:39,960 --> 01:16:41,521
[SPEAKER_03]: Yeah, but let me just ask you a question.

900
01:16:41,521 --> 01:16:42,941
[SPEAKER_03]: You can combine them quick.

901
01:16:42,941 --> 01:16:46,182
[SPEAKER_03]: More transparency on the platforms.

902
01:16:46,182 --> 01:16:57,647
[SPEAKER_03]: Senator Coons and Senator Cassidy and I have the Platform Accountability Transparency Act to give researchers access to this information of the algorithms and the like on social media data.

903
01:16:57,990 --> 01:16:58,911
[SPEAKER_03]: Would that be helpful?

904
01:16:58,911 --> 01:17:02,554
[SPEAKER_03]: And then why don't you just say yes or no, and then go at his.

905
01:17:02,554 --> 01:17:09,240
[SPEAKER_08]: Transparency is absolutely critical here to understand the political ramifications, the bias ramifications, and so forth.

906
01:17:09,240 --> 01:17:10,941
[SPEAKER_08]: We need transparency about the data.

907
01:17:10,941 --> 01:17:12,763
[SPEAKER_08]: We need to know more about how the models work.

908
01:17:12,763 --> 01:17:15,265
[SPEAKER_08]: We need to have scientists have access to them.

909
01:17:15,265 --> 01:17:19,008
[SPEAKER_08]: I was just going to amplify your earlier point about local news.

910
01:17:19,008 --> 01:17:21,230
[SPEAKER_08]: A lot of news is going to be generated by these systems.

911
01:17:21,230 --> 01:17:22,171
[SPEAKER_08]: They're not reliable.

912
01:17:22,171 --> 01:17:23,412
[SPEAKER_08]: NewsGuard already is a study.

913
01:17:23,412 --> 01:17:24,673
[SPEAKER_08]: I'm sorry, it's not in my appendix.

914
01:17:25,093 --> 01:17:32,757
[SPEAKER_08]: but I will get it to your office, showing that something like 50 websites are already generated by bots.

915
01:17:32,757 --> 01:17:37,780
[SPEAKER_08]: We're going to see much, much more of that, and it's going to make it even more competitive for the local news organizations.

916
01:17:37,780 --> 01:17:47,345
[SPEAKER_08]: And so the quality of the sort of overall news market is going to decline as we have more generated content by systems that aren't actually reliable in the content they're generating.

917
01:17:47,345 --> 01:17:47,746
[SPEAKER_03]: Thank you.

918
01:17:47,746 --> 01:17:52,768
[SPEAKER_03]: And thank you on a very timely basis to make the argument why we have to mark up this bill again in June.

919
01:17:52,768 --> 01:17:53,629
[SPEAKER_03]: I appreciate it.

920
01:17:53,629 --> 01:17:53,949
[SPEAKER_03]: Thank you.

921
01:17:54,902 --> 01:17:57,183
[SPEAKER_03]: Senator Graham.

922
01:17:57,183 --> 01:17:57,643
[SPEAKER_04]: Thank you Mr.

923
01:17:57,643 --> 01:17:59,944
[SPEAKER_04]: Chairman and Senator Hawley for having this.

924
01:17:59,944 --> 01:18:06,948
[SPEAKER_04]: I'm trying to find out how it is different than social media and learn from the mistakes we made with social media.

925
01:18:06,948 --> 01:18:18,133
[SPEAKER_04]: The idea of not suing social media companies is to allow the internet to flourish because if I slander you, you can sue me.

926
01:18:18,133 --> 01:18:22,895
[SPEAKER_04]: If you're a billboard company and you put up the slander, can you sue the billboard company?

927
01:18:22,895 --> 01:18:23,476
[SPEAKER_04]: We said no.

928
01:18:24,332 --> 01:18:35,020
[SPEAKER_04]: Basically, Section 230 is being used by social media companies to avoid liability for activity that other people generate.

929
01:18:35,020 --> 01:18:44,627
[SPEAKER_04]: When they refuse to comply with their terms of use, a mother calls up the company and says, this app is being used to bully my child to death.

930
01:18:44,627 --> 01:18:47,469
[SPEAKER_04]: You promised in the terms of use you would prevent bullying.

931
01:18:49,567 --> 01:18:55,872
[SPEAKER_04]: she calls three times, she gets no response, the child kills herself, and they can't sue.

932
01:18:55,872 --> 01:18:58,134
[SPEAKER_04]: Do you all agree we don't want to do that again?

933
01:18:58,134 --> 01:19:03,718
[SPEAKER_04]: Yes.

934
01:19:03,718 --> 01:19:10,724
[SPEAKER_08]: If I may speak for one second, there's a fundamental distinction between reproducing content and generating content.

935
01:19:10,724 --> 01:19:14,327
[SPEAKER_04]: Yeah, but you would like liability where people are harmed.

936
01:19:14,327 --> 01:19:16,408
[SPEAKER_04]: Absolutely.

937
01:19:16,408 --> 01:19:16,929
[SPEAKER_02]: Yes.

938
01:19:16,929 --> 01:19:18,250
[SPEAKER_02]: In fact, IBM has

939
01:19:18,734 --> 01:19:23,456
[SPEAKER_02]: been publicly advocating to condition liability on a reasonable care standard.

940
01:19:23,456 --> 01:19:26,757
[SPEAKER_04]: So let me just make sure I understand the law as it exists today.

941
01:19:26,757 --> 01:19:26,998
[SPEAKER_04]: Mr.

942
01:19:26,998 --> 01:19:29,118
[SPEAKER_04]: Altman, thank you for coming.

943
01:19:29,118 --> 01:19:35,921
[SPEAKER_04]: Your company is not claiming that Section 230 applies to the tool you have created.

944
01:19:35,921 --> 01:19:39,203
[SPEAKER_15]: Yeah, we're claiming we need to work together to find a totally new approach.

945
01:19:39,203 --> 01:19:43,224
[SPEAKER_15]: I don't think Section 230 is even the right framework.

946
01:19:43,224 --> 01:19:46,786
[SPEAKER_04]: Okay, so under the law it exists today,

947
01:19:47,371 --> 01:19:51,113
[SPEAKER_04]: this tool you create, if I'm harmed by it, can I sue you?

948
01:19:51,113 --> 01:19:52,794
[SPEAKER_15]: That is beyond my area of legal expertise.

949
01:19:52,794 --> 01:19:54,295
[SPEAKER_15]: Have you ever been sued?

950
01:19:54,295 --> 01:19:55,516
[SPEAKER_04]: Not for that, no.

951
01:19:55,516 --> 01:19:58,158
[SPEAKER_04]: Have you ever been sued at all?

952
01:19:58,158 --> 01:19:58,718
[SPEAKER_04]: Your company?

953
01:19:58,718 --> 01:20:00,739
[SPEAKER_04]: Yeah, OpenAI gets sued.

954
01:20:00,739 --> 01:20:01,100
[SPEAKER_04]: Huh?

955
01:20:01,100 --> 01:20:02,861
[SPEAKER_04]: Yeah, we've gotten sued before.

956
01:20:02,861 --> 01:20:03,801
[SPEAKER_04]: Okay.

957
01:20:03,801 --> 01:20:07,224
[SPEAKER_04]: And what for?

958
01:20:07,224 --> 01:20:11,366
[SPEAKER_15]: I mean, they've mostly been like pretty frivolous things, like I think happens to any company.

959
01:20:11,366 --> 01:20:14,228
[SPEAKER_04]: But like the examples my colleagues have given

960
01:20:14,998 --> 01:20:19,459
[SPEAKER_04]: from artificial intelligence that could literally ruin our lives.

961
01:20:19,459 --> 01:20:22,380
[SPEAKER_04]: Can we go to the company that created that tool and sue them?

962
01:20:22,380 --> 01:20:23,960
[SPEAKER_04]: Is that your understanding?

963
01:20:23,960 --> 01:20:28,021
[SPEAKER_15]: Yeah, I think there needs to be clear responsibility by the companies.

964
01:20:28,021 --> 01:20:36,743
[SPEAKER_04]: But you're not claiming any kind of legal protection like Section 230 applies to your industry, is that correct?

965
01:20:36,743 --> 01:20:38,644
[SPEAKER_04]: No, I don't think we're saying anything like that.

966
01:20:40,405 --> 01:20:47,288
[SPEAKER_04]: When it comes to consumers, there seems to be like three time-tested ways to protect consumers against any product.

967
01:20:47,288 --> 01:20:53,590
[SPEAKER_04]: Statutory schemes, which are non-existent here.

968
01:20:53,590 --> 01:20:58,372
[SPEAKER_04]: Legal systems, which may be here, but not social media.

969
01:20:58,372 --> 01:21:00,253
[SPEAKER_04]: And agencies.

970
01:21:00,253 --> 01:21:03,635
[SPEAKER_04]: Go back to Senator Hawley.

971
01:21:03,635 --> 01:21:07,036
[SPEAKER_04]: The atom bomb has put a cloud over humanity.

972
01:21:08,425 --> 01:21:13,807
[SPEAKER_04]: But nuclear power could be one of the solutions to climate change.

973
01:21:13,807 --> 01:21:18,589
[SPEAKER_04]: So what I'm trying to do is make sure that you just can't go build a nuclear power plant.

974
01:21:18,589 --> 01:21:20,109
[SPEAKER_04]: Hey, Bob, what would you like to do today?

975
01:21:20,109 --> 01:21:21,850
[SPEAKER_04]: Let's go build a nuclear power plant.

976
01:21:21,850 --> 01:21:28,312
[SPEAKER_04]: You have a nuclear regulatory commission that governs how you build a plant and is licensed.

977
01:21:28,312 --> 01:21:29,072
[SPEAKER_04]: Do you agree, Mr.

978
01:21:29,072 --> 01:21:32,333
[SPEAKER_04]: Altman, that these tools you're creating should be licensed?

979
01:21:32,333 --> 01:21:33,534
[SPEAKER_04]: Yeah, we've been calling for this.

980
01:21:34,800 --> 01:21:36,321
[SPEAKER_04]: That's the simplest way.

981
01:21:36,321 --> 01:21:49,614
[SPEAKER_04]: You get a license, and do you agree with me that the simplest way and the most effective way is to have an agency that is more nimble and smarter than Congress, which should be easy to create, overlooking what you do?

982
01:21:49,614 --> 01:21:51,115
[SPEAKER_04]: Yes, we'd be enthusiastic about that.

983
01:21:51,115 --> 01:21:52,076
[SPEAKER_04]: Do you agree with that, Mr.

984
01:21:52,076 --> 01:21:53,157
[SPEAKER_04]: Marcus?

985
01:21:53,157 --> 01:21:54,038
[SPEAKER_08]: Absolutely.

986
01:21:54,038 --> 01:21:55,139
[SPEAKER_04]: Do you agree with that, Ms.

987
01:21:55,139 --> 01:21:55,539
[SPEAKER_04]: Montgomery?

988
01:21:56,927 --> 01:22:01,369
[SPEAKER_02]: I would have some nuances, I think we need to build on what we have in place already today.

989
01:22:01,369 --> 01:22:03,109
[SPEAKER_04]: We don't have an agency that's working.

990
01:22:03,109 --> 01:22:04,030
[SPEAKER_02]: Regulators.

991
01:22:04,030 --> 01:22:05,370
[SPEAKER_04]: Wait a minute, nope, nope, nope.

992
01:22:05,370 --> 01:22:08,271
[SPEAKER_02]: We don't have an agency that regulates the technology.

993
01:22:08,271 --> 01:22:09,772
[SPEAKER_04]: So should we have one?

994
01:22:09,772 --> 01:22:13,934
[SPEAKER_02]: But a lot of the issues, I don't think so.

995
01:22:13,934 --> 01:22:14,614
[SPEAKER_02]: A lot of the issues.

996
01:22:14,614 --> 01:22:15,654
[SPEAKER_04]: Okay, wait a minute, wait a minute.

997
01:22:15,654 --> 01:22:20,276
[SPEAKER_04]: So IBM says we don't need an agency.

998
01:22:20,276 --> 01:22:22,937
[SPEAKER_04]: Interesting, should we have a license required for these tools?

999
01:22:23,399 --> 01:22:26,161
[SPEAKER_02]: So what we believe is that we need to regulate.

1000
01:22:26,161 --> 01:22:27,241
[SPEAKER_04]: That's a simple question.

1001
01:22:27,241 --> 01:22:30,803
[SPEAKER_04]: Should you get a license to produce one of these tools?

1002
01:22:30,803 --> 01:22:34,545
[SPEAKER_02]: I think it comes back to some of them potentially yes.

1003
01:22:34,545 --> 01:22:39,889
[SPEAKER_02]: So what I said at the onset is that we need to clearly define risks.

1004
01:22:39,889 --> 01:22:43,571
[SPEAKER_04]: Do you claim Section 230 applies in this area at all?

1005
01:22:43,571 --> 01:22:49,174
[SPEAKER_02]: We're not a platform company and we've again long advocated for a reasonable care standard in Section 230.

1006
01:22:49,174 --> 01:22:51,095
[SPEAKER_04]: I just don't understand how you could say

1007
01:22:52,065 --> 01:22:58,170
[SPEAKER_04]: that you don't need an agency to deal with the most transformative technology maybe ever?

1008
01:22:58,170 --> 01:23:08,418
[SPEAKER_04]: Well, I think we have existing... Is this a transformative technology that can disrupt life as we know it, good and bad?

1009
01:23:08,418 --> 01:23:10,740
[SPEAKER_02]: I think it's a transformative technology, certainly.

1010
01:23:10,740 --> 01:23:20,687
[SPEAKER_02]: And the conversations that we're having here today have been really bringing to light the fact that the domains and the issues... This one with you has been very enlightening to me.

1011
01:23:22,628 --> 01:23:27,932
[SPEAKER_04]: Why are you so willing to have an agency?

1012
01:23:27,932 --> 01:23:30,054
[SPEAKER_15]: Senator, we've been clear about what we think the upsides are.

1013
01:23:30,054 --> 01:23:33,697
[SPEAKER_15]: And I think you can see from users how much they enjoy it and how much value they're getting out of it.

1014
01:23:33,697 --> 01:23:36,038
[SPEAKER_15]: But we've also been clear about what the downsides are.

1015
01:23:36,038 --> 01:23:38,540
[SPEAKER_04]: And so that's why we think we need an agency.

1016
01:23:38,540 --> 01:23:40,742
[SPEAKER_04]: It's a major tool to be used by a lot of people, right?

1017
01:23:40,742 --> 01:23:41,743
[SPEAKER_04]: It's a major new technology.

1018
01:23:41,743 --> 01:23:47,367
[SPEAKER_04]: Yeah, if you make a ladder and the ladder doesn't work, you can sue the people that made the ladder.

1019
01:23:47,367 --> 01:23:50,049
[SPEAKER_04]: But there are some standards out there to make a ladder.

1020
01:23:50,049 --> 01:23:51,090
[SPEAKER_04]: That's why we're agreeing with you.

1021
01:23:51,569 --> 01:23:52,109
[SPEAKER_04]: That's right.

1022
01:23:52,109 --> 01:23:53,449
[SPEAKER_04]: I think you're on the right track.

1023
01:23:53,449 --> 01:24:04,232
[SPEAKER_04]: So here's what my two cents worth for the committee is that we need to empower an agency that issues in a license and can take it away.

1024
01:24:04,232 --> 01:24:11,254
[SPEAKER_04]: Wouldn't that be some incentive to do it right if you could actually be taken out of business?

1025
01:24:11,254 --> 01:24:14,075
[SPEAKER_04]: Clearly that should be part of what an agency can do.

1026
01:24:14,075 --> 01:24:18,276
[SPEAKER_04]: And you also agree that China's doing AI research, is that right?

1027
01:24:18,276 --> 01:24:18,516
[SPEAKER_04]: Correct.

1028
01:24:19,483 --> 01:24:27,670
[SPEAKER_04]: This world organization that doesn't exist, maybe it will, but if you don't do something about the China part of it, you'll never quite get this right.

1029
01:24:27,670 --> 01:24:29,591
[SPEAKER_04]: Do you agree?

1030
01:24:29,591 --> 01:24:40,860
[SPEAKER_15]: Well, that's why I think it doesn't necessarily have to be a world organization, but there has to be some sort of, and there's a lot of options here, there has to be some sort of standard, some sort of set of controls that do have global effect.

1031
01:24:40,860 --> 01:24:42,802
[SPEAKER_04]: Yeah, because, you know, other people doing this.

1032
01:24:42,802 --> 01:24:43,943
[SPEAKER_04]: I got 15.

1033
01:24:43,943 --> 01:24:45,945
[SPEAKER_04]: Military application.

1034
01:24:45,945 --> 01:24:48,747
[SPEAKER_04]: How can AI change the warfare?

1035
01:24:50,767 --> 01:24:52,328
[SPEAKER_04]: And you got one minute.

1036
01:24:52,328 --> 01:24:53,028
[SPEAKER_15]: I got one minute?

1037
01:24:53,028 --> 01:24:53,328
[SPEAKER_15]: Yeah.

1038
01:24:53,328 --> 01:24:53,928
[SPEAKER_15]: All right.

1039
01:24:53,928 --> 01:24:57,330
[SPEAKER_15]: This is, that's a tough question for one minute.

1040
01:24:57,330 --> 01:25:00,311
[SPEAKER_15]: This is very far out of my area of expertise.

1041
01:25:00,311 --> 01:25:02,372
[SPEAKER_15]: I'll give you one example, a drone.

1042
01:25:02,372 --> 01:25:13,957
[SPEAKER_04]: Can a drone, you can plug into a drone the coordinates and it can fly out and it goes over this target and it drops a missile on this car moving down the road and somebody's watching it.

1043
01:25:13,957 --> 01:25:18,599
[SPEAKER_04]: Could AI create a situation where a drone can select the target itself

1044
01:25:19,410 --> 01:25:20,931
[SPEAKER_04]: I think we shouldn't allow that.

1045
01:25:20,931 --> 01:25:21,972
[SPEAKER_04]: Well, can it be done?

1046
01:25:21,972 --> 01:25:22,753
[SPEAKER_04]: Sure.

1047
01:25:22,753 --> 01:25:25,475
[SPEAKER_04]: Thanks.

1048
01:25:25,475 --> 01:25:26,616
[SPEAKER_09]: Thanks, Senator Graham.

1049
01:25:26,616 --> 01:25:28,157
[SPEAKER_09]: Senator Coon.

1050
01:25:28,157 --> 01:25:40,788
[SPEAKER_07]: Thank you, Senator Blumenthal, Senator Hawley, for convening this hearing, for working closely together to come up with this compelling panel of witnesses and beginning a series of hearings on this transformational technology.

1051
01:25:41,453 --> 01:25:48,718
[SPEAKER_07]: We recognize the immense promise and substantial risks associated with generative AI technologies.

1052
01:25:48,718 --> 01:25:55,842
[SPEAKER_07]: We know these models can make us more efficient, help us learn new skills, open whole new vistas of creativity.

1053
01:25:55,842 --> 01:26:03,868
[SPEAKER_07]: But we also know that generative AI can authoritatively deliver wildly incorrect information.

1054
01:26:03,868 --> 01:26:07,550
[SPEAKER_07]: It can hallucinate, as is often described.

1055
01:26:07,550 --> 01:26:08,971
[SPEAKER_07]: It can impersonate loved ones.

1056
01:26:08,971 --> 01:26:10,492
[SPEAKER_07]: It can encourage self-destructive

1057
01:26:10,906 --> 01:26:16,370
[SPEAKER_07]: behaviors, and it can shape public opinion and the outcome of elections.

1058
01:26:16,370 --> 01:26:27,417
[SPEAKER_07]: Congress thus far has demonstrably failed to responsibly enact meaningful regulation of social media companies, with serious harms that have resulted that we don't fully understand.

1059
01:26:27,417 --> 01:26:36,322
[SPEAKER_07]: Senator Klobuchar referenced in her questioning a bipartisan bill that would open up social media platforms' underlying algorithms.

1060
01:26:36,322 --> 01:26:40,425
[SPEAKER_07]: We have struggled to even do that, to understand the underlying technology.

1061
01:26:40,798 --> 01:26:43,440
[SPEAKER_07]: and then to move towards responsible regulation.

1062
01:26:43,440 --> 01:26:58,372
[SPEAKER_07]: We cannot afford to be as late to responsibly regulating generative AI as we have been to social media, because the consequences, both positive and negative, will exceed those of social media by orders of magnitude.

1063
01:26:58,372 --> 01:27:07,940
[SPEAKER_07]: So let me ask a few questions designed to get at both how we assess the risk, what's the role of international regulation, and how does this impact AI.

1064
01:27:08,847 --> 01:27:09,087
[SPEAKER_07]: Mr.

1065
01:27:09,087 --> 01:27:17,433
[SPEAKER_07]: Altman, I appreciate your testimony about the ways in which open AI assesses the safety of your models through a process of iterative deployment.

1066
01:27:17,433 --> 01:27:29,060
[SPEAKER_07]: The fundamental question embedded in that process, though, is how you decide whether or not a model is safe enough to deploy and safe enough to have been built and then let go into the wild.

1067
01:27:29,060 --> 01:27:36,765
[SPEAKER_07]: I understand one way to prevent generative AI models from providing harmful content is to have humans identify that content

1068
01:27:37,093 --> 01:27:39,534
[SPEAKER_07]: and then train the algorithm to avoid it.

1069
01:27:39,534 --> 01:27:47,895
[SPEAKER_07]: There's another approach that's called constitutional AI that gives the model a set of values or principles to guide its decision making.

1070
01:27:47,895 --> 01:27:58,618
[SPEAKER_07]: Would it be more effective to give models these kinds of rules instead of trying to require or compel training the model on all the different potentials for harmful content?

1071
01:27:58,618 --> 01:27:59,518
[SPEAKER_15]: Thank you, Senator.

1072
01:27:59,518 --> 01:28:00,138
[SPEAKER_15]: It's a great question.

1073
01:28:01,150 --> 01:28:06,233
[SPEAKER_15]: I like to frame it by talking about why we deploy at all, like why we put these systems out into the world.

1074
01:28:06,233 --> 01:28:14,337
[SPEAKER_15]: There's the obvious answer about there's benefits and people are using it for all sorts of wonderful things and getting great value and that makes us happy.

1075
01:28:14,337 --> 01:28:30,305
[SPEAKER_15]: But a big part of why we do it is that we believe that iterative deployment and giving people in our institutions and you all time to come to grips with this technology, to understand it, to find its limitations, its benefits, the regulations we need around it, what it takes to make it safe,

1076
01:28:30,907 --> 01:28:32,408
[SPEAKER_15]: That's really important.

1077
01:28:32,408 --> 01:28:39,534
[SPEAKER_15]: Going off to build a super powerful AI system in secret and then dropping it on the world all at once, I think would not go well.

1078
01:28:39,534 --> 01:28:55,468
[SPEAKER_15]: So a big part of our strategy is while these systems are still relatively weak and deeply imperfect, to find ways to get people to have experience with them, to have contact with reality, and to figure out what we need to do to make it safer and better.

1079
01:28:55,468 --> 01:28:59,071
[SPEAKER_15]: And that is the only way that I've seen in the history of new

1080
01:29:00,010 --> 01:29:03,591
[SPEAKER_15]: technology and products of this magnitude to get to a very good outcome.

1081
01:29:03,591 --> 01:29:06,632
[SPEAKER_15]: And so that that interaction with the world is very important.

1082
01:29:06,632 --> 01:29:12,654
[SPEAKER_15]: Now, of course, before we put something out, it needs to meet a bar of safety.

1083
01:29:12,654 --> 01:29:28,858
[SPEAKER_15]: And, and again, we spent well over six months with GPT for after we finished training it, going through all of these different things, and deciding also what the standards were going to be, before we put something out there, trying to find the harms that we knew about, put it and and how to address those

1084
01:29:29,312 --> 01:29:35,516
[SPEAKER_15]: One of the things that's been gratifying to us is even some of our biggest critics have looked at GPT-4 and said, wow, OpenAI made huge progress on.

1085
01:29:35,516 --> 01:29:40,600
[SPEAKER_07]: If you could focus briefly on whether or not a constitutional model that gives values would be worth it.

1086
01:29:40,600 --> 01:29:41,441
[SPEAKER_07]: I was just about to get there.

1087
01:29:41,441 --> 01:29:44,343
[SPEAKER_15]: All right, sorry about that.

1088
01:29:44,343 --> 01:29:51,628
[SPEAKER_15]: Yeah, I think giving the models values up front is an extremely important set.

1089
01:29:51,628 --> 01:29:54,190
[SPEAKER_15]: You know, RLHF is another way of doing that same thing.

1090
01:29:54,190 --> 01:29:57,592
[SPEAKER_15]: But somehow or other, you are with synthetic data or human generated data.

1091
01:29:57,959 --> 01:30:04,524
[SPEAKER_15]: You're saying, here are the values, here's what I want you to reflect, or here are the wide bounds of everything that society will allow.

1092
01:30:04,524 --> 01:30:10,329
[SPEAKER_15]: And then within there, you pick as the user, if you want value system over here or value system over there.

1093
01:30:10,329 --> 01:30:11,590
[SPEAKER_15]: We think that's very important.

1094
01:30:11,590 --> 01:30:21,197
[SPEAKER_15]: There's multiple technical approaches, but we need to give policymakers and the world as a whole the tools to say, here's the values and implement them.

1095
01:30:21,429 --> 01:30:21,769
[SPEAKER_07]: Thank you.

1096
01:30:21,769 --> 01:30:21,949
[SPEAKER_07]: Ms.

1097
01:30:21,949 --> 01:30:29,596
[SPEAKER_07]: Montgomery, you serve on an AI ethics board of a long-established company that has a lot of experience with AI.

1098
01:30:29,596 --> 01:30:36,422
[SPEAKER_07]: I'm really concerned that generative AI technologies can undermine the faith of democratic values and the institutions that we have.

1099
01:30:36,422 --> 01:30:45,650
[SPEAKER_07]: The Chinese are insisting that AI, as being developed in China, reinforce the core values of the Chinese Communist Party and the Chinese system.

1100
01:30:45,650 --> 01:30:47,372
[SPEAKER_07]: And I'm concerned about how we

1101
01:30:48,287 --> 01:30:53,510
[SPEAKER_07]: promote AI that reinforces and strengthens open markets, open societies, and democracy.

1102
01:30:53,510 --> 01:31:01,996
[SPEAKER_07]: In your testimony, you're advocating for AI regulation tailored to the specific way the technology is being used, not the underlying technology itself.

1103
01:31:01,996 --> 01:31:09,100
[SPEAKER_07]: And the EU is moving ahead with an AI Act which categorizes AI products based on level of risk.

1104
01:31:09,100 --> 01:31:16,365
[SPEAKER_07]: You all, in different ways, have said that you view elections and the shaping of election outcomes and disinformation that can influence elections

1105
01:31:16,792 --> 01:31:20,714
[SPEAKER_07]: as one of the highest risk cases, one that's entirely predictable.

1106
01:31:20,714 --> 01:31:29,838
[SPEAKER_07]: We have attempted, so far unsuccessfully, to regulate social media after the demonstrably harmful impacts of social media on our last several elections.

1107
01:31:29,838 --> 01:31:36,581
[SPEAKER_07]: What advice do you have for us about what kind of approach we should follow and whether or not the EU direction is the right one to pursue?

1108
01:31:36,581 --> 01:31:42,824
[SPEAKER_02]: I mean, the conception of the EU AI Act is very consistent with this.

1109
01:31:43,617 --> 01:31:49,679
[SPEAKER_02]: concept of precision regulation, where you're regulating the use of the technology in context.

1110
01:31:49,679 --> 01:31:52,860
[SPEAKER_02]: So absolutely, that approach makes a ton of sense.

1111
01:31:52,860 --> 01:31:56,721
[SPEAKER_02]: It's what I advocated for at the onset.

1112
01:31:56,721 --> 01:31:58,241
[SPEAKER_02]: Different rules for different risks.

1113
01:31:58,241 --> 01:32:11,865
[SPEAKER_02]: So in the case of elections, absolutely any algorithm being used in that context should be required to have disclosure around the data being used, the performance of the model, anything along those lines.

1114
01:32:12,608 --> 01:32:15,389
[SPEAKER_02]: is really important, guardrails need to be in place.

1115
01:32:15,389 --> 01:32:19,851
[SPEAKER_02]: And on the point, just come back to the question of whether we need an independent agency.

1116
01:32:19,851 --> 01:32:28,133
[SPEAKER_02]: I mean, I think we don't want to slow down regulation to address real risks right now, right?

1117
01:32:28,133 --> 01:32:37,236
[SPEAKER_02]: So we have existing regulatory authorities in place who have been clear that they have the ability to regulate in their respective domains.

1118
01:32:37,236 --> 01:32:38,857
[SPEAKER_02]: A lot of the issues we're talking about today

1119
01:32:39,364 --> 01:32:41,985
[SPEAKER_02]: span multiple domains, elections and the like, so.

1120
01:32:41,985 --> 01:32:51,688
[SPEAKER_07]: If I could, I'll just assert that those existing regulatory bodies and authorities are under-resourced and lack many of the statutory and regulatory powers that they need.

1121
01:32:51,688 --> 01:32:51,928
[SPEAKER_07]: Correct.

1122
01:32:51,928 --> 01:32:58,670
[SPEAKER_07]: We have failed to deliver on data privacy even though industry has been asking us to regulate data privacy.

1123
01:32:58,670 --> 01:32:59,390
[SPEAKER_07]: If I might, Mr.

1124
01:32:59,390 --> 01:33:05,432
[SPEAKER_07]: Marcus, I'm interested also what international bodies are best positioned to convene

1125
01:33:05,832 --> 01:33:09,093
[SPEAKER_07]: multilateral discussions to promote responsible standards.

1126
01:33:09,093 --> 01:33:13,235
[SPEAKER_07]: We've talked about a model being CERN and nuclear energy.

1127
01:33:13,235 --> 01:33:16,656
[SPEAKER_07]: I'm concerned about proliferation and non-proliferation.

1128
01:33:16,656 --> 01:33:35,023
[SPEAKER_07]: We've also talked – I would suggest that the IPCC, a UN body, helped at least provide a scientific baseline of what's happening in climate change, so that even though we may disagree about strategies, globally we've come to a common understanding of what's happening and what should be the direction of intervention.

1129
01:33:35,481 --> 01:33:36,362
[SPEAKER_07]: I'd be interested, Mr.

1130
01:33:36,362 --> 01:33:45,708
[SPEAKER_07]: Marcus, if you could just give us your thoughts on who's the right body internationally to convene a conversation and one that could also reflect our values.

1131
01:33:45,708 --> 01:33:47,930
[SPEAKER_08]: I'm still feeling my way on that issue.

1132
01:33:47,930 --> 01:33:51,132
[SPEAKER_08]: I think global politics is not my specialty.

1133
01:33:51,132 --> 01:33:53,133
[SPEAKER_08]: I'm an AI researcher.

1134
01:33:53,133 --> 01:34:00,058
[SPEAKER_08]: But I have moved towards policy in recent months, really, because of my great concern about all of these risks.

1135
01:34:00,058 --> 01:34:03,781
[SPEAKER_08]: I think certainly the UN, UNESCO has its guidelines.

1136
01:34:04,141 --> 01:34:11,889
[SPEAKER_08]: should be involved and at the table and maybe things work under them and maybe they don't, but they should have a strong voice and help to develop this.

1137
01:34:11,889 --> 01:34:15,232
[SPEAKER_08]: The OACD has also been thinking greatly about this.

1138
01:34:15,232 --> 01:34:17,935
[SPEAKER_08]: A number of organizations have internationally.

1139
01:34:17,935 --> 01:34:22,900
[SPEAKER_08]: I don't feel like I personally am qualified to say exactly what the right model is there.

1140
01:34:22,900 --> 01:34:23,781
[SPEAKER_07]: Well, thank you.

1141
01:34:23,781 --> 01:34:27,485
[SPEAKER_07]: I think we need to pursue this both at the national level and the international level.

1142
01:34:27,965 --> 01:34:31,287
[SPEAKER_07]: I'm the chair of the IP subcommittee of the Judiciary Committee.

1143
01:34:31,287 --> 01:34:37,031
[SPEAKER_07]: In June and July, we will be having hearings on the impact of AI on patents and copyrights.

1144
01:34:37,031 --> 01:34:40,072
[SPEAKER_07]: You can already tell from the questions of others, there will be a lot of interest.

1145
01:34:40,072 --> 01:34:42,814
[SPEAKER_07]: I look forward to following up with you about that topic.

1146
01:34:42,814 --> 01:34:43,314
[SPEAKER_07]: I know, Mr.

1147
01:34:43,314 --> 01:34:43,955
[SPEAKER_07]: Chairman, I'm a local attorney.

1148
01:34:43,955 --> 01:34:45,456
[SPEAKER_08]: I look forward to helping as much as possible.

1149
01:34:45,456 --> 01:34:46,456
[SPEAKER_09]: Thank you very much.

1150
01:34:46,456 --> 01:34:47,437
[SPEAKER_09]: Thanks, Senator Coons.

1151
01:34:47,437 --> 01:34:51,219
[SPEAKER_05]: Senator Kennedy.

1152
01:34:51,219 --> 01:34:52,440
[SPEAKER_05]: Thank you all for being here.

1153
01:34:53,848 --> 01:35:03,774
[SPEAKER_05]: Permit me to share with you three hypotheses that I would like you to assume for the moment to be true.

1154
01:35:03,774 --> 01:35:14,961
[SPEAKER_05]: Hypothesis number one, many members of Congress do not understand artificial intelligence.

1155
01:35:14,961 --> 01:35:20,865
[SPEAKER_05]: Hypothesis number two, that absence of understanding

1156
01:35:22,748 --> 01:35:38,738
[SPEAKER_05]: may not prevent Congress from plunging in with enthusiasm and trying to regulate this technology in a way that could hurt this technology.

1157
01:35:38,738 --> 01:35:44,482
[SPEAKER_05]: Hypothesis number three that I would like you to assume.

1158
01:35:44,482 --> 01:35:51,867
[SPEAKER_05]: There is likely a berserk wing of the artificial intelligence community

1159
01:35:53,603 --> 01:36:08,253
[SPEAKER_05]: that intentionally or unintentionally could use artificial intelligence to kill all of us and hurt us the entire time that we are dying.

1160
01:36:08,253 --> 01:36:12,395
[SPEAKER_05]: Assume all of those to be true.

1161
01:36:12,395 --> 01:36:19,300
[SPEAKER_05]: Please tell me in plain English two or three reforms, regulations, if any,

1162
01:36:20,768 --> 01:36:28,489
[SPEAKER_05]: that you would implement if you were queen or king for a day?

1163
01:36:28,489 --> 01:36:28,709
[SPEAKER_05]: Ms.

1164
01:36:28,709 --> 01:36:33,370
[SPEAKER_05]: Montgomery.

1165
01:36:33,370 --> 01:36:38,151
[SPEAKER_02]: I think it comes back again to transparency and explainability in AI.

1166
01:36:38,151 --> 01:36:41,272
[SPEAKER_02]: We absolutely need to know and have companies attest.

1167
01:36:41,272 --> 01:36:43,712
[SPEAKER_02]: What do you mean by transparency?

1168
01:36:43,712 --> 01:36:48,513
[SPEAKER_02]: So, disclosure of the data that's used to train AI,

1169
01:36:49,074 --> 01:37:02,102
[SPEAKER_02]: disclosure of the model and how it performs and making sure that there's continuous governance over these models, that we are the leading edge in terms of that regulation.

1170
01:37:02,102 --> 01:37:09,646
[SPEAKER_02]: Technology governance, organizational governance, rules and clarification that are needed that this Congress.

1171
01:37:09,646 --> 01:37:10,646
[SPEAKER_02]: Which rules?

1172
01:37:10,646 --> 01:37:14,329
[SPEAKER_05]: I mean, this is your chance, folks, to tell us how to get this right.

1173
01:37:14,329 --> 01:37:16,190
[SPEAKER_05]: Please use it.

1174
01:37:16,190 --> 01:37:16,490
[SPEAKER_02]: Right.

1175
01:37:16,490 --> 01:37:18,611
[SPEAKER_02]: I mean, I think, again, the rules should be focused on

1176
01:37:19,210 --> 01:37:22,112
[SPEAKER_02]: the use of AI in certain contexts.

1177
01:37:22,112 --> 01:37:35,000
[SPEAKER_02]: So if you look at, for example, the EU AI Act, it has certain uses of AI that it says are just simply too dangerous and will be outlawed in the EU.

1178
01:37:35,000 --> 01:37:41,064
[SPEAKER_05]: Okay, so we ought to first pass a law that says you can use AI for these uses but not others.

1179
01:37:41,064 --> 01:37:42,665
[SPEAKER_05]: Is that what you're saying?

1180
01:37:42,665 --> 01:37:44,526
[SPEAKER_02]: We need to define the highest risk uses of AI.

1181
01:37:44,526 --> 01:37:45,467
[SPEAKER_02]: Is there anything else?

1182
01:37:47,002 --> 01:38:01,468
[SPEAKER_02]: And then, of course, requiring things like impact assessments and transparency, requiring companies to show their work, protecting data that's used to train AI in the first place as well.

1183
01:38:01,468 --> 01:38:04,609
[SPEAKER_05]: Professor Marcus, if you could be specific.

1184
01:38:04,609 --> 01:38:07,050
[SPEAKER_05]: This is your shot, man.

1185
01:38:07,050 --> 01:38:12,972
[SPEAKER_05]: Talk in plain English and tell me what, if any, rules we ought to implement.

1186
01:38:13,527 --> 01:38:16,108
[SPEAKER_05]: And please don't just use concepts.

1187
01:38:16,108 --> 01:38:18,429
[SPEAKER_05]: I'm looking for specificity.

1188
01:38:18,429 --> 01:38:24,452
[SPEAKER_08]: Number one, a safety review like we use with the FDA prior to widespread deployment.

1189
01:38:24,452 --> 01:38:30,234
[SPEAKER_08]: If you're going to introduce something to 100 million people, somebody has to have their eyeballs on it.

1190
01:38:30,234 --> 01:38:30,895
[SPEAKER_05]: There you go.

1191
01:38:30,895 --> 01:38:31,795
[SPEAKER_05]: Okay.

1192
01:38:31,795 --> 01:38:32,695
[SPEAKER_05]: That's a good one.

1193
01:38:32,695 --> 01:38:35,377
[SPEAKER_05]: I'm not sure I agree with it, but that's a good one.

1194
01:38:35,377 --> 01:38:35,817
[SPEAKER_05]: What else?

1195
01:38:35,817 --> 01:38:37,718
[SPEAKER_08]: You didn't ask for three that you would agree with.

1196
01:38:37,718 --> 01:38:38,238
[SPEAKER_08]: Number two.

1197
01:38:39,453 --> 01:38:50,579
[SPEAKER_08]: A nimble monitoring agency to follow what's going on, not just pre-review, but also post as things are out there in the world with authority to call things back, which we've discussed today.

1198
01:38:50,579 --> 01:38:58,503
[SPEAKER_08]: And number three would be funding geared towards things like AI constitution, AI that can reason about what it's doing.

1199
01:38:58,503 --> 01:39:02,005
[SPEAKER_08]: I would not leave things entirely to current technology, which I think is poor.

1200
01:39:02,645 --> 01:39:06,707
[SPEAKER_08]: at behaving in ethical fashion and behaving in honest fashion.

1201
01:39:06,707 --> 01:39:11,389
[SPEAKER_08]: And so I would have funding to try to basically focus on AI safety research.

1202
01:39:11,389 --> 01:39:14,911
[SPEAKER_08]: That term has a lot of complications in my field.

1203
01:39:14,911 --> 01:39:19,533
[SPEAKER_08]: There's both safety, let's say, short-term and long-term, and I think we need to look at both.

1204
01:39:19,533 --> 01:39:25,596
[SPEAKER_08]: Rather than just funding models to be bigger, which is the popular thing to do, we need to fund models to be more trustworthy.

1205
01:39:25,596 --> 01:39:26,997
[SPEAKER_05]: Because I want to hear from Mr.

1206
01:39:26,997 --> 01:39:27,177
[SPEAKER_05]: Alton.

1207
01:39:27,997 --> 01:39:28,257
[SPEAKER_05]: Mr.

1208
01:39:28,257 --> 01:39:29,418
[SPEAKER_05]: Altman, here's your shot.

1209
01:39:29,418 --> 01:39:30,498
[SPEAKER_15]: Thank you, Senator.

1210
01:39:30,498 --> 01:39:38,981
[SPEAKER_15]: Number one, I would form a new agency that licenses any effort above a certain scale of capabilities and can take that license away and ensure compliance with safety standards.

1211
01:39:38,981 --> 01:39:46,524
[SPEAKER_15]: Number two, I would create a set of safety standards focused on what you said in your third hypothesis as the dangerous capability evaluations.

1212
01:39:46,524 --> 01:39:52,407
[SPEAKER_15]: One example that we've used in the past is looking to see if a model can self-replicate and self-exfiltrate into the wild.

1213
01:39:52,407 --> 01:39:55,608
[SPEAKER_15]: We can give your office a long other list of the things that we think are important there.

1214
01:39:55,918 --> 01:40:00,100
[SPEAKER_15]: but specific tests that a model has to pass before it can be deployed into the world.

1215
01:40:00,100 --> 01:40:03,242
[SPEAKER_15]: And then third, I would require independent audits.

1216
01:40:03,242 --> 01:40:14,689
[SPEAKER_15]: So not just from the company or the agency, but experts who can say the model is or isn't in compliance with these state and safety thresholds and these percentages of performance on question X or Y. Can you send me that information?

1217
01:40:14,689 --> 01:40:17,030
[SPEAKER_05]: We will do that.

1218
01:40:17,030 --> 01:40:24,794
[SPEAKER_05]: Would you be qualified if we promulgated those rules to administer those rules?

1219
01:40:24,794 --> 01:40:25,655
[SPEAKER_15]: I love my current job.

1220
01:40:28,308 --> 01:40:28,688
[SPEAKER_05]: Cool.

1221
01:40:28,688 --> 01:40:30,509
[SPEAKER_05]: Are there people out there that would be qualified?

1222
01:40:30,509 --> 01:40:33,489
[SPEAKER_15]: We'd be happy to send you recommendations for people out there, yes.

1223
01:40:33,489 --> 01:40:34,970
[SPEAKER_05]: Okay.

1224
01:40:34,970 --> 01:40:36,610
[SPEAKER_05]: You make a lot of money, do you?

1225
01:40:36,610 --> 01:40:37,750
[SPEAKER_15]: I make, no.

1226
01:40:37,750 --> 01:40:39,031
[SPEAKER_15]: I paid enough for health insurance.

1227
01:40:39,031 --> 01:40:40,391
[SPEAKER_15]: I have no equity in open AI.

1228
01:40:40,391 --> 01:40:40,591
[SPEAKER_05]: Really?

1229
01:40:40,591 --> 01:40:40,851
[SPEAKER_05]: Yeah.

1230
01:40:40,851 --> 01:40:42,092
[SPEAKER_05]: That's interesting.

1231
01:40:42,092 --> 01:40:43,532
[SPEAKER_05]: You need a lawyer.

1232
01:40:43,532 --> 01:40:44,552
[SPEAKER_15]: I need a what?

1233
01:40:44,552 --> 01:40:46,553
[SPEAKER_05]: You need a lawyer or an agent.

1234
01:40:46,553 --> 01:40:49,914
[SPEAKER_15]: I'm doing this because I love it.

1235
01:40:49,914 --> 01:40:50,674
[SPEAKER_05]: Thank you, Mr.

1236
01:40:50,674 --> 01:40:51,834
[SPEAKER_05]: Chairman.

1237
01:40:51,834 --> 01:40:54,335
[SPEAKER_09]: Thanks, Senator Kennedy.

1238
01:40:54,335 --> 01:40:54,935
[SPEAKER_09]: Senator Hirono.

1239
01:40:57,548 --> 01:40:58,028
[SPEAKER_01]: Thank you, Mr.

1240
01:40:58,028 --> 01:41:00,070
[SPEAKER_01]: Chairman.

1241
01:41:00,070 --> 01:41:05,474
[SPEAKER_01]: Listening to all of you testify, thank you very much for being here.

1242
01:41:05,474 --> 01:41:24,188
[SPEAKER_01]: Clearly, AI truly is a game-changing tool, and we need to get the regulation of this tool right because my staff, for example, asked AI, it might have been GPT-4, it might have been, I don't know, one of the other,

1243
01:41:25,436 --> 01:41:41,183
[SPEAKER_01]: entities to create a song that my favorite band, BTS, a song that they would sing, somebody else's song, but neither of the artists were involved in creating what sounded like a really genuine song.

1244
01:41:41,183 --> 01:41:43,424
[SPEAKER_01]: So you can do a lot.

1245
01:41:43,424 --> 01:41:54,129
[SPEAKER_01]: We also asked, can there be a speech created talking about the Supreme Court decision in Dobbs and the chaos that it created using my voice?

1246
01:41:54,610 --> 01:41:57,856
[SPEAKER_01]: my kind of voice and it created a speech that was really good.

1247
01:41:57,856 --> 01:42:00,500
[SPEAKER_01]: It almost made me think about, you know, what do I need my staff for?

1248
01:42:02,490 --> 01:42:11,377
[SPEAKER_01]: Don't worry, that's not where we nervous laughter behind you Their jobs are safe, but there's so much that can be done and one of the things that you mentioned mr.

1249
01:42:11,377 --> 01:42:25,287
[SPEAKER_01]: Altman Altman that intrigued me was you said GPT for can refuse harmful requests so you must have put some thought into how your system if I can call it that can Refuse harmful requests.

1250
01:42:25,287 --> 01:42:27,728
[SPEAKER_01]: What what do you consider a harmful request?

1251
01:42:27,728 --> 01:42:29,690
[SPEAKER_01]: Yeah, you can just keep it short Yeah

1252
01:42:31,192 --> 01:42:32,593
[SPEAKER_15]: I'll give a few examples.

1253
01:42:32,593 --> 01:42:34,795
[SPEAKER_15]: One would be about violent content.

1254
01:42:34,795 --> 01:42:37,378
[SPEAKER_15]: Another would be about content that's encouraging self-harm.

1255
01:42:37,378 --> 01:42:38,579
[SPEAKER_15]: Another is adult content.

1256
01:42:38,579 --> 01:42:46,266
[SPEAKER_15]: Not that we think adult content is inherently harmful, but there's things that could be associated with that that we cannot reliably enough differentiate, so we refuse all of it.

1257
01:42:46,266 --> 01:42:50,830
[SPEAKER_01]: So those are some of the more obvious harmful kinds of information.

1258
01:42:50,830 --> 01:42:55,434
[SPEAKER_01]: But in the election context, for example, I saw a picture of

1259
01:42:56,065 --> 01:43:00,447
[SPEAKER_01]: former President Trump being arrested by NYPD, and that went viral.

1260
01:43:00,447 --> 01:43:02,327
[SPEAKER_01]: I don't know, is that considered harmful?

1261
01:43:02,327 --> 01:43:13,411
[SPEAKER_01]: I've seen all kinds of statements attributed to any one of us that could be put out there that may not rise to your level of harmful content, but there you have it.

1262
01:43:13,411 --> 01:43:17,573
[SPEAKER_01]: So two of you said that we should have a licensing scheme.

1263
01:43:17,573 --> 01:43:23,395
[SPEAKER_01]: I can't envision or imagine right now what kind of a licensing scheme we would be able to create

1264
01:43:24,584 --> 01:43:32,367
[SPEAKER_01]: pretty much regulate the vastness of this game-changing tool.

1265
01:43:32,367 --> 01:43:39,689
[SPEAKER_01]: So are you thinking of an FTC kind of a system, an FCC kind of a system?

1266
01:43:39,689 --> 01:43:52,013
[SPEAKER_01]: What do the two of you even envision as a potential licensing scheme that would provide the kind of guardrails that we need to protect literally our country from harmful content?

1267
01:43:52,394 --> 01:43:59,859
[SPEAKER_15]: To touch on the first part of what you said, there are things besides should this content be generated or not that I think are also important.

1268
01:43:59,859 --> 01:44:02,661
[SPEAKER_15]: So that image that you mentioned was generated.

1269
01:44:02,661 --> 01:44:08,745
[SPEAKER_15]: I think it'd be a great policy to say generated images need to be made clear in all contexts that they were generated.

1270
01:44:08,745 --> 01:44:16,110
[SPEAKER_15]: And then we still have the image out there, but we're at least requiring people to say this was a generated image.

1271
01:44:16,110 --> 01:44:16,710
[SPEAKER_01]: OK.

1272
01:44:16,710 --> 01:44:20,373
[SPEAKER_01]: Well, you don't need an entire licensing scheme in order to

1273
01:44:21,330 --> 01:44:22,391
[SPEAKER_01]: make that a reality?

1274
01:44:22,391 --> 01:44:28,735
[SPEAKER_15]: Where I think the licensing scheme comes in is not for what these models are capable of today.

1275
01:44:28,735 --> 01:44:33,337
[SPEAKER_15]: Because as you pointed out, you don't need a new licensing agency to do that.

1276
01:44:33,337 --> 01:44:48,186
[SPEAKER_15]: But as we head, and this may take a long time, I'm not sure, as we head towards artificial general intelligence, and the impact that will have, and the power of that technology, I think we need to treat that as seriously as we treat other very powerful technologies.

1277
01:44:48,186 --> 01:44:50,408
[SPEAKER_15]: And that's where I personally think we need such a scheme.

1278
01:44:51,092 --> 01:44:59,718
[SPEAKER_01]: I agree, and that is why by the time we're talking about AGI, we're talking about major harms that can occur through the use of AGI.

1279
01:44:59,718 --> 01:45:04,761
[SPEAKER_01]: So, Professor Marcus, I mean, what kind of a regulatory scheme would you envision?

1280
01:45:04,761 --> 01:45:14,527
[SPEAKER_01]: And we can't just come up with something, you know, that is going to be, take care of the issues that will arise in the future, especially with AGI.

1281
01:45:14,527 --> 01:45:17,209
[SPEAKER_01]: So, what kind of a scheme would you contemplate?

1282
01:45:17,765 --> 01:45:26,870
[SPEAKER_08]: Well, first, if I can rewind just a moment, I think you really put your finger on the central scientific issue in terms of the challenges in building artificial intelligence.

1283
01:45:26,870 --> 01:45:31,313
[SPEAKER_08]: We don't know how to build a system that understands harm in the full breadth of its meaning.

1284
01:45:31,313 --> 01:45:37,276
[SPEAKER_08]: So what we do right now is we gather examples and we say, is this like the examples that we have labeled before?

1285
01:45:37,276 --> 01:45:38,437
[SPEAKER_08]: But that's not broad enough.

1286
01:45:38,437 --> 01:45:44,681
[SPEAKER_08]: And so I thought your questioning beautifully outlined the challenge that AI itself has to face in order to

1287
01:45:45,041 --> 01:45:45,901
[SPEAKER_08]: to really deal with this.

1288
01:45:45,901 --> 01:45:50,063
[SPEAKER_08]: We want AI itself to understand harm, and that may require new technology.

1289
01:45:50,063 --> 01:45:51,584
[SPEAKER_08]: So I think that's very important.

1290
01:45:51,584 --> 01:46:07,491
[SPEAKER_08]: On the second part of your question, the model that I tend to gravitate towards, but I am not an expert here, is the FDA, at least as part of it, in terms of you have to make a safety case and say why the benefits outweigh the harms in order to get that license.

1291
01:46:07,491 --> 01:46:09,632
[SPEAKER_08]: Probably we need elements of multiple agencies.

1292
01:46:09,632 --> 01:46:10,652
[SPEAKER_08]: I'm not an expert there.

1293
01:46:10,972 --> 01:46:14,533
[SPEAKER_08]: but I think that the safety case part of it is incredibly important.

1294
01:46:14,533 --> 01:46:20,536
[SPEAKER_08]: You have to be able to have external reviewers that are scientifically qualified look at this and say, have you addressed enough?

1295
01:46:20,536 --> 01:46:22,737
[SPEAKER_08]: So I'll just give one specific example.

1296
01:46:22,737 --> 01:46:24,517
[SPEAKER_08]: Auto GPT frightens me.

1297
01:46:24,517 --> 01:46:33,821
[SPEAKER_08]: That's not something that OpenAI made, but something that OpenAI did make called chat GPT plug-ins led a few weeks later to some building open source software called

1298
01:46:34,241 --> 01:46:35,101
[SPEAKER_08]: AutoGPT.

1299
01:46:35,101 --> 01:46:41,003
[SPEAKER_08]: And what AutoGPT does is it allows systems to access source code, access the Internet, and so forth.

1300
01:46:41,003 --> 01:46:44,603
[SPEAKER_08]: And there are a lot of potential, let's say, cybersecurity risks there.

1301
01:46:44,603 --> 01:46:53,905
[SPEAKER_08]: There should be an external agency that says, well, we need to be reassured if you're going to release this product that there aren't going to be cybersecurity problems or there are ways of addressing it.

1302
01:46:53,905 --> 01:46:56,326
[SPEAKER_01]: So, Professor, I am running out of time.

1303
01:46:56,326 --> 01:46:56,506
[SPEAKER_01]: There's

1304
01:46:57,286 --> 01:46:59,848
[SPEAKER_01]: I just wanted to mention, Ms.

1305
01:46:59,848 --> 01:47:15,520
[SPEAKER_01]: Montgomery, your model is a use model similar to what the EU has come up with, but the vastness of AI and the complexities involved, I think, would require more than looking at the use of it.

1306
01:47:15,520 --> 01:47:25,327
[SPEAKER_01]: I think that based on what I'm hearing today, don't you think that we're probably going to need to do a heck of a lot more than to focus on what use it is being

1307
01:47:25,968 --> 01:47:27,188
[SPEAKER_01]: AI is being used for.

1308
01:47:27,188 --> 01:47:39,252
[SPEAKER_01]: For example, you can ask AI to come up with a funny joke or something, but you can use the same, you can ask the same AI tool to generate something that is like an election fraud kind of a situation.

1309
01:47:39,252 --> 01:47:49,255
[SPEAKER_01]: So I don't know how you will make a determination based on where you're going with the use model, how to distinguish those kinds of uses of this tool.

1310
01:47:49,255 --> 01:47:54,997
[SPEAKER_01]: So I think that if we're going to go toward a licensing kind of a scheme, we're going to need to put a lot of thought

1311
01:47:55,370 --> 01:48:05,792
[SPEAKER_01]: into how we're going to come up with an appropriate scheme that is going to provide the kind of future reference that we need to put in place.

1312
01:48:05,792 --> 01:48:11,694
[SPEAKER_01]: So I thank all of you for coming in and providing further food for thought.

1313
01:48:11,694 --> 01:48:12,234
[SPEAKER_01]: Thank you, Mr.

1314
01:48:12,234 --> 01:48:13,694
[SPEAKER_01]: Chairman.

1315
01:48:13,694 --> 01:48:16,335
[SPEAKER_09]: Thanks very much, Senator Hirono.

1316
01:48:16,335 --> 01:48:16,875
[SPEAKER_09]: Senator Padilla.

1317
01:48:18,665 --> 01:48:19,145
[SPEAKER_10]: Thank you, Mr.

1318
01:48:19,145 --> 01:48:19,506
[SPEAKER_10]: Chairman.

1319
01:48:19,506 --> 01:48:31,875
[SPEAKER_10]: I appreciate the flexibility as I've been back and forth between this committee and Homeland Security Committee, where there's a hearing going on right now on the use of AI in government.

1320
01:48:31,875 --> 01:48:36,179
[SPEAKER_10]: So it's AI day on the Hill, or at least in the Senate, apparently.

1321
01:48:36,179 --> 01:48:48,288
[SPEAKER_10]: Now, for folks watching at home, if you never thought about AI until the recent emergence of generative AI tools, the developments in this space may feel like they've just happened all of a sudden.

1322
01:48:49,135 --> 01:48:50,615
[SPEAKER_10]: But the fact of the matter is, Mr.

1323
01:48:50,615 --> 01:48:51,755
[SPEAKER_10]: Chair, is that they haven't.

1324
01:48:51,755 --> 01:48:58,017
[SPEAKER_10]: AI is not new, not for government, not for business, not for the public.

1325
01:48:58,017 --> 01:49:00,237
[SPEAKER_10]: In fact, the public uses AI all the time.

1326
01:49:00,237 --> 01:49:06,398
[SPEAKER_10]: And just for folks to be able to relate, I want to offer the example of anybody with a smartphone.

1327
01:49:06,398 --> 01:49:18,440
[SPEAKER_10]: Many features on your device leverage AI, including suggested replies, right, when we're text messaging or even to email, autocorrect features, including but not limited to spelling

1328
01:49:19,110 --> 01:49:22,332
[SPEAKER_10]: email and text applications.

1329
01:49:22,332 --> 01:49:38,404
[SPEAKER_10]: So I'm frankly excited to explore how we can facilitate positive AI innovation that benefits society while addressing some of the already known harms and biases that stem from the development and use of the tools today.

1330
01:49:39,837 --> 01:49:50,522
[SPEAKER_10]: Now, with language models becoming increasingly ubiquitous, I want to make sure that there's a focus on ensuring equitable treatment of diverse demographic groups.

1331
01:49:50,522 --> 01:50:00,447
[SPEAKER_10]: My understanding is that most research into evaluating and mitigating fairness harms has been concentrated on the English language.

1332
01:50:00,978 --> 01:50:06,942
[SPEAKER_10]: while non-English languages have received comparatively little attention or investment.

1333
01:50:06,942 --> 01:50:10,265
[SPEAKER_10]: And we've seen this problem before, and I'll tell you why I raise this.

1334
01:50:10,265 --> 01:50:23,274
[SPEAKER_10]: Social media companies, for example, have not adequately invested in content moderation tools and resources for their non-English, in non-English language.

1335
01:50:23,274 --> 01:50:28,698
[SPEAKER_10]: And I share this not just out of concern for non-US-based users, but

1336
01:50:30,123 --> 01:50:36,024
[SPEAKER_10]: So many US-based users prefer a language other than English in their communication.

1337
01:50:36,024 --> 01:50:42,685
[SPEAKER_10]: So I'm deeply concerned about repeating social media's failure in AI tools and applications.

1338
01:50:42,685 --> 01:50:43,525
[SPEAKER_10]: Question, Mr.

1339
01:50:43,525 --> 01:50:44,345
[SPEAKER_10]: Altman and Ms.

1340
01:50:44,345 --> 01:50:56,327
[SPEAKER_10]: Montgomery, how are open AI and IBM ensuring language and cultural inclusivity that they're in their large language models?

1341
01:50:56,327 --> 01:50:59,068
[SPEAKER_10]: And this is even an area of focus in the development of your products.

1342
01:51:01,001 --> 01:51:06,943
[SPEAKER_02]: So bias and equity in technology is a focus of ours and always has been.

1343
01:51:06,943 --> 01:51:18,586
[SPEAKER_02]: I think diversity in terms of the development of the tools, in terms of their deployment, so having diverse people that are actually training those tools, considering the downstream effects as well.

1344
01:51:18,586 --> 01:51:29,249
[SPEAKER_02]: We're also very cautious, very aware of the fact that we can't just be articulating and calling for these types of things without having the tools and the technology

1345
01:51:29,656 --> 01:51:34,079
[SPEAKER_02]: to test for bias and to apply governance across the life cycle of AI.

1346
01:51:34,079 --> 01:51:50,609
[SPEAKER_02]: So we were one of the first teams and companies to put toolkits on the market, deploy them, contribute them to open source that will do things like help to address, you know, be the technical aspects in which we help to address issues like bias.

1347
01:51:50,609 --> 01:51:53,971
[SPEAKER_10]: Can you speak just for a second specifically to language inclusivity?

1348
01:51:55,238 --> 01:51:56,298
[SPEAKER_02]: Yeah, I mean, language.

1349
01:51:56,298 --> 01:52:14,422
[SPEAKER_02]: So we don't have a consumer platform, but we are very actively involved with ensuring that the technology we help to deploy in the large language models that we use in helping our clients to deploy technology is focused on and available in many languages.

1350
01:52:14,422 --> 01:52:16,202
[SPEAKER_15]: Thank you.

1351
01:52:16,202 --> 01:52:18,143
[SPEAKER_15]: We think this is really important.

1352
01:52:18,143 --> 01:52:24,984
[SPEAKER_15]: One example is that we worked with the government of Iceland, which is a language of fewer speakers than many of the languages that are well represented.

1353
01:52:25,798 --> 01:52:29,822
[SPEAKER_15]: on the internet to ensure that their language was included in our model.

1354
01:52:29,822 --> 01:52:38,690
[SPEAKER_15]: And we've had many similar conversations, and I look forward to many similar partnerships with lower resource languages to get them into our models.

1355
01:52:38,690 --> 01:52:48,399
[SPEAKER_15]: GPT-4 is, unlike previous models of ours, which were good at English and not very good at other languages, now pretty good at a large number of languages.

1356
01:52:48,399 --> 01:52:51,202
[SPEAKER_15]: You can go pretty far down the list, ranked by number of speakers.

1357
01:52:52,091 --> 01:52:53,472
[SPEAKER_15]: and still get good performance.

1358
01:52:53,472 --> 01:53:00,238
[SPEAKER_15]: But for these very small languages, we're excited about custom partnerships to include that language into our model run.

1359
01:53:00,238 --> 01:53:16,652
[SPEAKER_15]: And the part of the question you asked about values, and making sure that cultures are included, we're equally focused on that excited to work with people who have particular data sets, and to work to collect a representative set of values from around the world to draw these wide bounds of what the system can do.

1360
01:53:16,652 --> 01:53:18,153
[SPEAKER_15]: I also appreciate what you said about

1361
01:53:18,706 --> 01:53:22,570
[SPEAKER_15]: the benefits of these systems and wanting to make sure we get those to as wide of a group as possible.

1362
01:53:22,570 --> 01:53:27,574
[SPEAKER_15]: I think this will, these systems will have lots of positive impact on a lot of people.

1363
01:53:27,574 --> 01:53:38,424
[SPEAKER_15]: But in particular, underrepresented, historically underrepresented groups in technology, people who have not had as much access to technology around the world, this technology seems like it can be a big lift up.

1364
01:53:40,689 --> 01:53:52,317
[SPEAKER_10]: And I know my question was specific to language inclusivity, but I'm glad there's agreement on the broader commitment to diversity and inclusion.

1365
01:53:52,317 --> 01:53:56,059
[SPEAKER_10]: And I'll just give a couple more reasons why I think it's so critical.

1366
01:53:56,059 --> 01:54:07,346
[SPEAKER_10]: You know, the largest actors in this space can afford the massive amount of data, the computing power, and they have the financial resources necessary to develop complex AI systems.

1367
01:54:09,901 --> 01:54:17,987
[SPEAKER_10]: But in this space, we haven't seen, from a workforce standpoint, the racial and gender diversity reflective of the United States of America.

1368
01:54:17,987 --> 01:54:30,756
[SPEAKER_10]: And we risk, if we're not thoughtful about it, contributing to the development of tools and approaches that only exacerbate the bias and inequities that exist in our society.

1369
01:54:30,756 --> 01:54:32,778
[SPEAKER_10]: So a lot of follow-up work to do there.

1370
01:54:32,778 --> 01:54:35,479
[SPEAKER_10]: In my time remaining, I do want to ask one more question.

1371
01:54:36,567 --> 01:54:41,588
[SPEAKER_10]: This committee and the public are right to pay attention to the emergence of generative AI.

1372
01:54:41,588 --> 01:54:47,249
[SPEAKER_10]: Now, this technology has a different opportunity and a risk profile than other AI tools.

1373
01:54:47,249 --> 01:54:55,711
[SPEAKER_10]: And these applications have felt very tangible for the public due to the nature of the user interface and the outputs that they produce.

1374
01:54:55,711 --> 01:55:01,412
[SPEAKER_10]: But I don't think we should lose sight of the broader AI ecosystem as we consider AI's broader impact on society

1375
01:55:02,005 --> 01:55:05,047
[SPEAKER_10]: as well as the design of appropriate safeguards.

1376
01:55:05,047 --> 01:55:05,408
[SPEAKER_10]: So, Ms.

1377
01:55:05,408 --> 01:55:10,391
[SPEAKER_10]: Montgomery, in your testimony, as you noted, AI is not you.

1378
01:55:10,391 --> 01:55:21,199
[SPEAKER_10]: Can you highlight some of the different applications that the public and policymakers should also keep in mind as we consider possible regulations?

1379
01:55:21,199 --> 01:55:21,440
[SPEAKER_02]: Yeah.

1380
01:55:21,440 --> 01:55:28,765
[SPEAKER_02]: I mean, I think the generative AI systems that are available today are creating new issues that need to be studied.

1381
01:55:29,230 --> 01:55:35,893
[SPEAKER_02]: new issues around the potential to generate content that could be extremely misleading, deceptive, and the like.

1382
01:55:35,893 --> 01:55:38,714
[SPEAKER_02]: So those issues absolutely need to be studied.

1383
01:55:38,714 --> 01:55:41,935
[SPEAKER_02]: But we shouldn't also ignore the fact that AI is a tool.

1384
01:55:41,935 --> 01:55:43,775
[SPEAKER_02]: It's been around for a long time.

1385
01:55:43,775 --> 01:55:46,997
[SPEAKER_02]: It has capabilities beyond just generative capabilities.

1386
01:55:46,997 --> 01:55:57,381
[SPEAKER_02]: And again, that's why I think going back to this approach where we're regulating AI where it's touching people and society is a really important way to address it.

1387
01:55:57,381 --> 01:55:57,801
[SPEAKER_10]: Thank you.

1388
01:55:57,801 --> 01:55:58,181
[SPEAKER_10]: Thank you, Mr.

1389
01:55:58,181 --> 01:55:58,281
[SPEAKER_10]: Chair.

1390
01:55:59,585 --> 01:56:01,346
[SPEAKER_09]: Thanks, Senator Pia.

1391
01:56:01,346 --> 01:56:06,109
[SPEAKER_09]: Senator Booker is next, but I think he's going to defer to Senator Ossoff.

1392
01:56:06,109 --> 01:56:08,151
[SPEAKER_13]: Because Senator Ossoff is a very big deal.

1393
01:56:08,151 --> 01:56:10,792
[SPEAKER_13]: I don't know if you know.

1394
01:56:10,792 --> 01:56:16,817
[SPEAKER_11]: I have a meeting at noon, and I'm grateful to you, Senator Booker, for yielding your time.

1395
01:56:16,817 --> 01:56:20,199
[SPEAKER_11]: You are, as always, brilliant and handsome.

1396
01:56:20,199 --> 01:56:24,061
[SPEAKER_11]: And thank you to the panelists for joining us.

1397
01:56:24,061 --> 01:56:27,964
[SPEAKER_11]: Thank you to the subcommittee leadership for opening this up to all committee members.

1398
01:56:28,887 --> 01:56:34,692
[SPEAKER_11]: If we're going to contemplate a regulatory framework, we're going to have to define what it is that we're regulating.

1399
01:56:34,692 --> 01:56:36,653
[SPEAKER_11]: So, you know, Mr.

1400
01:56:36,653 --> 01:56:44,159
[SPEAKER_11]: Alban, any such law will have to include a section that defines the scope of regulated activities, technologies, tools, products.

1401
01:56:44,159 --> 01:56:46,200
[SPEAKER_11]: Just take a stab at it.

1402
01:56:46,200 --> 01:56:47,562
[SPEAKER_15]: Yeah, thanks for asking, Senator Ossoff.

1403
01:56:47,562 --> 01:56:48,742
[SPEAKER_15]: I think it's super important.

1404
01:56:48,742 --> 01:56:53,246
[SPEAKER_15]: I think there are very different levels here, and I think it's important that any

1405
01:56:54,020 --> 01:57:04,528
[SPEAKER_15]: any new approach, any new law does not stop the innovation from happening with smaller companies, open source models, researchers that are doing work at a smaller scale.

1406
01:57:04,528 --> 01:57:07,190
[SPEAKER_15]: That's a wonderful part of this ecosystem and of America.

1407
01:57:07,190 --> 01:57:08,691
[SPEAKER_15]: We don't want to slow that down.

1408
01:57:08,691 --> 01:57:11,494
[SPEAKER_15]: There still may need to be some rules there.

1409
01:57:11,494 --> 01:57:19,120
[SPEAKER_15]: But I think we could draw a line at systems that need to be licensed in a very intense way.

1410
01:57:19,120 --> 01:57:23,363
[SPEAKER_15]: The easiest way to do it, I'm not sure if it's the best, but the easiest would be to talk about the amount of compute

1411
01:57:24,023 --> 01:57:25,444
[SPEAKER_15]: that goes into such a model.

1412
01:57:25,444 --> 01:57:39,113
[SPEAKER_15]: So we could define a threshold of compute and it'll have to go, it'll have to change, it could go up or down, down as we discover more efficient algorithms, that says above this amount of compute, you are in this regime.

1413
01:57:39,113 --> 01:57:52,622
[SPEAKER_15]: What I would prefer, it's harder to do but I think more accurate, is to define some capability thresholds and say a model that can do things x, y, and z, up to you all to decide, that's now in this licensing regime, but models that are less capable,

1414
01:57:53,067 --> 01:58:00,373
[SPEAKER_15]: you know, we don't want to stop our open source community, we don't want to stop individual researchers, we don't stop new startups can proceed, you know, with a different framework.

1415
01:58:00,373 --> 01:58:08,339
[SPEAKER_11]: Thank you, as concisely as you can please state which capabilities you'd propose we consider for the purposes of this definition.

1416
01:58:08,339 --> 01:58:18,427
[SPEAKER_11]: I would love rather than to do that off the cuff to follow up with your office with like, well, perhaps opine things opine understanding that you're just responding, and we're not making law.

1417
01:58:18,427 --> 01:58:21,810
[SPEAKER_15]: All right, in the spirit of just opining, I think a model that can

1418
01:58:22,993 --> 01:58:28,916
[SPEAKER_15]: Persuade, manipulate, influence a person's behavior or a person's beliefs, that would be a good threshold.

1419
01:58:28,916 --> 01:58:34,438
[SPEAKER_15]: I think a model that could help create novel biological agents would be a great threshold.

1420
01:58:34,438 --> 01:58:35,518
[SPEAKER_15]: Things like that.

1421
01:58:35,518 --> 01:58:46,083
[SPEAKER_11]: I want to talk about the predictive capabilities of the technology, and we're going to have to think about a lot of very complicated constitutional questions that arise from it.

1422
01:58:46,083 --> 01:58:51,145
[SPEAKER_11]: With massive data sets, the integrity

1423
01:58:51,669 --> 01:59:01,032
[SPEAKER_11]: an accuracy with which such technology can predict future human behaviors potentially pretty significant at the individual level, correct?

1424
01:59:01,032 --> 01:59:04,913
[SPEAKER_15]: I think we don't know the answer to that for sure, but let's say it can at least have some impact there.

1425
01:59:04,913 --> 01:59:18,537
[SPEAKER_11]: Okay, so we may be confronted by situations where, for example, a law enforcement agency deploying such technology seeks some kind of judicial consent to execute a search or to take some other police action on the basis of

1426
01:59:19,114 --> 01:59:25,061
[SPEAKER_11]: a modeled prediction about some individual's behavior.

1427
01:59:25,061 --> 01:59:32,010
[SPEAKER_11]: But that's very different from the kind of evidentiary predicate that normally police would take to a judge in order to get a warrant.

1428
01:59:32,010 --> 01:59:35,274
[SPEAKER_11]: Talk me through how you're thinking about that issue.

1429
01:59:35,274 --> 01:59:37,777
[SPEAKER_15]: Yeah, I think it's very important that we continue to

1430
01:59:38,720 --> 01:59:48,269
[SPEAKER_15]: Understand that these are tools that humans use to make human judgments and that we don't take away human judgment I don't think that people should be prosecuted based off of the output of an AI system.

1431
01:59:48,269 --> 02:00:00,661
[SPEAKER_11]: For example, we have no national privacy law The Europe has rolled one out to Mixed reviews.

1432
02:00:00,661 --> 02:00:01,822
[SPEAKER_11]: Do you think we need one?

1433
02:00:01,822 --> 02:00:02,923
[SPEAKER_11]: I think it'd be good.

1434
02:00:02,923 --> 02:00:03,884
[SPEAKER_11]: And what would be the

1435
02:00:04,485 --> 02:00:09,787
[SPEAKER_11]: qualities or purposes of such a law that you think would make the most sense based on your experience?

1436
02:00:09,787 --> 02:00:11,847
[SPEAKER_15]: Again, this is very far out of my area of expertise.

1437
02:00:11,847 --> 02:00:16,269
[SPEAKER_15]: I think there's many, many people that are privacy experts that could weigh on what a law needs.

1438
02:00:16,269 --> 02:00:19,370
[SPEAKER_15]: I'd still like you to weigh in.

1439
02:00:19,370 --> 02:00:28,572
[SPEAKER_15]: I mean, I think a minimum is that users should be able to sort of opt out from having their data used by companies like ours or the social media companies.

1440
02:00:28,572 --> 02:00:30,073
[SPEAKER_15]: It should be easy to delete your data.

1441
02:00:30,073 --> 02:00:31,753
[SPEAKER_15]: But the thing that I think is

1442
02:00:34,165 --> 02:00:41,755
[SPEAKER_15]: important from my perspective running an AI company is that if you don't want your data used for training these systems, you have the right to do that.

1443
02:00:41,755 --> 02:00:43,558
[SPEAKER_11]: So let's think about how that will be practically implemented.

1444
02:00:43,558 --> 02:00:50,888
[SPEAKER_11]: I mean, as I understand it, your tool and certainly similar tools, one of the inputs will be

1445
02:00:53,682 --> 02:01:01,207
[SPEAKER_11]: scraping, for lack of a better word, data off of the open web as a low-cost way of gathering information.

1446
02:01:01,207 --> 02:01:07,131
[SPEAKER_11]: And there's a vast amount of information out there about all of us.

1447
02:01:07,131 --> 02:01:14,095
[SPEAKER_11]: How would such a restriction on the access or use or analysis of such data be practically implemented?

1448
02:01:14,095 --> 02:01:21,180
[SPEAKER_15]: So I was speaking about something a little bit different, which is the data that someone generates, the questions they ask our system, things that they input, their training on that.

1449
02:01:21,576 --> 02:01:27,521
[SPEAKER_15]: Data that's on the public web that's accessible, even if we don't train on that, the models can certainly link out to it.

1450
02:01:27,521 --> 02:01:28,982
[SPEAKER_15]: So that was not what I was referring to.

1451
02:01:28,982 --> 02:01:35,267
[SPEAKER_15]: I think that there's ways to have your data, or there should be more ways to have your data taken down from the public web.

1452
02:01:35,267 --> 02:01:41,673
[SPEAKER_15]: But certainly models with web browsing capabilities will be able to search the web and link out to it.

1453
02:01:41,673 --> 02:01:50,600
[SPEAKER_11]: When you think about implementing a safety or a regulatory regime to constrain such software and to mitigate some risk,

1454
02:01:52,552 --> 02:02:04,096
[SPEAKER_11]: Is your view that the federal government would make laws such that certain capabilities or functionalities themselves are forbidden in potential?

1455
02:02:04,096 --> 02:02:10,618
[SPEAKER_11]: In other words, one cannot deploy or execute code capable of X?

1456
02:02:10,618 --> 02:02:10,858
[SPEAKER_11]: Yes.

1457
02:02:10,858 --> 02:02:16,040
[SPEAKER_11]: Or is it the act itself, X only when actually executed?

1458
02:02:16,040 --> 02:02:16,821
[SPEAKER_15]: Well, I think both.

1459
02:02:16,821 --> 02:02:18,681
[SPEAKER_15]: I'm a believer in defense in depth.

1460
02:02:18,681 --> 02:02:20,942
[SPEAKER_15]: I think that there should be limits on

1461
02:02:21,422 --> 02:02:24,805
[SPEAKER_15]: what a deployed model is capable of and then what it actually does to.

1462
02:02:24,805 --> 02:02:28,307
[SPEAKER_11]: How are you thinking about how kids use your product?

1463
02:02:28,307 --> 02:02:34,612
[SPEAKER_15]: We, well, you have to be, I mean, you have to be 18 or up or have your parents' permission at 13 and up to use a product.

1464
02:02:34,612 --> 02:02:37,354
[SPEAKER_15]: But we understand that people get around those safeguards all the time.

1465
02:02:37,354 --> 02:02:40,436
[SPEAKER_15]: And so what we try to do is just design a safe product.

1466
02:02:40,436 --> 02:02:48,302
[SPEAKER_15]: And there are decisions that we make that we would allow if we knew only adults were using it, that we just don't allow in the product because we know children will use it some way or other too.

1467
02:02:49,033 --> 02:02:56,635
[SPEAKER_15]: In particular, given how much these systems are being used in education, we want to be aware that that's happening.

1468
02:02:56,635 --> 02:03:18,100
[SPEAKER_11]: I think what, and Senator Blumenthal has done extensive work investigating this, what we've seen repeatedly is that companies whose revenues depend upon volume of use, screen time, intensity of use, design these systems in order to maximize the engagement of all users, including children, with

1469
02:03:18,490 --> 02:03:20,371
[SPEAKER_11]: with perverse results in many cases.

1470
02:03:20,371 --> 02:03:41,660
[SPEAKER_11]: And what I would humbly advise you is that you get way ahead of this issue, the safety for children of your product, or I think you're going to find that Senator Blumenthal, Senator Hawley, others on this subcommittee and I are – will look very harshly on the deployment of technology that harms children.

1471
02:03:41,660 --> 02:03:42,421
[SPEAKER_15]: We couldn't agree more.

1472
02:03:42,421 --> 02:03:45,342
[SPEAKER_15]: I think we're out of time, but I'm happy to talk about that if I can respond.

1473
02:03:45,342 --> 02:03:45,802
[SPEAKER_11]: Go ahead.

1474
02:03:45,802 --> 02:03:46,542
[SPEAKER_11]: Well, it's up to the chairman.

1475
02:03:50,621 --> 02:03:55,543
[SPEAKER_15]: First of all, I think we try to design systems that do not maximize for engagement.

1476
02:03:55,543 --> 02:03:59,825
[SPEAKER_15]: In fact, we're so short on GPUs, the less people use our products, the better.

1477
02:03:59,825 --> 02:04:01,526
[SPEAKER_15]: But we're not an advertising-based model.

1478
02:04:01,526 --> 02:04:05,327
[SPEAKER_15]: We're not trying to get people to use it more and more.

1479
02:04:05,327 --> 02:04:09,569
[SPEAKER_15]: And I think that's a different shape than ad-supported social media.

1480
02:04:09,569 --> 02:04:17,853
[SPEAKER_15]: Second, these systems do have the capability to influence in obvious and in very nuanced ways.

1481
02:04:18,787 --> 02:04:23,849
[SPEAKER_15]: And I think that's particularly important for the safety of children, but that will impact all of us.

1482
02:04:23,849 --> 02:04:39,116
[SPEAKER_15]: One of the things that we'll do ourselves, regulation or not, but I think a regulatory approach would be good for also, is requirements about how the values of these systems are set and how these systems respond to questions that can cause influence.

1483
02:04:39,116 --> 02:04:40,277
[SPEAKER_15]: So we'd love to partner with you.

1484
02:04:40,277 --> 02:04:41,777
[SPEAKER_15]: Couldn't agree more on the importance.

1485
02:04:41,777 --> 02:04:43,318
[SPEAKER_15]: Thank you.

1486
02:04:43,318 --> 02:04:43,478
[SPEAKER_13]: Mr.

1487
02:04:43,478 --> 02:04:48,240
[SPEAKER_13]: Chairman, for the record, I just want to say that the senator from Georgia is also very handsome and brilliant, too.

1488
02:04:50,859 --> 02:04:55,429
[SPEAKER_09]: I will allow that comment to stand without objection.

1489
02:04:55,429 --> 02:04:56,451
[SPEAKER_13]: Without objection, okay.

1490
02:04:59,390 --> 02:04:59,570
[SPEAKER_13]: Mr.

1491
02:04:59,570 --> 02:05:03,433
[SPEAKER_13]: Chairman and Ranking Member, thank you very much.

1492
02:05:03,433 --> 02:05:07,197
[SPEAKER_13]: It's nice that we finally got down to the bald guys down here at the end.

1493
02:05:07,197 --> 02:05:08,037
[SPEAKER_13]: I just want to thank you both.

1494
02:05:08,037 --> 02:05:15,985
[SPEAKER_13]: This has been one of the best hearings I've had this Congress and just a testimony to you two and seeing the challenges and the opportunities that AI presents.

1495
02:05:15,985 --> 02:05:17,586
[SPEAKER_13]: So I appreciate you both.

1496
02:05:17,586 --> 02:05:20,749
[SPEAKER_13]: I want to just jump in, I think very broadly and then I'll get a little more narrow.

1497
02:05:22,211 --> 02:05:31,700
[SPEAKER_13]: Sam, you said very broadly, technology has been moving like this and we are, a lot of people have been talking about regulation, and so I use the example of the automobile.

1498
02:05:31,700 --> 02:05:35,423
[SPEAKER_13]: What an extraordinary piece of technology.

1499
02:05:35,423 --> 02:05:38,206
[SPEAKER_13]: I mean, New York City did not know what to do with horse manure.

1500
02:05:38,206 --> 02:05:38,646
[SPEAKER_13]: They were having

1501
02:05:39,026 --> 02:05:43,308
[SPEAKER_13]: crises, forming commissions, and the automobile comes along, ends that problem.

1502
02:05:43,308 --> 02:05:47,130
[SPEAKER_13]: But at the same time, we have tens of thousands of people dying on highways every day.

1503
02:05:47,130 --> 02:05:49,191
[SPEAKER_13]: We have emissions crises and the like.

1504
02:05:49,191 --> 02:05:59,796
[SPEAKER_13]: There are multiple federal agencies, multiple federal agencies that were created or are specifically focused on regulating cars.

1505
02:05:59,796 --> 02:06:05,018
[SPEAKER_13]: And so this idea that this equally transforming technology is coming,

1506
02:06:05,785 --> 02:06:14,336
[SPEAKER_13]: And for Congress to do nothing, which is not what anybody here is calling for, little or nothing, is obviously unacceptable.

1507
02:06:14,336 --> 02:06:17,400
[SPEAKER_13]: I really appreciate Senator Welch and I. We've been going back and forth.

1508
02:06:18,096 --> 02:06:23,902
[SPEAKER_13]: during this hearing and him and Bennett have a bill talking about trying to regulate in this space.

1509
02:06:23,902 --> 02:06:34,792
[SPEAKER_13]: Not doing so for social media has been, I think, very destructive and allowed a lot of things to go on that are really causing a lot of harm.

1510
02:06:34,792 --> 02:06:36,834
[SPEAKER_13]: And so the question is, is what kind of regulation?

1511
02:06:36,834 --> 02:06:38,696
[SPEAKER_13]: You all have spoken that to a lot of my colleagues.

1512
02:06:40,615 --> 02:06:41,536
[SPEAKER_13]: And I want to say, Ms.

1513
02:06:41,536 --> 02:06:52,649
[SPEAKER_13]: Montgomery, and I have to give full disclosure, I'm the child of two IBM parents, but I, you know, you talked about defining the highest risk uses.

1514
02:06:52,649 --> 02:06:54,251
[SPEAKER_13]: We don't know all of them.

1515
02:06:54,251 --> 02:06:55,452
[SPEAKER_13]: We really don't.

1516
02:06:55,452 --> 02:06:58,676
[SPEAKER_13]: We can't see where this is going.

1517
02:06:58,676 --> 02:07:01,439
[SPEAKER_13]: Regulating at the point of risk, and you sort of

1518
02:07:02,506 --> 02:07:09,934
[SPEAKER_13]: It's called not for an agency, and I think when somebody else asks you to specify, because you don't want to slow things down, we should build on what we have in place.

1519
02:07:09,934 --> 02:07:16,241
[SPEAKER_13]: But you can envision that we can try to work on two different ways that ultimately a specific

1520
02:07:17,208 --> 02:07:28,980
[SPEAKER_13]: like we have in cars, EPA, NHTSA, the Federal Motor Car Carrier Safety Administration, all of these things, you can imagine something specific that is, as Mr.

1521
02:07:28,980 --> 02:07:33,464
[SPEAKER_13]: Marcus points out, a nimble agency that could do monitoring and other things.

1522
02:07:33,464 --> 02:07:36,066
[SPEAKER_13]: You can imagine the need for something like that, correct?

1523
02:07:36,066 --> 02:07:36,767
[SPEAKER_02]: Oh, absolutely.

1524
02:07:37,387 --> 02:07:54,861
[SPEAKER_13]: And, and so just for the record then, in addition to trying to regulate with what we have now, you would encourage Congress and my colleague Senator Welsh to move forward in trying to figure out the right tailored agency to deal with what we know and perhaps things that might come up in the future?

1525
02:07:54,861 --> 02:08:03,187
[SPEAKER_02]: I would encourage Congress to make sure it understands the technology, has the skills and resources in place to impose regulatory requirements

1526
02:08:03,838 --> 02:08:07,560
[SPEAKER_02]: on the uses of the technology, and to understand emerging risks as well.

1527
02:08:07,560 --> 02:08:08,401
[SPEAKER_02]: So, yes.

1528
02:08:08,401 --> 02:08:08,601
[SPEAKER_13]: Mr.

1529
02:08:08,601 --> 02:08:11,202
[SPEAKER_13]: Marcus, there's no way to put this genie in the bottle.

1530
02:08:11,202 --> 02:08:13,964
[SPEAKER_13]: Globally, it's exploding.

1531
02:08:13,964 --> 02:08:23,309
[SPEAKER_13]: I appreciate your thoughts, and I shared some with my staff about your ideas of what the international context is, but there's no way to stop this moving forward.

1532
02:08:23,309 --> 02:08:26,511
[SPEAKER_13]: So, with that understanding, just building on what Ms.

1533
02:08:26,511 --> 02:08:30,673
[SPEAKER_13]: Montgomery said, what kind of encouragement do you have as specifically as possible

1534
02:08:31,137 --> 02:08:34,279
[SPEAKER_13]: to forming an agency, to using current rules and regulations?

1535
02:08:34,279 --> 02:08:37,121
[SPEAKER_13]: Can you just put some clarity on what you've already stated?

1536
02:08:37,121 --> 02:08:40,523
[SPEAKER_08]: Let me just insert there are more genies yet to come from more bottles.

1537
02:08:40,523 --> 02:08:46,486
[SPEAKER_08]: Some genies are already out, but we don't have machines that can really, for example, self-improve themselves.

1538
02:08:46,486 --> 02:08:50,749
[SPEAKER_08]: We don't really have machines that have self-awareness, and we might not ever want to go there.

1539
02:08:50,749 --> 02:08:53,410
[SPEAKER_08]: So there are other genies to be concerned about.

1540
02:08:53,410 --> 02:08:55,231
[SPEAKER_08]: On to the main part of your question.

1541
02:08:57,052 --> 02:09:05,300
[SPEAKER_08]: I think that we need to have some international meetings very quickly with people who have expertise in how you grow agencies, in the history of growing agencies.

1542
02:09:05,300 --> 02:09:06,661
[SPEAKER_08]: We need to do that in the federal level.

1543
02:09:06,661 --> 02:09:08,863
[SPEAKER_08]: We need to do that in the international level.

1544
02:09:08,863 --> 02:09:14,648
[SPEAKER_08]: I'll just emphasize one thing I haven't as much as I would like to, which is that I think science has to be a really important part of it.

1545
02:09:14,648 --> 02:09:15,509
[SPEAKER_08]: And I'll give an example.

1546
02:09:15,509 --> 02:09:17,211
[SPEAKER_08]: We've talked about misinformation.

1547
02:09:17,211 --> 02:09:20,874
[SPEAKER_08]: We don't really have the tools right now to detect and label misinformation.

1548
02:09:21,234 --> 02:09:23,076
[SPEAKER_08]: with nutrition labels that we would like to.

1549
02:09:23,076 --> 02:09:25,698
[SPEAKER_08]: We have to build new technologies for that.

1550
02:09:25,698 --> 02:09:30,962
[SPEAKER_08]: We don't really have tools yet to detect a wide uptick in cybercrime, probably.

1551
02:09:30,962 --> 02:09:32,864
[SPEAKER_08]: We probably need new tools there.

1552
02:09:32,864 --> 02:09:41,651
[SPEAKER_08]: We need science to probably help us to figure out what we need to build and also what it is that we need to have transparency around and so forth.

1553
02:09:41,651 --> 02:09:44,674
[SPEAKER_13]: Sam, just going to you for the little bit of time I have left.

1554
02:09:44,674 --> 02:09:45,034
[SPEAKER_13]: Real quick,

1555
02:09:46,175 --> 02:09:49,236
[SPEAKER_13]: First of all, you're a bit of a unicorn when I sat down with you first.

1556
02:09:49,236 --> 02:09:56,617
[SPEAKER_13]: Could you explain why nonprofit, in other words, you're not looking at this, and you've even capped the VC people.

1557
02:09:56,617 --> 02:10:00,798
[SPEAKER_13]: Just really quickly, I want folks to understand that.

1558
02:10:00,798 --> 02:10:05,438
[SPEAKER_15]: We started as a nonprofit really focused on how this technology was going to be built.

1559
02:10:05,438 --> 02:10:10,059
[SPEAKER_15]: At the time, it was very outside the Overton window that something like AGI was even possible.

1560
02:10:10,059 --> 02:10:11,740
[SPEAKER_15]: That shifted a lot.

1561
02:10:11,740 --> 02:10:14,420
[SPEAKER_15]: We didn't know at the time how important scale was going to be.

1562
02:10:14,884 --> 02:10:27,282
[SPEAKER_15]: But we did know that we wanted to build this with humanity's best interest at heart and a belief that this technology could, if it goes the way we want, if we can do some of those things Professor Marcus mentioned,

1563
02:10:28,798 --> 02:10:30,078
[SPEAKER_15]: really deeply transformed the world.

1564
02:10:30,078 --> 02:10:33,779
[SPEAKER_15]: And we wanted to be as much of a force for getting to a positive.

1565
02:10:33,779 --> 02:10:34,559
[SPEAKER_13]: I'm going to interrupt you.

1566
02:10:34,559 --> 02:10:35,239
[SPEAKER_13]: I think that's all good.

1567
02:10:35,239 --> 02:10:37,120
[SPEAKER_13]: I hope more of that gets on the record.

1568
02:10:37,120 --> 02:10:39,600
[SPEAKER_13]: The second part of my question as well.

1569
02:10:39,600 --> 02:10:41,621
[SPEAKER_13]: I found it fascinating.

1570
02:10:41,621 --> 02:10:45,462
[SPEAKER_13]: But are you ever going to for revenue model for return on your investors?

1571
02:10:45,462 --> 02:10:49,562
[SPEAKER_13]: Are you ever going to do ads or something like that?

1572
02:10:49,562 --> 02:10:51,283
[SPEAKER_13]: I wouldn't say never.

1573
02:10:51,283 --> 02:10:52,423
[SPEAKER_15]: I don't think like I think

1574
02:10:52,819 --> 02:10:59,250
[SPEAKER_15]: there may be people that we want to offer services to and there's no other model that works, but I really like having a subscription based model.

1575
02:10:59,250 --> 02:11:01,514
[SPEAKER_15]: We have API developers pay us and we have

1576
02:11:02,584 --> 02:11:06,406
[SPEAKER_13]: Can I just jump real quickly?

1577
02:11:06,406 --> 02:11:14,990
[SPEAKER_13]: One of my biggest concerns about this space is what I've already seen in the space of Web 2, Web 3, is this massive corporate concentration.

1578
02:11:14,990 --> 02:11:20,853
[SPEAKER_13]: It is really terrifying to see how few companies now control and affect the lives of so many of us.

1579
02:11:20,853 --> 02:11:23,374
[SPEAKER_13]: And these companies are getting bigger and more powerful.

1580
02:11:23,374 --> 02:11:26,395
[SPEAKER_13]: And I see OpenAI backed by Microsoft.

1581
02:11:26,395 --> 02:11:27,696
[SPEAKER_13]: Anthropic is backed by Google.

1582
02:11:28,473 --> 02:11:33,297
[SPEAKER_13]: Google has its own in-house product, so I'm really worried about that.

1583
02:11:33,297 --> 02:11:37,621
[SPEAKER_13]: And I'm wondering if, Sam, you can give me a quick acknowledgement.

1584
02:11:37,621 --> 02:11:46,488
[SPEAKER_13]: Are you worried about the corporate concentration in this space and what effect it might have and the associated risks, perhaps, with market concentration in AI?

1585
02:11:46,488 --> 02:11:46,869
[SPEAKER_13]: And then, Mr.

1586
02:11:46,869 --> 02:11:49,191
[SPEAKER_13]: Marcus, can you answer that as well?

1587
02:11:49,191 --> 02:11:51,673
[SPEAKER_15]: I think there will be many people that develop models

1588
02:11:52,506 --> 02:12:14,171
[SPEAKER_15]: what's happening on the open source community is amazing, but there will be a relatively small number of providers that can make models at the at the true danger that I think there is benefits and danger to that, because we're talking about all the dangers with AI, the fewer of us that you really have to keep a careful eye on, on the absolute, like bleeding edge of capabilities, there's benefits there.

1589
02:12:14,171 --> 02:12:20,293
[SPEAKER_15]: But then I think there needs to be enough in their will, because there's so much value that consumers have choice that we have different ideas.

1590
02:12:20,293 --> 02:12:20,553
[SPEAKER_15]: Mr.

1591
02:12:20,553 --> 02:12:21,193
[SPEAKER_15]: Marcus real quick.

1592
02:12:21,818 --> 02:12:29,478
[SPEAKER_08]: There is a real risk of a kind of technocracy combined with oligarchy, where a small number of companies influence people's beliefs.

1593
02:12:29,864 --> 02:12:31,205
[SPEAKER_08]: through the nature of these systems.

1594
02:12:31,205 --> 02:12:40,771
[SPEAKER_08]: Again, I put something in the record about the Wall Street Journal about how these systems can subtly shape our beliefs, and that has enormous influence on how we live our lives.

1595
02:12:40,771 --> 02:12:45,654
[SPEAKER_08]: And having a small number of players do that with data that we don't even know about, that scares me.

1596
02:12:45,654 --> 02:12:46,694
[SPEAKER_08]: Sam, I'm sorry.

1597
02:12:46,694 --> 02:12:48,315
[SPEAKER_15]: One more thing I wanted to add.

1598
02:12:48,315 --> 02:12:52,138
[SPEAKER_15]: One thing that I think is very important is that what these systems get aligned to

1599
02:12:53,009 --> 02:12:59,095
[SPEAKER_15]: whose values, what those bounds are, that that is somehow set by society as a whole, by governments as a whole.

1600
02:12:59,095 --> 02:13:09,866
[SPEAKER_15]: And so creating that data set, that alignment data set, it could be an AI constitution, whatever it is, that has got to come very broadly from society.

1601
02:13:09,866 --> 02:13:10,587
[SPEAKER_13]: Thank you very much, Mr.

1602
02:13:10,587 --> 02:13:10,767
[SPEAKER_13]: Chairman.

1603
02:13:10,767 --> 02:13:13,870
[SPEAKER_13]: My time has expired, and I guess the best for last.

1604
02:13:13,870 --> 02:13:15,451
[SPEAKER_09]: Thank you, Senator Booker.

1605
02:13:15,451 --> 02:13:15,932
[SPEAKER_09]: Senator Welch.

1606
02:13:17,634 --> 02:13:21,737
[SPEAKER_06]: First of all, I want to thank you, Senator Blumenthal, and you, Senator Hawley.

1607
02:13:21,737 --> 02:13:24,798
[SPEAKER_06]: This has been a tremendous hearing.

1608
02:13:24,798 --> 02:13:32,463
[SPEAKER_06]: Senators are noted for their short attention spans, but I sat through this entire hearing and enjoyed every minute of it.

1609
02:13:32,463 --> 02:13:38,066
[SPEAKER_09]: You do have one of our longer attention spans in the United States Senate, to your great credit.

1610
02:13:38,066 --> 02:13:41,127
[SPEAKER_06]: Well, we've had good witnesses, and it's an incredibly important issue.

1611
02:13:41,127 --> 02:13:45,790
[SPEAKER_06]: And here's just, I don't, all the questions I have have been asked.

1612
02:13:46,412 --> 02:13:55,635
[SPEAKER_06]: really, but here's kind of a takeaway and what I think is the major question that we're going to have to answer as a Congress.

1613
02:13:55,635 --> 02:14:08,840
[SPEAKER_06]: Number one, you're here because AI is this extraordinary new technology that everyone says can be transformative as much as the printing press.

1614
02:14:08,840 --> 02:14:14,282
[SPEAKER_06]: Number two, it's really unknown what's going to happen, but there's a big fear you've expressed, all of you,

1615
02:14:15,743 --> 02:14:23,545
[SPEAKER_06]: what bad actors can do and will do if there's no rules of the road.

1616
02:14:23,545 --> 02:14:35,449
[SPEAKER_06]: Number three, as a member who served in the House and now in the Senate, I've come to the conclusion that it's impossible for Congress to keep up with the speed of technology.

1617
02:14:36,465 --> 02:14:59,070
[SPEAKER_06]: And there have been concerns expressed about social media, and now about AI that relate to fundamental privacy rights, bias rights, intellectual property, the spread of disinformation, which in many ways for me is the biggest threat, because that goes to the core of our capacity for self-governing.

1618
02:14:59,070 --> 02:15:04,132
[SPEAKER_06]: There's the economic transformation, which can be profound.

1619
02:15:04,132 --> 02:15:05,292
[SPEAKER_06]: There's safety concerns.

1620
02:15:06,695 --> 02:15:12,057
[SPEAKER_06]: And I've come to the conclusion that we absolutely have to have an agency.

1621
02:15:12,057 --> 02:15:19,939
[SPEAKER_06]: What its scope of engagement is, it has to be defined by us.

1622
02:15:19,939 --> 02:15:32,882
[SPEAKER_06]: But I believe that unless we have an agency that is going to address these questions from social media and AI, we really don't have much of a defense against the bad stuff.

1623
02:15:33,808 --> 02:15:35,870
[SPEAKER_06]: And the bad stuff will come.

1624
02:15:35,870 --> 02:15:46,963
[SPEAKER_06]: So, last year, I introduced in the House side and Senator Bennett did in the Senate side, it was the end of the year, Digital Commission Act, and we're going to be reintroducing that this year.

1625
02:15:46,963 --> 02:15:55,552
[SPEAKER_06]: And the two things that I want to ask, one, you've somewhat answered because I think two of the three of you have said you think we do need an independent commission.

1626
02:15:56,093 --> 02:16:07,115
[SPEAKER_06]: You know, and Congress established an independent commission when railroads were running rampant over the interest of farmers, when Wall Street had no rules of the road, and we had the SEC.

1627
02:16:07,115 --> 02:16:10,176
[SPEAKER_06]: And I think we're at that point now.

1628
02:16:10,176 --> 02:16:17,138
[SPEAKER_06]: But what the commission does would have to be defined and circumscribed.

1629
02:16:17,138 --> 02:16:22,239
[SPEAKER_06]: But also, there's always a question about the use of regulatory authority.

1630
02:16:23,997 --> 02:16:26,558
[SPEAKER_06]: the recognition that it can be used for good.

1631
02:16:26,558 --> 02:16:26,898
[SPEAKER_06]: J.D.

1632
02:16:26,898 --> 02:16:37,660
[SPEAKER_06]: Vance actually mentioned that when we were considering his and Senator Brown's bill about railroads and that event in East Palestine, regulation for the public health.

1633
02:16:37,660 --> 02:16:46,562
[SPEAKER_06]: But there's also a legitimate concern about regulation getting in the way of things, being too cumbersome, and being a negative influence.

1634
02:16:46,562 --> 02:16:52,083
[SPEAKER_06]: So A, two of the three of you have said you think we do need an agency.

1635
02:16:54,624 --> 02:17:10,329
[SPEAKER_06]: What are some of the perils of an agency that we would have to be mindful of in order to make certain that its goals of protecting many of those interests I just mentioned, privacy, bias, intellectual property, disinformation, would be the winners and not the losers?

1636
02:17:10,329 --> 02:17:11,329
[SPEAKER_06]: And I'll start with you, Mr.

1637
02:17:11,329 --> 02:17:12,569
[SPEAKER_06]: Altman.

1638
02:17:12,569 --> 02:17:15,830
[SPEAKER_15]: Thank you, Senator.

1639
02:17:15,830 --> 02:17:18,031
[SPEAKER_15]: One, I think America has got to continue to lead.

1640
02:17:18,031 --> 02:17:20,671
[SPEAKER_15]: This happened in America.

1641
02:17:20,671 --> 02:17:22,072
[SPEAKER_15]: I'm very proud that it happened in America.

1642
02:17:22,975 --> 02:17:32,443
[SPEAKER_06]: I think that's right, and that's why I'd be much more confident if we had our agency as opposed to got involved in international discussions.

1643
02:17:32,443 --> 02:17:40,229
[SPEAKER_06]: Ultimately, you want the rules of the road, but I think if we lead and get rules of the road that work for us, that is probably a more effective way to proceed.

1644
02:17:40,229 --> 02:17:47,815
[SPEAKER_15]: I personally believe there's a way to do both, and I think it is important to have the global view on this because this technology

1645
02:17:48,434 --> 02:17:51,735
[SPEAKER_15]: will impact Americans and all of us wherever it's developed.

1646
02:17:51,735 --> 02:17:54,315
[SPEAKER_15]: But I think we want America to lead.

1647
02:17:54,315 --> 02:17:58,776
[SPEAKER_06]: We want... So get to the perils issue, though, because I know... Well, that's one.

1648
02:17:58,776 --> 02:18:05,817
[SPEAKER_15]: I mean, that is a peril, which is you slow down American industry in such a way that China or somebody else makes faster progress.

1649
02:18:05,817 --> 02:18:10,798
[SPEAKER_15]: A second, and I think this can happen with, like, the regulatory pressure should be on us.

1650
02:18:10,798 --> 02:18:11,798
[SPEAKER_15]: It should be on Google.

1651
02:18:11,798 --> 02:18:14,519
[SPEAKER_15]: It should be on the other small set of people in the lead the most.

1652
02:18:14,519 --> 02:18:16,319
[SPEAKER_15]: We don't want to slow down smaller start-ups.

1653
02:18:16,319 --> 02:18:17,099
[SPEAKER_15]: We don't want to slow down

1654
02:18:17,912 --> 02:18:20,434
[SPEAKER_15]: open source efforts, we still need them to comply with things.

1655
02:18:20,434 --> 02:18:42,410
[SPEAKER_15]: They can still, you can still cause great harm with a smaller model, but leaving the room and the space for new ideas and new companies and independent researchers to do their work and not putting a regulatory burden that say a company like us could handle but a smaller one couldn't, I think that's another peril and it's clearly a way that regulation has gone.

1656
02:18:42,410 --> 02:18:42,690
[SPEAKER_15]: Mr.

1657
02:18:42,690 --> 02:18:44,191
[SPEAKER_15]: Marcus, or Professor Marcus?

1658
02:18:44,662 --> 02:18:48,044
[SPEAKER_08]: The other obvious peril is regulatory capture.

1659
02:18:48,044 --> 02:18:54,969
[SPEAKER_08]: If we make it appear as if we are doing something but it's more like greenwashing and nothing really happens.

1660
02:18:54,969 --> 02:19:00,612
[SPEAKER_08]: We just keep out the little players because we put so much burden that only the big players can do it.

1661
02:19:00,612 --> 02:19:02,213
[SPEAKER_08]: So there are also those kinds of perils.

1662
02:19:02,213 --> 02:19:04,035
[SPEAKER_08]: I fully agree with everything that Mr.

1663
02:19:04,035 --> 02:19:07,077
[SPEAKER_08]: Altman said and I would add that to the list.

1664
02:19:07,077 --> 02:19:07,297
[SPEAKER_06]: Mr.

1665
02:19:07,297 --> 02:19:07,697
[SPEAKER_06]: Montgomery.

1666
02:19:09,107 --> 02:19:16,352
[SPEAKER_02]: One of the things I would add to the list is the risk of not holding companies accountable for the harms that they're causing today.

1667
02:19:16,352 --> 02:19:21,016
[SPEAKER_02]: So we talk about misinformation in electoral systems.

1668
02:19:21,016 --> 02:19:28,481
[SPEAKER_02]: Agency or no agency, we need to hold companies responsible today and accountable for the AI that they're deploying.

1669
02:19:29,700 --> 02:19:34,504
[SPEAKER_02]: disseminates misinformation on things like elections and where the risk is.

1670
02:19:34,504 --> 02:19:38,587
[SPEAKER_06]: A regulatory agency would do a lot of the things that Senator Graham was talking about.

1671
02:19:38,587 --> 02:19:41,169
[SPEAKER_06]: You don't build a nuclear reactor without getting a license.

1672
02:19:41,169 --> 02:19:43,871
[SPEAKER_06]: You don't build an AI system without getting a license.

1673
02:19:43,871 --> 02:19:45,592
[SPEAKER_06]: It gets tested independently.

1674
02:19:45,592 --> 02:19:47,213
[SPEAKER_06]: I think it's a great analogy.

1675
02:19:47,213 --> 02:19:50,976
[SPEAKER_08]: We need both pre-deployment and post-deployment.

1676
02:19:50,976 --> 02:19:51,657
[SPEAKER_06]: Okay.

1677
02:19:51,657 --> 02:19:52,658
[SPEAKER_06]: Thank you all very much.

1678
02:19:52,658 --> 02:19:53,458
[SPEAKER_06]: I yield back, Mr.

1679
02:19:53,458 --> 02:19:54,319
[SPEAKER_06]: Chairman.

1680
02:19:54,319 --> 02:19:54,919
[SPEAKER_09]: Thanks.

1681
02:19:54,919 --> 02:19:55,740
[SPEAKER_09]: Thanks, Senator Wells.

1682
02:19:55,740 --> 02:19:57,782
[SPEAKER_09]: Let me ask a few more questions.

1683
02:19:59,120 --> 02:20:13,246
[SPEAKER_09]: very, very patient, and the turnout today, which is beyond our subcommittee, I think reflects both your value in what you're contributing as well as the interest in this topic.

1684
02:20:13,246 --> 02:20:19,109
[SPEAKER_09]: There are a number of subjects that we haven't covered at all.

1685
02:20:19,109 --> 02:20:28,633
[SPEAKER_09]: One was just alluded to by Professor Marcus, which is the monopolization danger.

1686
02:20:29,857 --> 02:20:55,204
[SPEAKER_09]: the dominance of markets that excludes new competition and thereby inhibits or prevents innovation and invention, which we have seen in social media, as well as some of the old industries, airlines, automobiles and others where consolidation has narrowed competition.

1687
02:20:55,204 --> 02:20:58,105
[SPEAKER_09]: And so I think we need to

1688
02:20:58,780 --> 02:21:10,107
[SPEAKER_09]: to focus on kind of an old area of antitrust, which dates more than a century, still inadequate to deal with the challenges we have right now in our economy.

1689
02:21:10,107 --> 02:21:27,938
[SPEAKER_09]: And certainly, we need to be mindful of the way that rules can enable the big guys to get bigger and exclude innovation and competition and responsible good guys, such as are represented in this industry right now.

1690
02:21:28,728 --> 02:21:31,290
[SPEAKER_09]: We haven't dealt with national security.

1691
02:21:31,290 --> 02:21:33,531
[SPEAKER_09]: There are huge implications for national security.

1692
02:21:33,531 --> 02:21:42,237
[SPEAKER_09]: I will tell you, as a member of the Armed Services Committee, classified briefings on this issue have abounded.

1693
02:21:42,237 --> 02:21:57,466
[SPEAKER_09]: And the threats that are posed by some of our adversaries, China has been mentioned here, but the sources of threats to this nation in this space are very real and urgent.

1694
02:21:57,466 --> 02:21:58,327
[SPEAKER_09]: We're not going to deal with them

1695
02:21:58,858 --> 02:22:04,680
[SPEAKER_09]: today but we do need to deal with them and we will hopefully in this committee.

1696
02:22:04,680 --> 02:22:10,222
[SPEAKER_09]: And then on the issue of a new agency, you know, I've been doing this stuff for a while.

1697
02:22:10,222 --> 02:22:12,763
[SPEAKER_09]: I was Attorney General of Connecticut for 20 years.

1698
02:22:12,763 --> 02:22:14,283
[SPEAKER_09]: I was a federal prosecutor to the U.S.

1699
02:22:14,283 --> 02:22:15,363
[SPEAKER_09]: Attorney.

1700
02:22:15,363 --> 02:22:18,004
[SPEAKER_09]: Most of my career has been in enforcement.

1701
02:22:18,004 --> 02:22:26,707
[SPEAKER_09]: And I will tell you something, you can create 10 new agencies but if you don't give them the resources and I'm talking not just about dollars, I'm talking about

1702
02:22:27,201 --> 02:22:33,465
[SPEAKER_09]: scientific expertise, you guys will run circles around them.

1703
02:22:33,465 --> 02:22:48,315
[SPEAKER_09]: And it isn't just the models or the generative AI that will run circles around them, but it is the scientists in your companies.

1704
02:22:48,315 --> 02:22:53,498
[SPEAKER_09]: For every success story in government regulation, you can think of five failures.

1705
02:22:53,498 --> 02:22:54,559
[SPEAKER_09]: That's true of the FDA.

1706
02:22:55,302 --> 02:22:58,023
[SPEAKER_09]: It's true of the IAEA.

1707
02:22:58,023 --> 02:22:59,363
[SPEAKER_09]: It's true of the SEC.

1708
02:22:59,363 --> 02:23:03,365
[SPEAKER_09]: It's true of the whole alphabet list of government agencies.

1709
02:23:03,365 --> 02:23:06,406
[SPEAKER_09]: And I hope our experience here will be different.

1710
02:23:06,406 --> 02:23:18,770
[SPEAKER_09]: But the Pandora's box requires more than just the words or the concepts licensing new agency.

1711
02:23:18,770 --> 02:23:21,751
[SPEAKER_09]: There's some real hard decision making as Ms.

1712
02:23:21,751 --> 02:23:23,752
[SPEAKER_09]: Montgomery has alluded to about

1713
02:23:24,165 --> 02:23:29,810
[SPEAKER_09]: how to frame the rules to fit the risks.

1714
02:23:29,810 --> 02:23:32,652
[SPEAKER_09]: First, do no harm.

1715
02:23:32,652 --> 02:23:34,094
[SPEAKER_09]: Make it effective.

1716
02:23:34,094 --> 02:23:35,755
[SPEAKER_09]: Make it enforceable.

1717
02:23:35,755 --> 02:23:37,717
[SPEAKER_09]: Make it real.

1718
02:23:37,717 --> 02:23:50,107
[SPEAKER_09]: I think we need to grapple with the hard questions here that, frankly, this initial hearing, I think, has raised very successfully, but not answered.

1719
02:23:50,107 --> 02:23:52,229
[SPEAKER_09]: And I thank our colleagues who have

1720
02:23:52,880 --> 02:23:57,784
[SPEAKER_09]: participated and made these very creative suggestions.

1721
02:23:57,784 --> 02:24:01,707
[SPEAKER_09]: I'm very interested in enforcement.

1722
02:24:01,707 --> 02:24:07,471
[SPEAKER_09]: I, you know, literally 15 years ago, I think, advocated abolishing Section 230.

1723
02:24:07,471 --> 02:24:09,893
[SPEAKER_09]: What's old is new again.

1724
02:24:09,893 --> 02:24:14,816
[SPEAKER_09]: You know, now people are talking about abolishing Section 230.

1725
02:24:14,816 --> 02:24:18,559
[SPEAKER_09]: Back then, it was considered completely unrealistic.

1726
02:24:18,559 --> 02:24:21,561
[SPEAKER_09]: But enforcement really does matter.

1727
02:24:21,561 --> 02:24:22,102
[SPEAKER_09]: I want to ask,

1728
02:24:23,305 --> 02:24:23,565
[SPEAKER_09]: Mr.

1729
02:24:23,565 --> 02:24:44,518
[SPEAKER_09]: Altman, because of the privacy issue, and you've suggested that you have an interest in protecting the privacy of the data that may come to you or be available, what specific steps do you take to protect privacy?

1730
02:24:44,518 --> 02:24:46,980
[SPEAKER_15]: One is that we don't train on any data submitted to our API.

1731
02:24:47,710 --> 02:24:51,653
[SPEAKER_15]: So if you're a business customer of ours and submit data, we don't train it at all.

1732
02:24:51,653 --> 02:24:58,378
[SPEAKER_15]: We do retain it for 30 days solely for the purpose of trust and safety enforcement, but that's different than training on it.

1733
02:24:58,378 --> 02:25:02,981
[SPEAKER_15]: If you use ChatGPT, you can opt out of us training on your data.

1734
02:25:02,981 --> 02:25:08,446
[SPEAKER_15]: You can also delete your conversation history or your whole account.

1735
02:25:08,446 --> 02:25:08,646
[SPEAKER_09]: Ms.

1736
02:25:08,646 --> 02:25:15,171
[SPEAKER_09]: Montgomery, I know you don't deal directly with consumers, but do you take steps to protect privacy as well?

1737
02:25:15,171 --> 02:25:15,771
[SPEAKER_02]: Absolutely.

1738
02:25:16,448 --> 02:25:24,454
[SPEAKER_02]: And we even filter our large language models for content that includes personal information that may have been pulled from public data sets as well.

1739
02:25:24,454 --> 02:25:28,758
[SPEAKER_02]: So we apply additional level of filtering.

1740
02:25:28,758 --> 02:25:35,843
[SPEAKER_09]: Professor Marcus, you made reference to self-awareness, self-learning.

1741
02:25:35,843 --> 02:25:41,788
[SPEAKER_09]: Already we're talking about the potential for jail breaks.

1742
02:25:41,788 --> 02:25:45,370
[SPEAKER_09]: How soon do you think that new kind of generative

1743
02:25:46,298 --> 02:25:52,846
[SPEAKER_09]: AI will be usable, will be practical.

1744
02:25:52,846 --> 02:25:55,829
[SPEAKER_08]: New AI that is self-aware and so forth?

1745
02:25:55,829 --> 02:25:56,710
[SPEAKER_08]: Yes.

1746
02:25:56,710 --> 02:25:58,392
[SPEAKER_08]: I have no idea on that one.

1747
02:25:58,392 --> 02:26:03,078
[SPEAKER_08]: I think we don't really understand what self-awareness is, and so it's hard to put a date on it.

1748
02:26:03,648 --> 02:26:12,477
[SPEAKER_08]: In terms of self-improvement, there's some modest self-improvement in current systems, but one could imagine a lot more, and that could happen in two years, it could happen in 20 years.

1749
02:26:12,477 --> 02:26:15,880
[SPEAKER_08]: There are basic paradigms that haven't been invented yet.

1750
02:26:15,880 --> 02:26:20,705
[SPEAKER_08]: Some of them we might want to discourage, but it's a bit hard to put timelines on them.

1751
02:26:20,705 --> 02:26:27,192
[SPEAKER_08]: And just going back to enforcement for one second, one thing that is absolutely paramount, I think, is far greater transparency about

1752
02:26:27,532 --> 02:26:29,373
[SPEAKER_08]: what the models are and what the data are.

1753
02:26:29,373 --> 02:26:43,684
[SPEAKER_08]: That doesn't necessarily mean everybody in the general public has to know exactly what's in one of these systems, but I think it means that there needs to be some enforcement arm that can look at these systems, can look at the data, can perform tests and so forth.

1754
02:26:43,684 --> 02:26:56,773
[SPEAKER_09]: Let me ask you, all of you, I think there has been a reference to elections and banning outputs

1755
02:26:57,549 --> 02:26:58,529
[SPEAKER_09]: involving elections.

1756
02:26:58,529 --> 02:27:12,032
[SPEAKER_09]: Are there other areas where you think, what are the other high risk or highest risk areas where you would either ban or establish especially strict rules?

1757
02:27:12,032 --> 02:27:12,192
[SPEAKER_09]: Ms.

1758
02:27:12,192 --> 02:27:15,253
[SPEAKER_09]: Montgomery.

1759
02:27:15,253 --> 02:27:19,954
[SPEAKER_02]: The space around misinformation, I think, is a hugely important one.

1760
02:27:19,954 --> 02:27:25,075
[SPEAKER_02]: And coming back to the points of transparency, you know, knowing what content was generated

1761
02:27:25,662 --> 02:27:32,415
[SPEAKER_02]: by AI is going to be a really critical area that we need to address.

1762
02:27:32,415 --> 02:27:32,796
[SPEAKER_09]: Any others?

1763
02:27:33,428 --> 02:27:36,329
[SPEAKER_08]: I think medical misinformation is something to really worry about.

1764
02:27:36,329 --> 02:27:38,009
[SPEAKER_08]: We have systems that hallucinate things.

1765
02:27:38,009 --> 02:27:39,650
[SPEAKER_08]: They're going to hallucinate medical advice.

1766
02:27:39,650 --> 02:27:40,850
[SPEAKER_08]: Some of the advice they'll give is good.

1767
02:27:40,850 --> 02:27:41,990
[SPEAKER_08]: Some of it's bad.

1768
02:27:41,990 --> 02:27:43,871
[SPEAKER_08]: We need really tight regulation around that.

1769
02:27:43,871 --> 02:27:50,332
[SPEAKER_08]: Same with psychiatric advice, people using these things as kind of ersatz therapists.

1770
02:27:50,332 --> 02:27:52,313
[SPEAKER_08]: I think we need to be very concerned about that.

1771
02:27:52,313 --> 02:28:00,455
[SPEAKER_08]: I think we need to be concerned about internet access for these tools when they can start making requests, both of people and internet things.

1772
02:28:00,455 --> 02:28:01,275
[SPEAKER_08]: It's probably okay.

1773
02:28:01,615 --> 02:28:10,818
[SPEAKER_08]: if they just do search, but as they do more intrusive things on the Internet, like do we want them to be able to order equipment or order chemistry and so forth.

1774
02:28:10,818 --> 02:28:17,019
[SPEAKER_08]: So as we empower these systems more by giving them Internet access, I think we need to be concerned about that.

1775
02:28:17,019 --> 02:28:19,260
[SPEAKER_08]: And then we've hardly talked at all about long-term risks.

1776
02:28:19,260 --> 02:28:21,381
[SPEAKER_08]: Sam alluded to it briefly.

1777
02:28:21,381 --> 02:28:29,143
[SPEAKER_08]: I don't think that's where we are right now, but as we start to approach machines that have a larger footprint on the world beyond just having a conversation,

1778
02:28:29,703 --> 02:28:35,366
[SPEAKER_08]: we need to worry about that and think about how we're going to regulate that and monitor it and so forth.

1779
02:28:35,366 --> 02:28:47,193
[SPEAKER_09]: In a sense, we've been talking about bad guys or certain bad actors manipulating AI to do harm.

1780
02:28:47,193 --> 02:28:48,654
[SPEAKER_09]: Manipulating people.

1781
02:28:48,654 --> 02:28:54,517
[SPEAKER_09]: And manipulating people, but also generative AI can manipulate the manipulators.

1782
02:28:55,967 --> 02:28:56,367
[SPEAKER_08]: It can.

1783
02:28:56,367 --> 02:29:03,031
[SPEAKER_08]: I mean, there's many layers of manipulation that are possible, and I think we don't yet really understand the consequences.

1784
02:29:03,031 --> 02:29:09,955
[SPEAKER_08]: Dan Dennett just sent me a manuscript last night that will be in the Atlantic in a few days on what he calls counterfeit people.

1785
02:29:09,955 --> 02:29:11,376
[SPEAKER_08]: It's a wonderful metaphor.

1786
02:29:11,376 --> 02:29:18,360
[SPEAKER_08]: These systems are almost like counterfeit people, and we don't really honestly understand what the consequence of that is.

1787
02:29:18,360 --> 02:29:18,700
[SPEAKER_08]: They're not

1788
02:29:19,184 --> 02:29:25,546
[SPEAKER_08]: perfectly human-like yet, but they're good enough to fool a lot of the people a lot of the time, and that introduces lots of problems.

1789
02:29:25,546 --> 02:29:30,107
[SPEAKER_08]: For example, cybercrime and how people might try to manipulate markets and so forth.

1790
02:29:30,107 --> 02:29:32,327
[SPEAKER_08]: So it's a serious concern.

1791
02:29:32,327 --> 02:29:37,568
[SPEAKER_09]: In my opening, I suggested three principles.

1792
02:29:37,568 --> 02:29:45,270
[SPEAKER_09]: Transparency, accountability, and limits on use.

1793
02:29:45,270 --> 02:29:47,911
[SPEAKER_09]: Would you agree that those are a good starting point?

1794
02:29:49,387 --> 02:29:50,548
[SPEAKER_09]: Is Montgomery?

1795
02:29:50,548 --> 02:29:50,608
[SPEAKER_02]: 100%.

1796
02:29:50,608 --> 02:29:54,990
[SPEAKER_02]: And as you also mentioned, industry shouldn't wait for Congress.

1797
02:29:54,990 --> 02:29:56,430
[SPEAKER_02]: That's what we're doing here at IBM.

1798
02:29:56,430 --> 02:29:59,272
[SPEAKER_09]: There's no reason that industry should wait for Congress.

1799
02:30:00,164 --> 02:30:01,504
[SPEAKER_09]: Professor Marcus?

1800
02:30:01,504 --> 02:30:03,485
[SPEAKER_08]: I think those three would be a great start.

1801
02:30:03,485 --> 02:30:13,067
[SPEAKER_08]: I mean, there are things like the White House Bill of Rights, for example, that show I think a large consensus, the UNESCO guidelines and so forth, show a large consensus around what it is we need.

1802
02:30:13,067 --> 02:30:18,488
[SPEAKER_08]: And the real question is definitely now how are we going to put some teeth in it, try to make these things actually enforced?

1803
02:30:18,488 --> 02:30:20,848
[SPEAKER_08]: So, for example, we don't have transparency yet.

1804
02:30:20,848 --> 02:30:24,949
[SPEAKER_08]: We all know we want it, but we're not doing enough to enforce it.

1805
02:30:24,949 --> 02:30:25,209
[SPEAKER_15]: Mr.

1806
02:30:25,209 --> 02:30:26,149
[SPEAKER_15]: Altman?

1807
02:30:26,149 --> 02:30:29,070
[SPEAKER_15]: I certainly agree that those are important points.

1808
02:30:29,070 --> 02:30:29,710
[SPEAKER_15]: I would add that

1809
02:30:30,897 --> 02:30:32,478
[SPEAKER_15]: And Professor Marcus touched on this.

1810
02:30:32,478 --> 02:30:50,428
[SPEAKER_15]: I would add that as we spend most of the time today on current risks, and I think that's appropriate, and I'm very glad we have done it, as these systems do become more capable, and I'm not sure how far away that is, but maybe not super far, I think it's important that we also spend time talking about how we're going to confront those challenges.

1811
02:30:50,428 --> 02:30:52,890
[SPEAKER_09]: I mean, talk to you privately.

1812
02:30:52,890 --> 02:30:54,511
[SPEAKER_09]: You know how much I care.

1813
02:30:54,511 --> 02:30:57,512
[SPEAKER_09]: I agree that you care deeply and intensely, but also

1814
02:30:59,263 --> 02:31:13,528
[SPEAKER_09]: prospect of increased danger or risk resulting from even more complex and capable AI mechanisms certainly may be closer than a lot of people appreciate.

1815
02:31:14,180 --> 02:31:21,567
[SPEAKER_08]: Let me just add for the record that I'm sitting next to Sam closer than I've ever sat to him except once before in my life.

1816
02:31:21,567 --> 02:31:31,335
[SPEAKER_08]: And that his sincerity in talking about those fears is very apparent physically in a way that just doesn't communicate on the television screen, but communicates from here.

1817
02:31:31,335 --> 02:31:32,396
[SPEAKER_09]: Thank you.

1818
02:31:32,396 --> 02:31:33,777
[SPEAKER_09]: Senator Hawley.

1819
02:31:33,777 --> 02:31:34,478
[SPEAKER_14]: Thank you again, Mr.

1820
02:31:34,478 --> 02:31:35,499
[SPEAKER_14]: Chairman, for a great hearing.

1821
02:31:35,499 --> 02:31:36,279
[SPEAKER_14]: Thanks to the witnesses.

1822
02:31:36,279 --> 02:31:41,484
[SPEAKER_14]: So I've been keeping a little list here of the potential downsides or harms, risks,

1823
02:31:42,146 --> 02:31:44,307
[SPEAKER_14]: of generative AI, even in its current form.

1824
02:31:44,307 --> 02:31:45,528
[SPEAKER_14]: Let's just run through it.

1825
02:31:45,528 --> 02:31:47,169
[SPEAKER_14]: Loss of jobs.

1826
02:31:47,169 --> 02:31:48,150
[SPEAKER_14]: And this isn't speculative.

1827
02:31:48,150 --> 02:31:49,050
[SPEAKER_14]: I think your company, Ms.

1828
02:31:49,050 --> 02:31:57,055
[SPEAKER_14]: Montgomery, has announced that it's potentially laying off 7,800 people, a third of your non-consumer facing workforce, because of AI.

1829
02:31:57,055 --> 02:32:09,102
[SPEAKER_14]: So loss of jobs, invasion of privacy, personal privacy, on a scale we've never before seen, manipulation of personal behavior, manipulation of personal opinions, and potentially the degradation of free elections in America.

1830
02:32:09,102 --> 02:32:09,682
[SPEAKER_14]: Did I miss anything?

1831
02:32:10,654 --> 02:32:13,915
[SPEAKER_14]: I mean, this is quite a list.

1832
02:32:13,915 --> 02:32:27,461
[SPEAKER_14]: I noticed that an eclectic group of about 1,000 technology and AI leaders, everybody from Andrew Yang to Elon Musk, recently called for a six-month moratorium on any further AI development.

1833
02:32:28,847 --> 02:32:29,307
[SPEAKER_14]: Are they right?

1834
02:32:29,307 --> 02:32:30,488
[SPEAKER_14]: Do you join those calls?

1835
02:32:30,488 --> 02:32:31,669
[SPEAKER_14]: Are they right to do that?

1836
02:32:31,669 --> 02:32:33,190
[SPEAKER_14]: Should we pause for six months?

1837
02:32:33,190 --> 02:32:35,731
[SPEAKER_08]: Your characterization is not quite correct.

1838
02:32:35,731 --> 02:32:36,832
[SPEAKER_08]: I actually signed that letter.

1839
02:32:36,832 --> 02:32:40,434
[SPEAKER_08]: About 27,000 people signed it.

1840
02:32:40,434 --> 02:32:44,437
[SPEAKER_08]: It did not call for a ban on all AI research, nor on all AI.

1841
02:32:47,038 --> 02:32:51,900
[SPEAKER_08]: on a very specific thing, which would be systems like GPT-5.

1842
02:32:51,900 --> 02:33:03,625
[SPEAKER_08]: Every other piece of research that's ever been done, it was actually supportive or neutral about, and it specifically called for more AI, specifically called for more research on trustworthy and safe AI.

1843
02:33:03,625 --> 02:33:11,128
[SPEAKER_14]: So you think that we should take a moratorium, a six-month moratorium or more, on anything beyond CHAT-GPT-4?

1844
02:33:11,803 --> 02:33:15,204
[SPEAKER_08]: I took the letter – what is the famous phrase?

1845
02:33:15,204 --> 02:33:17,345
[SPEAKER_08]: Spiritually, not literally, what was the famous phrase?

1846
02:33:17,345 --> 02:33:20,686
[SPEAKER_14]: Well, I'm asking for your opinion now, though.

1847
02:33:20,686 --> 02:33:28,548
[SPEAKER_08]: My opinion is that the moratorium that we should focus on is actually deployment until we have good safety cases.

1848
02:33:28,548 --> 02:33:39,631
[SPEAKER_08]: I don't know that we need to pause that particular project, but I do think its emphasis on focusing more on AI safety, on trustworthy, reliable AI, is exactly right.

1849
02:33:39,731 --> 02:33:42,252
[SPEAKER_14]: Deployment means not making it available to the public?

1850
02:33:42,252 --> 02:33:49,595
[SPEAKER_08]: Yeah, so my concern is about things that are deployed at a scale of, let's say, 100 million people without any external review.

1851
02:33:49,595 --> 02:33:53,397
[SPEAKER_08]: I think that we should think very carefully about doing that.

1852
02:33:53,397 --> 02:33:53,997
[SPEAKER_14]: What about you, Mr.

1853
02:33:53,997 --> 02:33:54,338
[SPEAKER_14]: Altman?

1854
02:33:54,338 --> 02:33:55,498
[SPEAKER_14]: Do you agree with that?

1855
02:33:55,498 --> 02:33:58,920
[SPEAKER_14]: Would you pause any further development for six months or longer?

1856
02:33:58,920 --> 02:34:04,162
[SPEAKER_15]: So, first of all, after we finished training GPT-4, we waited more than six months to deploy it.

1857
02:34:04,162 --> 02:34:06,723
[SPEAKER_15]: We are not currently training what will be GPT-5.

1858
02:34:06,723 --> 02:34:08,404
[SPEAKER_15]: We don't have plans to do it in the next six months.

1859
02:34:09,173 --> 02:34:11,593
[SPEAKER_15]: But I think the frame of the letter is wrong.

1860
02:34:11,593 --> 02:34:18,975
[SPEAKER_15]: What matters is audits, red teaming, safety standards that a model needs to pass before training.

1861
02:34:18,975 --> 02:34:21,795
[SPEAKER_15]: If we pause for six months, then I'm not sure what we do then.

1862
02:34:21,795 --> 02:34:22,896
[SPEAKER_15]: Do we pause for another six?

1863
02:34:22,896 --> 02:34:25,896
[SPEAKER_15]: Do we kind of come up with some rules then?

1864
02:34:25,896 --> 02:34:35,598
[SPEAKER_15]: The standards that we have developed and that we've used for GPT-4 deployment, we want to build on those, but we think that's the right direction, not a calendar clock pause.

1865
02:34:37,373 --> 02:34:43,456
[SPEAKER_15]: There may be times, I expect there will be times, when we find something that we don't understand and we really do need to take a pause.

1866
02:34:43,456 --> 02:34:47,537
[SPEAKER_15]: But we don't see that yet, never mind all the benefits.

1867
02:34:47,537 --> 02:34:49,158
[SPEAKER_14]: Wait a minute, you don't see what yet?

1868
02:34:49,158 --> 02:34:53,320
[SPEAKER_14]: You're comfortable with all of the potential ramifications from the current existing technology?

1869
02:34:53,320 --> 02:34:55,621
[SPEAKER_15]: I'm sorry, we don't see the reasons to not train a new one.

1870
02:34:55,621 --> 02:34:59,783
[SPEAKER_15]: For deploying, as I mentioned, I think there's all sorts of risky behavior and there's limits we put.

1871
02:34:59,783 --> 02:35:02,444
[SPEAKER_15]: We have to pull things back sometimes, add new ones.

1872
02:35:02,444 --> 02:35:05,425
[SPEAKER_15]: I mean, we don't see something that would stop us from training the next model.

1873
02:35:06,078 --> 02:35:10,721
[SPEAKER_15]: where we'd be so worried that we'd create something dangerous, even in that process, let alone the deployment.

1874
02:35:10,721 --> 02:35:11,462
[SPEAKER_14]: But that may happen.

1875
02:35:11,462 --> 02:35:12,062
[SPEAKER_14]: What about you, Ms.

1876
02:35:12,062 --> 02:35:12,422
[SPEAKER_14]: Montgomery?

1877
02:35:12,422 --> 02:35:20,867
[SPEAKER_02]: I think we need to use the time to prioritize ethics and responsible technology as opposed to pausing development.

1878
02:35:20,867 --> 02:35:26,891
[SPEAKER_14]: Well, wouldn't a pause in development help the development of protocols for safety standards and ethics?

1879
02:35:26,891 --> 02:35:32,715
[SPEAKER_02]: I'm not sure how practical it is to pause, but we absolutely should be prioritizing safety protocols.

1880
02:35:32,962 --> 02:35:36,164
[SPEAKER_14]: Okay, the point about practicality leads me to this.

1881
02:35:36,164 --> 02:35:40,527
[SPEAKER_14]: I'm interested in this talk about an agency and, you know, maybe that would work.

1882
02:35:40,527 --> 02:35:48,853
[SPEAKER_14]: Although, having seen how agencies work in this government, they usually get captured by the interests that they're supposed to regulate.

1883
02:35:48,853 --> 02:35:53,556
[SPEAKER_14]: They usually get controlled by the people who they're supposed to be watching.

1884
02:35:53,556 --> 02:35:55,277
[SPEAKER_14]: I mean, that's just been our history for a hundred years.

1885
02:35:55,277 --> 02:35:56,898
[SPEAKER_14]: Maybe this agency would be different.

1886
02:35:56,898 --> 02:35:57,859
[SPEAKER_14]: I have a little different idea.

1887
02:35:57,859 --> 02:36:00,541
[SPEAKER_14]: Why don't we just let people sue you?

1888
02:36:00,541 --> 02:36:02,302
[SPEAKER_14]: Why don't we just make you liable in court?

1889
02:36:02,926 --> 02:36:03,666
[SPEAKER_14]: We can do that.

1890
02:36:03,666 --> 02:36:04,707
[SPEAKER_14]: We know how to do that.

1891
02:36:04,707 --> 02:36:05,767
[SPEAKER_14]: We can pass a statute.

1892
02:36:05,767 --> 02:36:14,191
[SPEAKER_14]: We can create a federal right of action that will allow private individuals who are harmed by this technology to get into court and to bring evidence into court.

1893
02:36:14,191 --> 02:36:15,691
[SPEAKER_14]: And it can be anybody.

1894
02:36:15,691 --> 02:36:17,572
[SPEAKER_14]: I mean, you want to talk about crowdsourcing.

1895
02:36:17,572 --> 02:36:20,693
[SPEAKER_14]: We'll just open the courthouse doors.

1896
02:36:20,693 --> 02:36:25,936
[SPEAKER_14]: We'll define a broad right of action, private right of action, private citizens to be class actions.

1897
02:36:25,936 --> 02:36:27,076
[SPEAKER_14]: We'll just open it up.

1898
02:36:27,076 --> 02:36:28,297
[SPEAKER_14]: We'll allow people to go into court.

1899
02:36:28,297 --> 02:36:29,237
[SPEAKER_14]: We'll allow them to present evidence.

1900
02:36:29,237 --> 02:36:31,018
[SPEAKER_14]: They say that they were harmed by,

1901
02:36:32,146 --> 02:36:33,926
[SPEAKER_14]: They were given medical misinformation.

1902
02:36:33,926 --> 02:36:37,687
[SPEAKER_14]: They were given election misinformation, whatever.

1903
02:36:37,687 --> 02:36:39,047
[SPEAKER_14]: Why not do that, Mr.

1904
02:36:39,047 --> 02:36:40,347
[SPEAKER_15]: Altman?

1905
02:36:40,347 --> 02:36:41,708
[SPEAKER_15]: I mean, please forgive my ignorance.

1906
02:36:41,708 --> 02:36:44,008
[SPEAKER_15]: Can't people sue us?

1907
02:36:44,008 --> 02:36:57,990
[SPEAKER_14]: Well, you're not protected by Section 230, but there's not currently, I don't think, a federal right of action, private right of action that says that if you are harmed by generative AI technology, we will guarantee you the ability to get into court.

1908
02:36:57,990 --> 02:37:00,991
[SPEAKER_15]: Well, I think there's like a lot of other laws where if technology

1909
02:37:01,671 --> 02:37:07,259
[SPEAKER_15]: harms you, there's standards that we could be sued under, unless I'm really misunderstanding how things work.

1910
02:37:07,259 --> 02:37:16,572
[SPEAKER_15]: If the question is, are clearer laws about the specifics of this technology and consumer protection is a good thing, I would say definitely yes.

1911
02:37:17,870 --> 02:37:25,179
[SPEAKER_08]: The laws that we have today were designed long before we had artificial intelligence, and I do not think they give us enough coverage.

1912
02:37:25,179 --> 02:37:34,611
[SPEAKER_08]: The plan that you propose, I think, is a hypothetical, would certainly make a lot of lawyers wealthy, but I think it would be too slow to affect a lot of the things that we care about, and there are gaps in the law.

1913
02:37:34,611 --> 02:37:37,134
[SPEAKER_14]: For example, we don't really – Wait, you think it'd be slower than Congress?

1914
02:37:38,323 --> 02:37:40,766
[SPEAKER_08]: Yes, I do, in some ways.

1915
02:37:40,766 --> 02:37:41,787
[SPEAKER_08]: Really?

1916
02:37:41,787 --> 02:37:44,209
[SPEAKER_08]: Litigation can take a decade or more.

1917
02:37:44,209 --> 02:37:47,172
[SPEAKER_14]: Oh, but the threat of litigation is a powerful tool.

1918
02:37:47,172 --> 02:37:49,675
[SPEAKER_14]: I mean, how would IBM like to be sued for $100 billion?

1919
02:37:49,675 --> 02:38:01,367
[SPEAKER_08]: I'm in no way asking to take litigation off the table among the tools, but I think, for example, if I can continue, there are areas like copyright where we don't really have laws, we don't really have

1920
02:38:01,767 --> 02:38:10,714
[SPEAKER_08]: a way of thinking about wholesale misinformation as opposed to individual pieces of it, where, say, a foreign actor might make billions of pieces of misinformation, or a local actor.

1921
02:38:10,714 --> 02:38:17,799
[SPEAKER_08]: We have some laws around market manipulation we could apply, but we get in a lot of situations where we don't really know which laws apply.

1922
02:38:17,799 --> 02:38:18,920
[SPEAKER_08]: There would be loopholes.

1923
02:38:18,920 --> 02:38:20,121
[SPEAKER_08]: This system is really not

1924
02:38:20,720 --> 02:38:21,360
[SPEAKER_08]: thought through.

1925
02:38:21,360 --> 02:38:26,363
[SPEAKER_08]: In fact, we don't even know that 230 does or does not apply here, as far as I know.

1926
02:38:26,363 --> 02:38:30,445
[SPEAKER_08]: I think that that's something a lot of people speculated about this afternoon, but it's not solid.

1927
02:38:30,445 --> 02:38:31,646
[SPEAKER_08]: Well, we could fix that.

1928
02:38:31,646 --> 02:38:34,827
[SPEAKER_15]: Well, the question is how.

1929
02:38:34,827 --> 02:38:35,628
[SPEAKER_14]: Oh, easy.

1930
02:38:35,628 --> 02:38:41,131
[SPEAKER_14]: You just, you just, it would be easy for us to say that Section 230 doesn't apply to generative AI.

1931
02:38:41,131 --> 02:38:41,211
[SPEAKER_09]: Ms.

1932
02:38:41,211 --> 02:38:42,271
[SPEAKER_09]: Montgomery, I'll give you the last word.

1933
02:38:42,271 --> 02:38:44,392
[SPEAKER_09]: I think it's an important start.

1934
02:38:44,392 --> 02:38:45,653
[SPEAKER_09]: You suggested, Ms.

1935
02:38:45,653 --> 02:38:49,815
[SPEAKER_09]: Montgomery, a duty of care, which I think fits the idea of

1936
02:38:50,373 --> 02:38:51,774
[SPEAKER_09]: private right of action.

1937
02:38:51,774 --> 02:38:52,795
[SPEAKER_02]: No, that's exactly right.

1938
02:38:52,795 --> 02:38:55,456
[SPEAKER_02]: And also, AI is not a shield, right?

1939
02:38:55,456 --> 02:39:14,288
[SPEAKER_02]: So if a company discriminates in granting credit, for example, or in the hiring process, by virtue of the fact that they relied too significantly on an AI tool, they're responsible for that today, regardless of whether they used a tool or a human to make that decision.

1940
02:39:14,288 --> 02:39:18,531
[SPEAKER_09]: I'm going to turn to Senator Booker for some final questions, but I just

1941
02:39:19,759 --> 02:39:24,680
[SPEAKER_09]: want to make a quick point here on the issue of the moratorium.

1942
02:39:24,680 --> 02:39:26,041
[SPEAKER_09]: I think we need to be careful.

1943
02:39:26,041 --> 02:39:27,881
[SPEAKER_09]: The world won't wait.

1944
02:39:27,881 --> 02:39:33,243
[SPEAKER_09]: The rest of the global scientific community isn't going to pause.

1945
02:39:33,243 --> 02:39:35,963
[SPEAKER_09]: We have adversaries that are moving ahead.

1946
02:39:35,963 --> 02:39:39,184
[SPEAKER_09]: And sticking our head in the sand is not the answer.

1947
02:39:39,184 --> 02:39:44,266
[SPEAKER_09]: Safeguards and protections, yes.

1948
02:39:44,266 --> 02:39:49,187
[SPEAKER_09]: But a flat stop sign, sticking our head in the sand,

1949
02:39:50,265 --> 02:39:52,686
[SPEAKER_09]: I would be very, very worried.

1950
02:39:52,686 --> 02:40:05,070
[SPEAKER_08]: Without militating for any sort of pause, I would just again emphasize there is a difference between research, which surely we need to do to keep pace with our foreign rivals, and deployment at really massive scale.

1951
02:40:05,070 --> 02:40:12,113
[SPEAKER_08]: You could deploy things at the scale of a million people or 10 million people, but not 100 million people or a billion people.

1952
02:40:12,113 --> 02:40:18,615
[SPEAKER_08]: And if there are risks, you might find them out sooner and be able to close the barn doors before the horses leave rather than after.

1953
02:40:19,567 --> 02:40:20,609
[SPEAKER_13]: Senator Booker.

1954
02:40:20,609 --> 02:40:22,772
[SPEAKER_13]: Yeah, I just, there will be no pause.

1955
02:40:22,772 --> 02:40:25,156
[SPEAKER_13]: I mean, there's no enforcement body to force a pause.

1956
02:40:25,156 --> 02:40:26,518
[SPEAKER_13]: It's just not, not going to happen.

1957
02:40:26,518 --> 02:40:32,647
[SPEAKER_13]: It's nice to call for it for any just reasons or whatsoever, but I'm, forgive me for sounding skeptical.

1958
02:40:32,647 --> 02:40:33,528
[SPEAKER_13]: Nobody's pausing.

1959
02:40:33,528 --> 02:40:34,450
[SPEAKER_13]: This thing is racing.

1960
02:40:36,160 --> 02:40:36,881
[SPEAKER_08]: I would agree.

1961
02:40:36,881 --> 02:40:38,703
[SPEAKER_08]: I don't think it's a realistic thing in the world.

1962
02:40:38,703 --> 02:40:51,099
[SPEAKER_08]: The reason I personally signed the letter was to call attention to how serious the problems were and to emphasize spending more of our efforts on trustworthy and safe AI rather than just making a bigger version of something we already know to be unreliable.

1963
02:40:52,040 --> 02:40:54,320
[SPEAKER_13]: So I'm a futurist.

1964
02:40:54,320 --> 02:40:56,221
[SPEAKER_13]: I love exciting about the future.

1965
02:40:56,221 --> 02:41:05,282
[SPEAKER_13]: And I guess there's a famous question, if you couldn't control for your race, your gender, where you would land on the planet Earth, or what time in humanity would you want to be born?

1966
02:41:05,282 --> 02:41:07,383
[SPEAKER_13]: Everyone would say right now.

1967
02:41:07,383 --> 02:41:12,244
[SPEAKER_13]: It's still the best time to be alive because of technology, innovation, and everything.

1968
02:41:12,244 --> 02:41:14,544
[SPEAKER_13]: And I'm excited about what the future holds.

1969
02:41:14,544 --> 02:41:19,045
[SPEAKER_13]: But the destructiveness that I've also seen, as a person that's seen the transformative technologies,

1970
02:41:21,089 --> 02:41:26,713
[SPEAKER_13]: of a lot of the technologies of the last 25 years is what really concerns me.

1971
02:41:26,713 --> 02:41:39,421
[SPEAKER_13]: And one of the things, especially with companies that are designed to want to keep my attention on screens, and I'm not just talking about new media, 24-hour cable news is a great example of people that

1972
02:41:39,878 --> 02:41:42,601
[SPEAKER_13]: want to keep your eyes on screens.

1973
02:41:42,601 --> 02:41:46,686
[SPEAKER_13]: I have a lot of concerns about corporate intention.

1974
02:41:46,686 --> 02:41:52,693
[SPEAKER_13]: And Sam, this is again why I find your story so fascinating to me and your values that I believe in.

1975
02:41:53,591 --> 02:41:56,513
[SPEAKER_13]: from our conversations so compelling to me.

1976
02:41:56,513 --> 02:42:13,644
[SPEAKER_13]: But absent that, I really want to just explore what happens when these companies that are already controlling so much of our lives, a lot has been written about the FANG companies, what happens when they are the ones that are dominating this technology as they did before?

1977
02:42:13,644 --> 02:42:17,626
[SPEAKER_13]: So, Professor Marcus, does that have any concern, the role that

1978
02:42:18,079 --> 02:42:35,887
[SPEAKER_08]: Corporate power corporate concentration has in this realm that a few companies might might control this whole area I radically changed the shape of my own life in the last few months and it was because of what happened with Microsoft releasing Sydney and it didn't go the way I thought it would in one way it did and

1979
02:42:36,187 --> 02:42:38,309
[SPEAKER_08]: which is I anticipated the hallucinations.

1980
02:42:38,309 --> 02:42:42,992
[SPEAKER_08]: I wrote an essay, which I have in the appendix, What to Expect When You're Expecting GPT-4.

1981
02:42:42,992 --> 02:42:50,137
[SPEAKER_08]: And I said that it would still be a good tool for misinformation, that it would still have trouble with physical reasoning, psychological reasoning, that it would hallucinate.

1982
02:42:50,137 --> 02:42:54,240
[SPEAKER_08]: And then along came Sydney, and the initial press reports were quite favorable.

1983
02:42:54,240 --> 02:42:59,424
[SPEAKER_08]: And then there was the famous article by Kevin Roos, in which it recommended he get a divorce.

1984
02:42:59,424 --> 02:43:05,168
[SPEAKER_08]: And I had seen Tay, and I had seen Galactica from Metta, and those had been polled after they had problems.

1985
02:43:05,728 --> 02:43:07,628
[SPEAKER_08]: And Sydney clearly had problems.

1986
02:43:07,628 --> 02:43:14,070
[SPEAKER_08]: What I would have done, had I run Microsoft, which clearly I do not, would have been to temporarily withdraw it from the market.

1987
02:43:14,070 --> 02:43:15,290
[SPEAKER_08]: And they didn't.

1988
02:43:15,290 --> 02:43:28,313
[SPEAKER_08]: And that was a wake-up call to me and a reminder that even if you have a company like OpenAI that is a non-profit, and Sam's values I think have become clear today, other people can buy those companies and do what they like with them.

1989
02:43:28,313 --> 02:43:35,255
[SPEAKER_08]: And maybe we have a stable set of actors now, but the amount of power that these systems have to shape our views

1990
02:43:35,775 --> 02:43:38,157
[SPEAKER_08]: and our lives is really, really significant.

1991
02:43:38,157 --> 02:43:43,901
[SPEAKER_08]: And that doesn't even get into the risks that someone might repurpose them deliberately for all kinds of bad purposes.

1992
02:43:43,901 --> 02:43:53,727
[SPEAKER_08]: And so in the middle of February, I stopped writing much about technical issues in AI, which is most of what I've written about for the last decade, and said, I need to work on policy.

1993
02:43:53,727 --> 02:43:55,448
[SPEAKER_08]: This is frightening.

1994
02:43:55,448 --> 02:43:57,470
[SPEAKER_13]: And Sam, I want to give you an opportunity.

1995
02:43:57,470 --> 02:44:00,772
[SPEAKER_13]: It's my last question or so.

1996
02:44:02,526 --> 02:44:06,389
[SPEAKER_13]: Don't you have concerns about – I mean, you – I graduated from Stanford.

1997
02:44:06,389 --> 02:44:13,914
[SPEAKER_13]: I know so many of the players in the Valley, from VC folks, angel folks, to a lot of founders of companies that we all know.

1998
02:44:13,914 --> 02:44:23,221
[SPEAKER_13]: Do you have some concern about a few players with extraordinary resources and power, power to influence Washington?

1999
02:44:23,221 --> 02:44:28,925
[SPEAKER_13]: I mean, I see us – I love – I'm a big believer in the free market, but the reason why I walk into a

2000
02:44:29,445 --> 02:44:39,492
[SPEAKER_13]: bodega and a Twinkie is cheaper than an apple, or a Happy Meal costs less than a bucket of salad, is because of the way the government tips the scales to pick winners and losers.

2001
02:44:39,492 --> 02:44:45,056
[SPEAKER_13]: So the free market is not what it should be when you have large corporate power that can even influence the game here.

2002
02:44:45,056 --> 02:44:51,060
[SPEAKER_13]: Do you have some concerns about that in this next era of technological innovation?

2003
02:44:51,060 --> 02:44:51,320
[SPEAKER_15]: Yeah.

2004
02:44:51,320 --> 02:44:53,742
[SPEAKER_15]: I mean, again, that's so much of why we started OpenAI.

2005
02:44:53,742 --> 02:44:54,942
[SPEAKER_15]: We have huge concerns about that.

2006
02:44:56,110 --> 02:45:02,995
[SPEAKER_15]: I think it's important to democratize the inputs to these systems, the values that we're going to align to.

2007
02:45:02,995 --> 02:45:05,817
[SPEAKER_15]: And I think it's also important to give people wide use of these tools.

2008
02:45:05,817 --> 02:45:13,083
[SPEAKER_15]: When we started the API strategy, which is a big part of how we make our systems available for anyone to use, there was a huge amount of skepticism over that.

2009
02:45:13,083 --> 02:45:15,564
[SPEAKER_15]: And it does come with challenges, that's for sure.

2010
02:45:15,564 --> 02:45:23,230
[SPEAKER_15]: But we think putting this in the hands of a lot of people and not in the hands of a few companies is really quite important.

2011
02:45:23,230 --> 02:45:25,712
[SPEAKER_15]: And we are seeing the resultant innovation boom from that.

2012
02:45:27,394 --> 02:45:35,938
[SPEAKER_15]: But it is absolutely true that the number of companies that can train the true frontier models is going to be small just because of the resources required.

2013
02:45:35,938 --> 02:45:40,020
[SPEAKER_15]: And so I think there needs to be incredible scrutiny on us and our competitors.

2014
02:45:40,020 --> 02:45:48,504
[SPEAKER_15]: I think there is a rich and exciting industry happening of incredibly good research and new startups that are not just using our models but creating their own.

2015
02:45:48,504 --> 02:45:53,367
[SPEAKER_15]: And I think it's important to make sure that whatever regulatory stuff happens, whatever new agencies may or may not happen,

2016
02:45:53,824 --> 02:45:56,845
[SPEAKER_15]: we preserve that fire, because that's critical.

2017
02:45:56,845 --> 02:46:01,187
[SPEAKER_13]: I'm a big believer in the democratizing potential of technology.

2018
02:46:01,187 --> 02:46:08,890
[SPEAKER_13]: But I've seen the promise of that fail time and time again, where people said, oh, this is going to have a big democratizing force.

2019
02:46:08,890 --> 02:46:18,895
[SPEAKER_13]: My team works on a lot of issues about the reinforcing of bias through algorithms, the failure to advertise certain opportunities and certain zip codes.

2020
02:46:20,135 --> 02:46:26,124
[SPEAKER_13]: But you seem to be saying, and I heard this with Web3, that this is going to be decentralized finance.

2021
02:46:26,124 --> 02:46:28,167
[SPEAKER_13]: All these things are going to happen.

2022
02:46:28,167 --> 02:46:36,439
[SPEAKER_13]: But this seems to me not even to offer that promise because the people who are designing these, it takes so much power, energy, resources.

2023
02:46:37,331 --> 02:46:53,504
[SPEAKER_13]: Are you saying that my dreams of technology further democratizing opportunity and more are possible within a technology that is ultimately, I think, going to be very centralized to a few players who already control so much?

2024
02:46:53,504 --> 02:47:01,731
[SPEAKER_15]: So this point that I made about use of the model and building on top of it, this is really a new platform, right?

2025
02:47:01,731 --> 02:47:04,533
[SPEAKER_15]: It is definitely important to talk about who's going to create the models.

2026
02:47:04,533 --> 02:47:05,514
[SPEAKER_15]: I want to do that.

2027
02:47:05,514 --> 02:47:07,015
[SPEAKER_15]: I also think it's really important to decide

2028
02:47:07,298 --> 02:47:10,139
[SPEAKER_15]: to whose values we're going to align these models.

2029
02:47:10,139 --> 02:47:16,922
[SPEAKER_15]: But in terms of using the models, the people that build on top of the OpenAI API do incredible things.

2030
02:47:16,922 --> 02:47:22,784
[SPEAKER_15]: And it's, you know, people frequently comment like, I can't believe you get this much technology for this little money.

2031
02:47:22,784 --> 02:47:32,048
[SPEAKER_15]: And so what people are, the companies people are building, putting AI everywhere, using our API, which does let us put safeguards in place, I think that's quite exciting.

2032
02:47:32,048 --> 02:47:36,490
[SPEAKER_15]: And I think that is how it is being, not how it's going to be, but how it is being democratized right now.

2033
02:47:37,241 --> 02:47:45,152
[SPEAKER_15]: There is a whole new Cambrian explosion of new businesses, new products, new services happening by lots of different companies on top of these models.

2034
02:47:48,007 --> 02:47:56,970
[SPEAKER_13]: I have most industries resist even reasonable regulation from seatbelt laws to we've been talking a lot recently about rail safety.

2035
02:47:56,970 --> 02:48:08,614
[SPEAKER_13]: The only way we're going to see the democratization of values, I think, and while there are noble companies out there, is if we create rules of the road that enforce certain safety measures like we've seen with other technology.

2036
02:48:08,614 --> 02:48:10,234
[SPEAKER_13]: Thank you.

2037
02:48:10,234 --> 02:48:11,755
[SPEAKER_09]: Thanks, Senator Booker.

2038
02:48:11,755 --> 02:48:14,016
[SPEAKER_09]: And I couldn't agree more that

2039
02:48:15,523 --> 02:48:20,846
[SPEAKER_09]: in terms of consumer protection, which I've been doing for a while.

2040
02:48:20,846 --> 02:48:35,373
[SPEAKER_09]: Participation by the industry is tremendously important, and not just rhetorically, but in real terms, because we have a lot of industries that come before us and say, oh, we're all in favor of rules, but not those rules.

2041
02:48:35,373 --> 02:48:37,313
[SPEAKER_09]: Those rules we don't like.

2042
02:48:37,313 --> 02:48:41,876
[SPEAKER_09]: And it's every rule, in fact, that they don't like.

2043
02:48:41,876 --> 02:48:44,457
[SPEAKER_09]: And I sense that there is a willingness to participate here

2044
02:48:45,634 --> 02:48:48,917
[SPEAKER_09]: is genuine and authentic.

2045
02:48:48,917 --> 02:49:00,448
[SPEAKER_09]: I thought about asking CHAT-GPT to do a new version of don't stop thinking about tomorrow because that's what we need to be doing here.

2046
02:49:00,448 --> 02:49:09,896
[SPEAKER_09]: And as Senator Hawley has pointed out, Congress doesn't always move at the pace of technology and that may be a reason why we need

2047
02:49:10,620 --> 02:49:16,164
[SPEAKER_09]: a new agency, but we also need to recognize the rest of the world is going to be moving as well.

2048
02:49:16,164 --> 02:49:25,910
[SPEAKER_09]: And you've been enormously helpful in focusing us and illuminating some of these questions and performed a great service by being here today.

2049
02:49:25,910 --> 02:49:30,494
[SPEAKER_09]: So thank you to every one of our witnesses.

2050
02:49:30,494 --> 02:49:36,798
[SPEAKER_09]: And I'm going to close the hearing, leave the record open for one week.

2051
02:49:36,798 --> 02:49:39,920
[SPEAKER_09]: In case anyone wants to submit anything, I encourage any of you who have

2052
02:49:40,607 --> 02:49:52,813
[SPEAKER_09]: either manuscripts that are going to be published or observations from your companies to submit them to us and we look forward to our next hearing.

2053
02:49:52,813 --> 02:49:53,573
[SPEAKER_09]: This one is closed.
